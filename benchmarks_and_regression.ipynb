{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "from tensorflow.compat.v1.keras import layers\n",
    "import tensorflow.compat.v1 as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tf_graph_util import convert_variables_to_constants\n",
    "from lstm import create_lstm\n",
    "\n",
    "from seq2seq import create_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def benchmark_model(model, cmd=None):\n",
    "    bench_path = f\"{model.name}_benchmark.txt\"\n",
    "    if not os.path.exists(f\"{model.name}.pbtxt\") and not os.path.exists(bench_path):\n",
    "        if not os.path.exists(f\"{model.name}.pbtxt\"):\n",
    "            print(\"Saving model...\")\n",
    "            tf.keras.backend.clear_session()\n",
    "            sess = tf.keras.backend.get_session()\n",
    "    #         output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            output_graph_def = convert_variables_to_constants(\n",
    "                sess,\n",
    "                sess.graph.as_graph_def(),\n",
    "                [node.op.name for node in model.outputs])\n",
    "            tf.io.write_graph(output_graph_def, './', f'{model.name}.pbtxt')\n",
    "        else:\n",
    "            print(\"Retrieving saved model.\")\n",
    "    \n",
    "    \n",
    "        if not os.path.exists(bench_path):\n",
    "            if not cmd:\n",
    "                input_shape = f\"1,{','.join(str(dim) for dim in model.input.shape[1:])}\"\n",
    "                cmd = f'../tensorflow/bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph={model.name}.pbtxt --input_layer=\"{model.input.name}\" --input_layer_shape=\"{input_shape}\" --output_layer=\"{model.output.name}\"'\n",
    "                print(cmd)\n",
    "            print(\"Running benchmark...\")\n",
    "            benchmark = subprocess.run([cmd], stderr=subprocess.PIPE, shell=True)\n",
    "            print(\"Done.\")\n",
    "\n",
    "            output = benchmark.stderr.decode('unicode_escape')\n",
    "            split_output = output[output.find('Run Order'):output.find('Top by Computation Time')].split('\\n')\n",
    "\n",
    "            with open(bench_path, 'w') as f:\n",
    "                f.write(\"\\n\".join(split_output[1:-2]))\n",
    "        else:\n",
    "            print(\"Retrieving saved benchmark results.\")\n",
    "    else:\n",
    "        print(\"Retrieving saved model and benchmark results.\")\n",
    "    \n",
    "    f = open(bench_path)\n",
    "    benchmark = pd.read_csv(f, sep=\"\\t\").rename(columns=lambda x: x.strip())\n",
    "    benchmark = benchmark.drop(benchmark.columns[0], axis=1)\n",
    "    benchmark['name'] = benchmark['[Name]'].apply(lambda x: x.split('/')[0])\n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_layer_features(model):\n",
    "    layers = pd.DataFrame()\n",
    "    layers['name'] = pd.Series([layer.name for layer in model.layers])\n",
    "    \n",
    "#     input_dims = {layer.name: [dim.value for dim in layer.input.shape.dims] for layer in model.layers}\n",
    "    \n",
    "#     layers['input_shape'] = pd.Series([[dim.value for dim in layer.input.shape.dims] for layer in model.layers])\n",
    "#     layers['output_shape'] = pd.Series([[dim.value for dim in layer.output.shape.dims] for layer in model.layers])\n",
    "    layers['input_shape'] = pd.Series([layer.input_shape for layer in model.layers])\n",
    "    layers['output_shape'] = pd.Series([layer.output_shape for layer in model.layers])\n",
    "\n",
    "    features = ['units','filters','activation','strides','kernel_size']\n",
    "    for feature in features:\n",
    "        layers[feature] = pd.Series(\n",
    "            [layer.get_config()[feature] if feature in layer.get_config() else None for layer in model.layers])\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_benchmark(features, benchmark):\n",
    "    speed = benchmark[['name', '[avg ms]']].groupby('name').sum()\n",
    "    mem = benchmark[['name', '[mem KB]']].groupby('name').max()\n",
    "    \n",
    "    return features.join(speed, on='name').join(mem, on='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def flatten_shape(shape):\n",
    "    if not shape:\n",
    "        return None\n",
    "    \n",
    "    def reduce(tup):\n",
    "        acc = 1\n",
    "        for val in tup:\n",
    "            if val:\n",
    "                acc *= val\n",
    "        return acc\n",
    "    \n",
    "    if isinstance(shape, list):\n",
    "        return sum(reduce(tup) for tup in shape)\n",
    "    \n",
    "    return reduce(shape)\n",
    "\n",
    "\n",
    "def clean(data, inference=False):\n",
    "    if inference:\n",
    "        cleaned = pd.get_dummies(data, columns=['activation'], dummy_na=True)\n",
    "    else:\n",
    "        cleaned = pd.get_dummies(\n",
    "            data.dropna(subset=['[avg ms]', '[mem KB]']), columns=['activation'], dummy_na=True)\n",
    "    \n",
    "    for activation in ['selu', 'elu', 'softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'exponential', 'linear']:\n",
    "        col = f\"activation_{activation}\"\n",
    "        if col not in cleaned.columns:\n",
    "            cleaned[col] = pd.Series(0)\n",
    "    \n",
    "\n",
    "    cleaned['input_size'] = cleaned['input_shape'].apply(flatten_shape)\n",
    "    cleaned['output_size'] = cleaned['output_shape'].apply(flatten_shape)\n",
    "    cleaned['stride_size'] = cleaned['strides'].apply(flatten_shape)\n",
    "    cleaned['kernel_size'] = cleaned['kernel_size'].apply(flatten_shape)\n",
    "\n",
    "    return cleaned.fillna(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rf_regression_model(data, column):\n",
    "    X = data.drop(['name', '[avg ms]', '[mem KB]', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "    y = data[column]  # Labels\n",
    "\n",
    "    # Split dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state = RANDOM_SEED) # 70% training and 30% test\n",
    "\n",
    "    # Instantiate model with 1000 decision trees\n",
    "    rf = RandomForestRegressor(n_estimators = 1000, random_state = RANDOM_SEED)\n",
    "    # Train the model on training data\n",
    "    rf.fit(X_train, y_train);\n",
    "\n",
    "    return rf, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "def stacked_regression_model(data, column):\n",
    "\n",
    "    X = data.drop(['name', '[avg ms]', '[mem KB]', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "    y = data[column]  # Labels\n",
    "\n",
    "    estimators = [\n",
    "        ('rf', RandomForestRegressor(n_estimators=1000, random_state=RANDOM_SEED)),\n",
    "        ('lr', RidgeCV()),\n",
    "        ('svr', LinearSVR(random_state=42)),\n",
    "        \n",
    "    ]\n",
    "    reg = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "#         final_estimator=RandomForestRegressor(n_estimators=1000, random_state=RANDOM_SEED)\n",
    "    )\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    \n",
    "    reg.fit(X_train, y_train).score(X_test, y_test)\n",
    "    \n",
    "    return reg, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def stacked_regression_model_2(data, column):\n",
    "\n",
    "    X = data.drop(['name', '[avg ms]', '[mem KB]', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "    y = data[column]  # Labels\n",
    "    \n",
    "    poly = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=5)),\n",
    "        ('linear', LinearRegression(fit_intercept=False))\n",
    "    ])\n",
    "\n",
    "    estimators = [\n",
    "        ('poly', poly),\n",
    "        ('lr', RidgeCV()),\n",
    "        ('svr', LinearSVR(random_state=42)),\n",
    "        \n",
    "    ]\n",
    "    reg = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "#         final_estimator=RandomForestRegressor(n_estimators=1000, random_state=RANDOM_SEED)\n",
    "    )\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    \n",
    "    reg.fit(X_train, y_train).score(X_test, y_test)\n",
    "    \n",
    "    return reg, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/brian/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session() # IMPORTANT for layer names to match up.\n",
    "model = tf.keras.applications.vgg16.VGG16(\n",
    "    include_top=True,\n",
    "    weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving saved model and benchmark results.\n"
     ]
    }
   ],
   "source": [
    "benchmark = benchmark_model(model)\n",
    "features = get_layer_features(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_df = join_benchmark(features, benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving saved model and benchmark results.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session() # IMPORTANT for layer names to match up.\n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "inception = tf. keras.applications.inception_v3.InceptionV3(\n",
    "    include_top=True, weights='imagenet')\n",
    "inception_benchmark = benchmark_model(inception)\n",
    "inception_features = get_layer_features(inception)\n",
    "inception_df = join_benchmark(inception_features, inception_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # lstm.summary()\n",
    "\n",
    "# tf.keras.backend.clear_session() # IMPORTANT for layer names to match up.\n",
    "\n",
    "# lstm = create_lstm()\n",
    "\n",
    "# # tf.keras.backend.set_learning_phase(0)\n",
    "# lstm_benchmark = benchmark_model(lstm)\n",
    "# lstm_features = get_layer_features(lstm)\n",
    "# lstm_df = join_benchmark(lstm_features, lstm_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>input_shape</th>\n",
       "      <th>output_shape</th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>activation</th>\n",
       "      <th>strides</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>[avg ms]</th>\n",
       "      <th>[mem KB]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>batch_normalization_28</td>\n",
       "      <td>(None, 35, 35, 96)</td>\n",
       "      <td>(None, 35, 35, 96)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>activation_35</td>\n",
       "      <td>(None, 17, 17, 128)</td>\n",
       "      <td>(None, 17, 17, 128)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.849</td>\n",
       "      <td>147.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>conv2d_89</td>\n",
       "      <td>(None, 8, 8, 2048)</td>\n",
       "      <td>(None, 8, 8, 448)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>448.0</td>\n",
       "      <td>linear</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>activation_84</td>\n",
       "      <td>(None, 8, 8, 192)</td>\n",
       "      <td>(None, 8, 8, 192)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.719</td>\n",
       "      <td>49.152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>batch_normalization_45</td>\n",
       "      <td>(None, 17, 17, 160)</td>\n",
       "      <td>(None, 17, 17, 160)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>activation_61</td>\n",
       "      <td>(None, 17, 17, 192)</td>\n",
       "      <td>(None, 17, 17, 192)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.203</td>\n",
       "      <td>221.952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>mixed10</td>\n",
       "      <td>[(None, 8, 8, 320), (None, 8, 8, 768), (None, ...</td>\n",
       "      <td>(None, 8, 8, 2048)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.159</td>\n",
       "      <td>524.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>batch_normalization_15</td>\n",
       "      <td>(None, 35, 35, 64)</td>\n",
       "      <td>(None, 35, 35, 64)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>batch_normalization_84</td>\n",
       "      <td>(None, 8, 8, 192)</td>\n",
       "      <td>(None, 8, 8, 192)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>max_pooling2d_2</td>\n",
       "      <td>(None, 35, 35, 288)</td>\n",
       "      <td>(None, 17, 17, 288)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>0.340</td>\n",
       "      <td>332.928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name  \\\n",
       "91   batch_normalization_28   \n",
       "106           activation_35   \n",
       "280               conv2d_89   \n",
       "278           activation_84   \n",
       "137  batch_normalization_45   \n",
       "207           activation_61   \n",
       "310                 mixed10   \n",
       "42   batch_normalization_15   \n",
       "274  batch_normalization_84   \n",
       "99          max_pooling2d_2   \n",
       "\n",
       "                                           input_shape         output_shape  \\\n",
       "91                                  (None, 35, 35, 96)   (None, 35, 35, 96)   \n",
       "106                                (None, 17, 17, 128)  (None, 17, 17, 128)   \n",
       "280                                 (None, 8, 8, 2048)    (None, 8, 8, 448)   \n",
       "278                                  (None, 8, 8, 192)    (None, 8, 8, 192)   \n",
       "137                                (None, 17, 17, 160)  (None, 17, 17, 160)   \n",
       "207                                (None, 17, 17, 192)  (None, 17, 17, 192)   \n",
       "310  [(None, 8, 8, 320), (None, 8, 8, 768), (None, ...   (None, 8, 8, 2048)   \n",
       "42                                  (None, 35, 35, 64)   (None, 35, 35, 64)   \n",
       "274                                  (None, 8, 8, 192)    (None, 8, 8, 192)   \n",
       "99                                 (None, 35, 35, 288)  (None, 17, 17, 288)   \n",
       "\n",
       "     units  filters activation strides kernel_size  [avg ms]  [mem KB]  \n",
       "91     NaN      NaN       None    None        None     0.007     0.000  \n",
       "106    NaN      NaN       relu    None        None     0.849   147.968  \n",
       "280    NaN    448.0     linear  (1, 1)      (1, 1)     0.002     0.000  \n",
       "278    NaN      NaN       relu    None        None     0.719    49.152  \n",
       "137    NaN      NaN       None    None        None     0.005     0.000  \n",
       "207    NaN      NaN       relu    None        None     1.203   221.952  \n",
       "310    NaN      NaN       None    None        None     0.159   524.288  \n",
       "42     NaN      NaN       None    None        None     0.003     0.000  \n",
       "274    NaN      NaN       None    None        None     0.003     0.000  \n",
       "99     NaN      NaN       None  (2, 2)        None     0.340   332.928  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([vgg_df, inception_df])\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>input_shape</th>\n",
       "      <th>output_shape</th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>strides</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>[avg ms]</th>\n",
       "      <th>[mem KB]</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>...</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>block1_conv1</td>\n",
       "      <td>(None, 224, 224, 3)</td>\n",
       "      <td>(None, 224, 224, 64)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.657</td>\n",
       "      <td>12845.056</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>150528</td>\n",
       "      <td>3211264</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>block1_conv2</td>\n",
       "      <td>(None, 224, 224, 64)</td>\n",
       "      <td>(None, 224, 224, 64)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.123</td>\n",
       "      <td>12845.056</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3211264</td>\n",
       "      <td>3211264</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>block1_pool</td>\n",
       "      <td>(None, 224, 224, 64)</td>\n",
       "      <td>(None, 112, 112, 64)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.437</td>\n",
       "      <td>3211.264</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3211264</td>\n",
       "      <td>802816</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>block2_conv1</td>\n",
       "      <td>(None, 112, 112, 64)</td>\n",
       "      <td>(None, 112, 112, 128)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.181</td>\n",
       "      <td>6422.528</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>802816</td>\n",
       "      <td>1605632</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>block2_conv2</td>\n",
       "      <td>(None, 112, 112, 128)</td>\n",
       "      <td>(None, 112, 112, 128)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.004</td>\n",
       "      <td>6422.528</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1605632</td>\n",
       "      <td>1605632</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           name            input_shape           output_shape  units  filters  \\\n",
       "1  block1_conv1    (None, 224, 224, 3)   (None, 224, 224, 64)   -1.0     64.0   \n",
       "2  block1_conv2   (None, 224, 224, 64)   (None, 224, 224, 64)   -1.0     64.0   \n",
       "3   block1_pool   (None, 224, 224, 64)   (None, 112, 112, 64)   -1.0     -1.0   \n",
       "4  block2_conv1   (None, 112, 112, 64)  (None, 112, 112, 128)   -1.0    128.0   \n",
       "5  block2_conv2  (None, 112, 112, 128)  (None, 112, 112, 128)   -1.0    128.0   \n",
       "\n",
       "  strides  kernel_size  [avg ms]   [mem KB]  activation_linear  ...  \\\n",
       "1  (1, 1)          9.0     2.657  12845.056                  0  ...   \n",
       "2  (1, 1)          9.0    18.123  12845.056                  0  ...   \n",
       "3  (2, 2)         -1.0     3.437   3211.264                  0  ...   \n",
       "4  (1, 1)          9.0     7.181   6422.528                  0  ...   \n",
       "5  (1, 1)          9.0    13.004   6422.528                  0  ...   \n",
       "\n",
       "   activation_elu  activation_softplus  activation_softsign  activation_tanh  \\\n",
       "1            -1.0                 -1.0                 -1.0             -1.0   \n",
       "2            -1.0                 -1.0                 -1.0             -1.0   \n",
       "3            -1.0                 -1.0                 -1.0             -1.0   \n",
       "4            -1.0                 -1.0                 -1.0             -1.0   \n",
       "5            -1.0                 -1.0                 -1.0             -1.0   \n",
       "\n",
       "   activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "1                -1.0                     -1.0                    -1.0   \n",
       "2                -1.0                     -1.0                    -1.0   \n",
       "3                -1.0                     -1.0                    -1.0   \n",
       "4                -1.0                     -1.0                    -1.0   \n",
       "5                -1.0                     -1.0                    -1.0   \n",
       "\n",
       "   input_size  output_size  stride_size  \n",
       "1      150528      3211264          1.0  \n",
       "2     3211264      3211264          1.0  \n",
       "3     3211264       802816          4.0  \n",
       "4      802816      1605632          1.0  \n",
       "5     1605632      1605632          1.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned = clean(data)\n",
    "\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 17.10178814125108\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>221952</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.237503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.920930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>36992</td>\n",
       "      <td>36992</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.882680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>221952</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.237503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.911988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.911988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>235200</td>\n",
       "      <td>78400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>36992</td>\n",
       "      <td>36992</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.882680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.089</td>\n",
       "      <td>0.924709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>117600</td>\n",
       "      <td>117600</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.043</td>\n",
       "      <td>1.366511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.911988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>352800</td>\n",
       "      <td>78400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.014869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>81920</td>\n",
       "      <td>12288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "228   -1.0     -1.0         -1.0                  0                0   \n",
       "89    -1.0     -1.0         -1.0                  0                1   \n",
       "197   -1.0    192.0          1.0                  1                0   \n",
       "111   -1.0     -1.0         -1.0                  0                1   \n",
       "164   -1.0     -1.0         -1.0                  0                0   \n",
       "158   -1.0     -1.0         -1.0                  0                0   \n",
       "190   -1.0     -1.0         -1.0                  0                0   \n",
       "212   -1.0     -1.0         -1.0                  0                0   \n",
       "214   -1.0     -1.0         -1.0                  0                1   \n",
       "162   -1.0     -1.0         -1.0                  0                1   \n",
       "78    -1.0     -1.0         -1.0                  0                0   \n",
       "18    -1.0     64.0          1.0                  1                0   \n",
       "117   -1.0     -1.0         -1.0                  0                1   \n",
       "256   -1.0     -1.0         -1.0                  0                1   \n",
       "84    -1.0     -1.0         -1.0                  0                1   \n",
       "208   -1.0     -1.0         -1.0                  0                1   \n",
       "64    -1.0     64.0          1.0                  1                0   \n",
       "210   -1.0    192.0          7.0                  1                0   \n",
       "191   -1.0     -1.0         -1.0                  0                0   \n",
       "268   -1.0    192.0          1.0                  1                0   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "228                   0               1             -1.0            -1.0   \n",
       "89                    0               0             -1.0            -1.0   \n",
       "197                   0               0             -1.0            -1.0   \n",
       "111                   0               0             -1.0            -1.0   \n",
       "164                   0               1             -1.0            -1.0   \n",
       "158                   0               1             -1.0            -1.0   \n",
       "190                   0               1             -1.0            -1.0   \n",
       "212                   0               1             -1.0            -1.0   \n",
       "214                   0               0             -1.0            -1.0   \n",
       "162                   0               0             -1.0            -1.0   \n",
       "78                    0               1             -1.0            -1.0   \n",
       "18                    0               0             -1.0            -1.0   \n",
       "117                   0               0             -1.0            -1.0   \n",
       "256                   0               0             -1.0            -1.0   \n",
       "84                    0               0             -1.0            -1.0   \n",
       "208                   0               0             -1.0            -1.0   \n",
       "64                    0               0             -1.0            -1.0   \n",
       "210                   0               0             -1.0            -1.0   \n",
       "191                   0               1             -1.0            -1.0   \n",
       "268                   0               0             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "228                 -1.0                 -1.0             -1.0   \n",
       "89                  -1.0                 -1.0             -1.0   \n",
       "197                 -1.0                 -1.0             -1.0   \n",
       "111                 -1.0                 -1.0             -1.0   \n",
       "164                 -1.0                 -1.0             -1.0   \n",
       "158                 -1.0                 -1.0             -1.0   \n",
       "190                 -1.0                 -1.0             -1.0   \n",
       "212                 -1.0                 -1.0             -1.0   \n",
       "214                 -1.0                 -1.0             -1.0   \n",
       "162                 -1.0                 -1.0             -1.0   \n",
       "78                  -1.0                 -1.0             -1.0   \n",
       "18                  -1.0                 -1.0             -1.0   \n",
       "117                 -1.0                 -1.0             -1.0   \n",
       "256                 -1.0                 -1.0             -1.0   \n",
       "84                  -1.0                 -1.0             -1.0   \n",
       "208                 -1.0                 -1.0             -1.0   \n",
       "64                  -1.0                 -1.0             -1.0   \n",
       "210                 -1.0                 -1.0             -1.0   \n",
       "191                 -1.0                 -1.0             -1.0   \n",
       "268                 -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "228                -1.0                     -1.0                    -1.0   \n",
       "89                 -1.0                     -1.0                    -1.0   \n",
       "197                -1.0                     -1.0                    -1.0   \n",
       "111                -1.0                     -1.0                    -1.0   \n",
       "164                -1.0                     -1.0                    -1.0   \n",
       "158                -1.0                     -1.0                    -1.0   \n",
       "190                -1.0                     -1.0                    -1.0   \n",
       "212                -1.0                     -1.0                    -1.0   \n",
       "214                -1.0                     -1.0                    -1.0   \n",
       "162                -1.0                     -1.0                    -1.0   \n",
       "78                 -1.0                     -1.0                    -1.0   \n",
       "18                 -1.0                     -1.0                    -1.0   \n",
       "117                -1.0                     -1.0                    -1.0   \n",
       "256                -1.0                     -1.0                    -1.0   \n",
       "84                 -1.0                     -1.0                    -1.0   \n",
       "208                -1.0                     -1.0                    -1.0   \n",
       "64                 -1.0                     -1.0                    -1.0   \n",
       "210                -1.0                     -1.0                    -1.0   \n",
       "191                -1.0                     -1.0                    -1.0   \n",
       "268                -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  avg_ms_actual  avg_ms_pred  \n",
       "228      221952       221952         -1.0          0.182     0.237503  \n",
       "89        78400        78400         -1.0          0.683     0.920930  \n",
       "197      221952        55488          1.0          0.002     0.002323  \n",
       "111       36992        36992         -1.0          0.992     0.882680  \n",
       "164      221952       221952         -1.0          0.272     0.237503  \n",
       "158       55488        55488         -1.0          0.006     0.005435  \n",
       "190       55488        55488         -1.0          0.007     0.005435  \n",
       "212       55488        55488         -1.0          0.006     0.005435  \n",
       "214       55488        55488         -1.0          0.654     0.911988  \n",
       "162       55488        55488         -1.0          0.546     0.911988  \n",
       "78        78400        78400         -1.0          0.005     0.003876  \n",
       "18       235200        78400          1.0          0.002     0.002029  \n",
       "117       36992        36992         -1.0          0.868     0.882680  \n",
       "256       24576        24576         -1.0          1.089     0.924709  \n",
       "84       117600       117600         -1.0          1.043     1.366511  \n",
       "208       55488        55488         -1.0          0.999     0.911988  \n",
       "64       352800        78400          1.0          0.002     0.014869  \n",
       "210       55488        55488          1.0          0.003     0.002422  \n",
       "191       55488        55488         -1.0          0.005     0.005435  \n",
       "268       81920        12288          1.0          0.002     0.003071  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = cleaned.drop(['name', '[avg ms]', '[mem KB]', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "y = cleaned['[avg ms]']  # Labels\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train);\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1).sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 317183.27335722075\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>mem_KB_actual</th>\n",
       "      <th>mem_KB_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>36992</td>\n",
       "      <td>36992</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>147.968</td>\n",
       "      <td>147.710592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>117600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.896512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>58800</td>\n",
       "      <td>58800</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>235.200</td>\n",
       "      <td>230.245248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>12288</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>46.583680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>46240</td>\n",
       "      <td>46240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>117600</td>\n",
       "      <td>117600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.772352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>58800</td>\n",
       "      <td>58800</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>352800</td>\n",
       "      <td>78400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.330496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>117600</td>\n",
       "      <td>117600</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.359296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>221952</td>\n",
       "      <td>1.0</td>\n",
       "      <td>887.808</td>\n",
       "      <td>899.217408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>36992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>81920</td>\n",
       "      <td>12288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>58800</td>\n",
       "      <td>58800</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "111   -1.0     -1.0         -1.0                  0                1   \n",
       "291   -1.0    384.0          3.0                  1                0   \n",
       "222   -1.0     -1.0         -1.0                  0                0   \n",
       "68    -1.0     96.0          9.0                  1                0   \n",
       "218   -1.0    192.0          7.0                  1                0   \n",
       "25    -1.0     -1.0         -1.0                  0                1   \n",
       "242   -1.0    192.0          9.0                  1                0   \n",
       "209   -1.0    192.0          7.0                  1                0   \n",
       "178   -1.0    160.0          7.0                  1                0   \n",
       "76    -1.0     96.0          9.0                  1                0   \n",
       "46    -1.0     -1.0         -1.0                  0                0   \n",
       "238   -1.0     -1.0         -1.0                  0                0   \n",
       "230   -1.0     -1.0         -1.0                  0                0   \n",
       "87    -1.0     64.0          1.0                  1                0   \n",
       "200   -1.0    192.0          7.0                  1                0   \n",
       "80    -1.0     -1.0         -1.0                  0                0   \n",
       "119   -1.0     -1.0         -1.0                  0                0   \n",
       "107   -1.0    128.0          1.0                  1                0   \n",
       "268   -1.0    192.0          1.0                  1                0   \n",
       "69    -1.0     -1.0         -1.0                  0                0   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "111                   0               0             -1.0            -1.0   \n",
       "291                   0               0             -1.0            -1.0   \n",
       "222                   0               1             -1.0            -1.0   \n",
       "68                    0               0             -1.0            -1.0   \n",
       "218                   0               0             -1.0            -1.0   \n",
       "25                    0               0             -1.0            -1.0   \n",
       "242                   0               0             -1.0            -1.0   \n",
       "209                   0               0             -1.0            -1.0   \n",
       "178                   0               0             -1.0            -1.0   \n",
       "76                    0               0             -1.0            -1.0   \n",
       "46                    0               1             -1.0            -1.0   \n",
       "238                   0               1             -1.0            -1.0   \n",
       "230                   0               1             -1.0            -1.0   \n",
       "87                    0               0             -1.0            -1.0   \n",
       "200                   0               0             -1.0            -1.0   \n",
       "80                    0               1             -1.0            -1.0   \n",
       "119                   0               1             -1.0            -1.0   \n",
       "107                   0               0             -1.0            -1.0   \n",
       "268                   0               0             -1.0            -1.0   \n",
       "69                    0               1             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "111                 -1.0                 -1.0             -1.0   \n",
       "291                 -1.0                 -1.0             -1.0   \n",
       "222                 -1.0                 -1.0             -1.0   \n",
       "68                  -1.0                 -1.0             -1.0   \n",
       "218                 -1.0                 -1.0             -1.0   \n",
       "25                  -1.0                 -1.0             -1.0   \n",
       "242                 -1.0                 -1.0             -1.0   \n",
       "209                 -1.0                 -1.0             -1.0   \n",
       "178                 -1.0                 -1.0             -1.0   \n",
       "76                  -1.0                 -1.0             -1.0   \n",
       "46                  -1.0                 -1.0             -1.0   \n",
       "238                 -1.0                 -1.0             -1.0   \n",
       "230                 -1.0                 -1.0             -1.0   \n",
       "87                  -1.0                 -1.0             -1.0   \n",
       "200                 -1.0                 -1.0             -1.0   \n",
       "80                  -1.0                 -1.0             -1.0   \n",
       "119                 -1.0                 -1.0             -1.0   \n",
       "107                 -1.0                 -1.0             -1.0   \n",
       "268                 -1.0                 -1.0             -1.0   \n",
       "69                  -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "111                -1.0                     -1.0                    -1.0   \n",
       "291                -1.0                     -1.0                    -1.0   \n",
       "222                -1.0                     -1.0                    -1.0   \n",
       "68                 -1.0                     -1.0                    -1.0   \n",
       "218                -1.0                     -1.0                    -1.0   \n",
       "25                 -1.0                     -1.0                    -1.0   \n",
       "242                -1.0                     -1.0                    -1.0   \n",
       "209                -1.0                     -1.0                    -1.0   \n",
       "178                -1.0                     -1.0                    -1.0   \n",
       "76                 -1.0                     -1.0                    -1.0   \n",
       "46                 -1.0                     -1.0                    -1.0   \n",
       "238                -1.0                     -1.0                    -1.0   \n",
       "230                -1.0                     -1.0                    -1.0   \n",
       "87                 -1.0                     -1.0                    -1.0   \n",
       "200                -1.0                     -1.0                    -1.0   \n",
       "80                 -1.0                     -1.0                    -1.0   \n",
       "119                -1.0                     -1.0                    -1.0   \n",
       "107                -1.0                     -1.0                    -1.0   \n",
       "268                -1.0                     -1.0                    -1.0   \n",
       "69                 -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  mem_KB_actual  mem_KB_pred  \n",
       "111       36992        36992         -1.0        147.968   147.710592  \n",
       "291       24576        24576          1.0          0.000     0.000000  \n",
       "222       55488        55488         -1.0          0.000     0.000000  \n",
       "68        78400       117600          1.0          0.000    14.896512  \n",
       "218       55488        55488          1.0          0.000     0.000000  \n",
       "25        58800        58800         -1.0        235.200   230.245248  \n",
       "242       55488        12288          4.0          0.000    46.583680  \n",
       "209       55488        55488          1.0          0.000     0.000000  \n",
       "178       46240        46240          1.0          0.000     0.000000  \n",
       "76       117600       117600          1.0          0.000     2.772352  \n",
       "46        58800        58800         -1.0          0.000     0.000000  \n",
       "238       55488        55488         -1.0          0.000     0.000000  \n",
       "230       55488        55488         -1.0          0.000     0.000000  \n",
       "87       352800        78400          1.0          0.000     2.330496  \n",
       "200       55488        55488          1.0          0.000     0.000000  \n",
       "80       117600       117600         -1.0          0.000     2.359296  \n",
       "119      221952       221952          1.0        887.808   899.217408  \n",
       "107      221952        36992          1.0          0.000     0.000000  \n",
       "268       81920        12288          1.0          0.000     0.008192  \n",
       "69        58800        58800         -1.0          0.000     0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = cleaned.drop(['name', '[avg ms]', '[mem KB]', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "y = cleaned['[mem KB]']  # Labels\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train);\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_KB_actual': y_test, 'mem_KB_pred': y_pred})], axis=1).sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.3499471311167615\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100352</td>\n",
       "      <td>100352</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.547</td>\n",
       "      <td>5.884671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12288</td>\n",
       "      <td>12288</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.287</td>\n",
       "      <td>0.836314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>28672</td>\n",
       "      <td>28672</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>131072</td>\n",
       "      <td>131072</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.479530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>58800</td>\n",
       "      <td>78400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>401408</td>\n",
       "      <td>802816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.049</td>\n",
       "      <td>6.026361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>39200</td>\n",
       "      <td>39200</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.799501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "56    -1.0     -1.0         -1.0                  0                0   \n",
       "155   -1.0    192.0          1.0                  1                0   \n",
       "17    -1.0    512.0          9.0                  0                1   \n",
       "309   -1.0     -1.0         -1.0                  0                1   \n",
       "250   -1.0     -1.0         -1.0                  0                0   \n",
       "293   -1.0     -1.0         -1.0                  0                0   \n",
       "42    -1.0     -1.0         -1.0                  0                0   \n",
       "52    -1.0     64.0         25.0                  1                0   \n",
       "7     -1.0    256.0          9.0                  0                1   \n",
       "39    -1.0     -1.0         -1.0                  0                1   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "56                    0               1             -1.0            -1.0   \n",
       "155                   0               0             -1.0            -1.0   \n",
       "17                    0               0             -1.0            -1.0   \n",
       "309                   0               0             -1.0            -1.0   \n",
       "250                   0               1             -1.0            -1.0   \n",
       "293                   0               1             -1.0            -1.0   \n",
       "42                    0               1             -1.0            -1.0   \n",
       "52                    0               0             -1.0            -1.0   \n",
       "7                     0               0             -1.0            -1.0   \n",
       "39                    0               0             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "56                  -1.0                 -1.0             -1.0   \n",
       "155                 -1.0                 -1.0             -1.0   \n",
       "17                  -1.0                 -1.0             -1.0   \n",
       "309                 -1.0                 -1.0             -1.0   \n",
       "250                 -1.0                 -1.0             -1.0   \n",
       "293                 -1.0                 -1.0             -1.0   \n",
       "42                  -1.0                 -1.0             -1.0   \n",
       "52                  -1.0                 -1.0             -1.0   \n",
       "7                   -1.0                 -1.0             -1.0   \n",
       "39                  -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "56                 -1.0                     -1.0                    -1.0   \n",
       "155                -1.0                     -1.0                    -1.0   \n",
       "17                 -1.0                     -1.0                    -1.0   \n",
       "309                -1.0                     -1.0                    -1.0   \n",
       "250                -1.0                     -1.0                    -1.0   \n",
       "293                -1.0                     -1.0                    -1.0   \n",
       "42                 -1.0                     -1.0                    -1.0   \n",
       "52                 -1.0                     -1.0                    -1.0   \n",
       "7                  -1.0                     -1.0                    -1.0   \n",
       "39                 -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  avg_ms_actual  avg_ms_pred  \n",
       "56        78400        78400         -1.0          0.004     0.004020  \n",
       "155      221952        55488          1.0          0.002     0.002239  \n",
       "17       100352       100352          1.0          4.547     5.884671  \n",
       "309       12288        12288         -1.0          1.287     0.836314  \n",
       "250       28672        28672         -1.0          0.005     0.006932  \n",
       "293      131072       131072          1.0          0.762     0.479530  \n",
       "42        78400        78400         -1.0          0.003     0.004020  \n",
       "52        58800        78400          1.0          0.002     0.004720  \n",
       "7        401408       802816          1.0          7.049     6.026361  \n",
       "39        39200        39200         -1.0          0.434     0.799501  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AVG MS RF\n",
    "cnn_ms_rf, X_test, y_test = rf_regression_model(cleaned, '[avg ms]')\n",
    "\n",
    "y_pred = cnn_ms_rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "cnn_ms_rf_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 70984.64512575942\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>268203</td>\n",
       "      <td>710432</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>400.351360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>221952</td>\n",
       "      <td>1.0</td>\n",
       "      <td>887.808</td>\n",
       "      <td>888.982528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>117600</td>\n",
       "      <td>117600</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>470.400</td>\n",
       "      <td>463.789312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100352</td>\n",
       "      <td>25088</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.352</td>\n",
       "      <td>117.181696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1382976</td>\n",
       "      <td>341056</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1364.224</td>\n",
       "      <td>1497.272128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>235200</td>\n",
       "      <td>58800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>98.304</td>\n",
       "      <td>98.304000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "229   -1.0    192.0          1.0                  1                0   \n",
       "1     -1.0     32.0          9.0                  1                0   \n",
       "151   -1.0     -1.0         -1.0                  0                0   \n",
       "92    -1.0     -1.0         -1.0                  0                1   \n",
       "18    -1.0     -1.0         -1.0                  0                0   \n",
       "10    -1.0     -1.0         -1.0                  0                0   \n",
       "123   -1.0    192.0          1.0                  1                0   \n",
       "21    -1.0     48.0          1.0                  1                0   \n",
       "232   -1.0    192.0          7.0                  1                0   \n",
       "288   -1.0     -1.0         -1.0                  0                1   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "229                   0               0             -1.0            -1.0   \n",
       "1                     0               0             -1.0            -1.0   \n",
       "151                   0               1             -1.0            -1.0   \n",
       "92                    0               0             -1.0            -1.0   \n",
       "18                    0               1             -1.0            -1.0   \n",
       "10                    0               1             -1.0            -1.0   \n",
       "123                   0               0             -1.0            -1.0   \n",
       "21                    0               0             -1.0            -1.0   \n",
       "232                   0               0             -1.0            -1.0   \n",
       "288                   0               0             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "229                 -1.0                 -1.0             -1.0   \n",
       "1                   -1.0                 -1.0             -1.0   \n",
       "151                 -1.0                 -1.0             -1.0   \n",
       "92                  -1.0                 -1.0             -1.0   \n",
       "18                  -1.0                 -1.0             -1.0   \n",
       "10                  -1.0                 -1.0             -1.0   \n",
       "123                 -1.0                 -1.0             -1.0   \n",
       "21                  -1.0                 -1.0             -1.0   \n",
       "232                 -1.0                 -1.0             -1.0   \n",
       "288                 -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "229                -1.0                     -1.0                    -1.0   \n",
       "1                  -1.0                     -1.0                    -1.0   \n",
       "151                -1.0                     -1.0                    -1.0   \n",
       "92                 -1.0                     -1.0                    -1.0   \n",
       "18                 -1.0                     -1.0                    -1.0   \n",
       "10                 -1.0                     -1.0                    -1.0   \n",
       "123                -1.0                     -1.0                    -1.0   \n",
       "21                 -1.0                     -1.0                    -1.0   \n",
       "232                -1.0                     -1.0                    -1.0   \n",
       "288                -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  mem_kb_actual  mem_kb_pred  \n",
       "229      221952        55488          1.0          0.000     0.000000  \n",
       "1        268203       710432          4.0          0.000   400.351360  \n",
       "151      221952       221952          1.0        887.808   888.982528  \n",
       "92       117600       117600         -1.0        470.400   463.789312  \n",
       "18       100352        25088          4.0        100.352   117.181696  \n",
       "10      1382976       341056          4.0       1364.224  1497.272128  \n",
       "123      221952        55488          1.0          0.000     0.000000  \n",
       "21       235200        58800          1.0          0.000     0.000000  \n",
       "232       55488        55488          1.0          0.000     0.000000  \n",
       "288       24576        24576         -1.0         98.304    98.304000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MEM KB RF\n",
    "cnn_mem_rf, X_test, y_test = rf_regression_model(cleaned, '[mem KB]')\n",
    "\n",
    "y_pred = cnn_mem_rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "cnn_mem_rf_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.2940391907913917\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>46240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.420535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.263402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.286609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12288</td>\n",
       "      <td>12288</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.287</td>\n",
       "      <td>0.749305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.336043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.992539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>117600</td>\n",
       "      <td>117600</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.290</td>\n",
       "      <td>1.376630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.925</td>\n",
       "      <td>0.824567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>58800</td>\n",
       "      <td>58800</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.522446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.992539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "139   -1.0    160.0          1.0                  1                0   \n",
       "55    -1.0     -1.0         -1.0                  0                0   \n",
       "125   -1.0     -1.0         -1.0                  0                0   \n",
       "309   -1.0     -1.0         -1.0                  0                1   \n",
       "216   -1.0    192.0          1.0                  1                0   \n",
       "129   -1.0     -1.0         -1.0                  0                1   \n",
       "49    -1.0     -1.0         -1.0                  0                1   \n",
       "83    -1.0     -1.0         -1.0                  0                1   \n",
       "25    -1.0     -1.0         -1.0                  0                1   \n",
       "239   -1.0     -1.0         -1.0                  0                1   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "139                   0               0             -1.0            -1.0   \n",
       "55                    0               1             -1.0            -1.0   \n",
       "125                   0               1             -1.0            -1.0   \n",
       "309                   0               0             -1.0            -1.0   \n",
       "216                   0               0             -1.0            -1.0   \n",
       "129                   0               0             -1.0            -1.0   \n",
       "49                    0               0             -1.0            -1.0   \n",
       "83                    0               0             -1.0            -1.0   \n",
       "25                    0               0             -1.0            -1.0   \n",
       "239                   0               0             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "139                 -1.0                 -1.0             -1.0   \n",
       "55                  -1.0                 -1.0             -1.0   \n",
       "125                 -1.0                 -1.0             -1.0   \n",
       "309                 -1.0                 -1.0             -1.0   \n",
       "216                 -1.0                 -1.0             -1.0   \n",
       "129                 -1.0                 -1.0             -1.0   \n",
       "49                  -1.0                 -1.0             -1.0   \n",
       "83                  -1.0                 -1.0             -1.0   \n",
       "25                  -1.0                 -1.0             -1.0   \n",
       "239                 -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "139                -1.0                     -1.0                    -1.0   \n",
       "55                 -1.0                     -1.0                    -1.0   \n",
       "125                -1.0                     -1.0                    -1.0   \n",
       "309                -1.0                     -1.0                    -1.0   \n",
       "216                -1.0                     -1.0                    -1.0   \n",
       "129                -1.0                     -1.0                    -1.0   \n",
       "49                 -1.0                     -1.0                    -1.0   \n",
       "83                 -1.0                     -1.0                    -1.0   \n",
       "25                 -1.0                     -1.0                    -1.0   \n",
       "239                -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  avg_ms_actual  avg_ms_pred  \n",
       "139      221952        46240          1.0          0.002     0.420535  \n",
       "55        78400        78400         -1.0          0.005     0.263402  \n",
       "125       55488        55488         -1.0          0.003     0.286609  \n",
       "309       12288        12288         -1.0          1.287     0.749305  \n",
       "216      221952        55488          1.0          0.002     0.336043  \n",
       "129       55488        55488         -1.0          0.708     0.992539  \n",
       "49       117600       117600         -1.0          1.290     1.376630  \n",
       "83        78400        78400         -1.0          1.925     0.824567  \n",
       "25        58800        58800         -1.0          0.450     0.522446  \n",
       "239       55488        55488         -1.0          0.671     0.992539  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AVG MS stacked\n",
    "cnn_ms_stacked_1, X_test, y_test = stacked_regression_model(cleaned, '[avg ms]')\n",
    "\n",
    "y_pred = cnn_ms_stacked_1.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "cnn_ms_stacked_1_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1076248.8717503173\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>117600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.702398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>46240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.656129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221.952</td>\n",
       "      <td>276.701087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>39200</td>\n",
       "      <td>39200</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>156.800</td>\n",
       "      <td>213.228637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>117600</td>\n",
       "      <td>117600</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.991929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>98.304</td>\n",
       "      <td>166.092819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>117600</td>\n",
       "      <td>117600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>28.740704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>221952</td>\n",
       "      <td>1.0</td>\n",
       "      <td>887.808</td>\n",
       "      <td>784.072508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>352800</td>\n",
       "      <td>352800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1411.200</td>\n",
       "      <td>1207.010633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1605632</td>\n",
       "      <td>401408</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1605.632</td>\n",
       "      <td>1485.698043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "68    -1.0     96.0          9.0                  1                0   \n",
       "171   -1.0    160.0          1.0                  1                0   \n",
       "214   -1.0     -1.0         -1.0                  0                1   \n",
       "39    -1.0     -1.0         -1.0                  0                1   \n",
       "80    -1.0     -1.0         -1.0                  0                0   \n",
       "303   -1.0     -1.0         -1.0                  0                1   \n",
       "76    -1.0     96.0          9.0                  1                0   \n",
       "183   -1.0     -1.0         -1.0                  0                0   \n",
       "73    -1.0     -1.0         -1.0                  0                0   \n",
       "6     -1.0     -1.0         -1.0                  0                0   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "68                    0               0             -1.0            -1.0   \n",
       "171                   0               0             -1.0            -1.0   \n",
       "214                   0               0             -1.0            -1.0   \n",
       "39                    0               0             -1.0            -1.0   \n",
       "80                    0               1             -1.0            -1.0   \n",
       "303                   0               0             -1.0            -1.0   \n",
       "76                    0               0             -1.0            -1.0   \n",
       "183                   0               1             -1.0            -1.0   \n",
       "73                    0               1             -1.0            -1.0   \n",
       "6                     0               1             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "68                  -1.0                 -1.0             -1.0   \n",
       "171                 -1.0                 -1.0             -1.0   \n",
       "214                 -1.0                 -1.0             -1.0   \n",
       "39                  -1.0                 -1.0             -1.0   \n",
       "80                  -1.0                 -1.0             -1.0   \n",
       "303                 -1.0                 -1.0             -1.0   \n",
       "76                  -1.0                 -1.0             -1.0   \n",
       "183                 -1.0                 -1.0             -1.0   \n",
       "73                  -1.0                 -1.0             -1.0   \n",
       "6                   -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "68                 -1.0                     -1.0                    -1.0   \n",
       "171                -1.0                     -1.0                    -1.0   \n",
       "214                -1.0                     -1.0                    -1.0   \n",
       "39                 -1.0                     -1.0                    -1.0   \n",
       "80                 -1.0                     -1.0                    -1.0   \n",
       "303                -1.0                     -1.0                    -1.0   \n",
       "76                 -1.0                     -1.0                    -1.0   \n",
       "183                -1.0                     -1.0                    -1.0   \n",
       "73                 -1.0                     -1.0                    -1.0   \n",
       "6                  -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  mem_kb_actual  mem_kb_pred  \n",
       "68        78400       117600          1.0          0.000    20.702398  \n",
       "171      221952        46240          1.0          0.000    -2.656129  \n",
       "214       55488        55488         -1.0        221.952   276.701087  \n",
       "39        39200        39200         -1.0        156.800   213.228637  \n",
       "80       117600       117600         -1.0          0.000   100.991929  \n",
       "303       24576        24576         -1.0         98.304   166.092819  \n",
       "76       117600       117600          1.0          0.000    28.740704  \n",
       "183      221952       221952          1.0        887.808   784.072508  \n",
       "73       352800       352800          1.0       1411.200  1207.010633  \n",
       "6       1605632       401408          4.0       1605.632  1485.698043  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AVG MEM stacked\n",
    "cnn_mem_stacked_1, X_test, y_test = stacked_regression_model(cleaned, '[mem KB]')\n",
    "\n",
    "y_pred = cnn_mem_stacked_1.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "cnn_mem_stacked_1_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 79.61104081858598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>426320</td>\n",
       "      <td>967872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-7.434774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-8.968249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-9.039889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.073</td>\n",
       "      <td>-7.682186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-8.534105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "14    -1.0    192.0          9.0                  1                0   \n",
       "298   -1.0     -1.0         -1.0                  0                0   \n",
       "155   -1.0    192.0          1.0                  1                0   \n",
       "192   -1.0     -1.0         -1.0                  0                1   \n",
       "217   -1.0    192.0          7.0                  1                0   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "14                    0               0             -1.0            -1.0   \n",
       "298                   0               1             -1.0            -1.0   \n",
       "155                   0               0             -1.0            -1.0   \n",
       "192                   0               0             -1.0            -1.0   \n",
       "217                   0               0             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "14                  -1.0                 -1.0             -1.0   \n",
       "298                 -1.0                 -1.0             -1.0   \n",
       "155                 -1.0                 -1.0             -1.0   \n",
       "192                 -1.0                 -1.0             -1.0   \n",
       "217                 -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "14                 -1.0                     -1.0                    -1.0   \n",
       "298                -1.0                     -1.0                    -1.0   \n",
       "155                -1.0                     -1.0                    -1.0   \n",
       "192                -1.0                     -1.0                    -1.0   \n",
       "217                -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  avg_ms_actual  avg_ms_pred  \n",
       "14       426320       967872          1.0          0.002    -7.434774  \n",
       "298       24576        24576         -1.0          0.008    -8.968249  \n",
       "155      221952        55488          1.0          0.002    -9.039889  \n",
       "192       55488        55488         -1.0          1.073    -7.682186  \n",
       "217       55488        55488          1.0          0.003    -8.534105  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lstm stacked avg ms\n",
    "cnn_ms_stacked_2, X_test, y_test = stacked_regression_model_2(cleaned, '[avg ms]')\n",
    "\n",
    "y_pred = cnn_ms_stacked_2.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "cnn_ms_stacked_2_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.0917266885366172e+19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.304109e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>117600</td>\n",
       "      <td>117600</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>470.400</td>\n",
       "      <td>-3.304109e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12288</td>\n",
       "      <td>12288</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.304109e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100352</td>\n",
       "      <td>100352</td>\n",
       "      <td>1.0</td>\n",
       "      <td>401.408</td>\n",
       "      <td>-3.304106e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>58800</td>\n",
       "      <td>58800</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.304109e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "222   -1.0     -1.0         -1.0                  0                0   \n",
       "84    -1.0     -1.0         -1.0                  0                1   \n",
       "274   -1.0     -1.0         -1.0                  0                0   \n",
       "16    -1.0    512.0          9.0                  0                1   \n",
       "23    -1.0     -1.0         -1.0                  0                0   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "222                   0               1             -1.0            -1.0   \n",
       "84                    0               0             -1.0            -1.0   \n",
       "274                   0               1             -1.0            -1.0   \n",
       "16                    0               0             -1.0            -1.0   \n",
       "23                    0               1             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "222                 -1.0                 -1.0             -1.0   \n",
       "84                  -1.0                 -1.0             -1.0   \n",
       "274                 -1.0                 -1.0             -1.0   \n",
       "16                  -1.0                 -1.0             -1.0   \n",
       "23                  -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "222                -1.0                     -1.0                    -1.0   \n",
       "84                 -1.0                     -1.0                    -1.0   \n",
       "274                -1.0                     -1.0                    -1.0   \n",
       "16                 -1.0                     -1.0                    -1.0   \n",
       "23                 -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  mem_kb_actual   mem_kb_pred  \n",
       "222       55488        55488         -1.0          0.000 -3.304109e+09  \n",
       "84       117600       117600         -1.0        470.400 -3.304109e+09  \n",
       "274       12288        12288         -1.0          0.000 -3.304109e+09  \n",
       "16       100352       100352          1.0        401.408 -3.304106e+09  \n",
       "23        58800        58800         -1.0          0.000 -3.304109e+09  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lstm stacked mem\n",
    "cnn_mem_stacked_2, X_test, y_test = stacked_regression_model_2(cleaned, '[mem KB]')\n",
    "\n",
    "y_pred = cnn_mem_stacked_2.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "cnn_mem_stacked_2_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/brian/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/brian/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Retrieving saved model and benchmark results.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[node type]</th>\n",
       "      <th>[start]</th>\n",
       "      <th>[first]</th>\n",
       "      <th>[avg ms]</th>\n",
       "      <th>[%]</th>\n",
       "      <th>[cdf%]</th>\n",
       "      <th>[mem KB]</th>\n",
       "      <th>[times called]</th>\n",
       "      <th>[Name]</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Enter</td>\n",
       "      <td>171.058</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002%</td>\n",
       "      <td>9.435%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm5/while/TensorArrayReadV3/Enter_1</td>\n",
       "      <td>lstm5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Const</td>\n",
       "      <td>-24.271</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001%</td>\n",
       "      <td>75.558%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm2/kernel</td>\n",
       "      <td>lstm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>Switch</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.341%</td>\n",
       "      <td>88.465%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>251</td>\n",
       "      <td>lstm1/while/Switch_2</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>Enter</td>\n",
       "      <td>-24.129</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001%</td>\n",
       "      <td>75.611%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm3/while/Enter_1</td>\n",
       "      <td>lstm3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>Range</td>\n",
       "      <td>-23.890</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.005%</td>\n",
       "      <td>84.005%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm1/TensorArrayUnstack/range</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Const</td>\n",
       "      <td>190.609</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.229%</td>\n",
       "      <td>15.452%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>250</td>\n",
       "      <td>ConstantFolding/lstm5/while/split_1-folded-1</td>\n",
       "      <td>ConstantFolding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Enter</td>\n",
       "      <td>120.857</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003%</td>\n",
       "      <td>34.794%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm4/while/TensorArrayReadV3/Enter</td>\n",
       "      <td>lstm4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>Mul</td>\n",
       "      <td>46.095</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.221%</td>\n",
       "      <td>62.502%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>250</td>\n",
       "      <td>lstm2/while/Mul</td>\n",
       "      <td>lstm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Const</td>\n",
       "      <td>-24.330</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001%</td>\n",
       "      <td>75.544%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm5/bias</td>\n",
       "      <td>lstm5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Less</td>\n",
       "      <td>190.664</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.338%</td>\n",
       "      <td>17.811%</td>\n",
       "      <td>0.001</td>\n",
       "      <td>251</td>\n",
       "      <td>lstm5/while/Less_1</td>\n",
       "      <td>lstm5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>Identity</td>\n",
       "      <td>-5.152</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.242%</td>\n",
       "      <td>85.160%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>250</td>\n",
       "      <td>lstm1/while/Identity</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Const</td>\n",
       "      <td>41.708</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.233%</td>\n",
       "      <td>74.002%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>250</td>\n",
       "      <td>lstm2/while/strided_slice_1</td>\n",
       "      <td>lstm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>NextIteration</td>\n",
       "      <td>91.052</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.420%</td>\n",
       "      <td>52.040%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>250</td>\n",
       "      <td>lstm3/while/NextIteration</td>\n",
       "      <td>lstm3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>Const</td>\n",
       "      <td>-24.249</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001%</td>\n",
       "      <td>75.563%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm5/strided_slice_7/stack</td>\n",
       "      <td>lstm5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Merge</td>\n",
       "      <td>144.830</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.437%</td>\n",
       "      <td>34.792%</td>\n",
       "      <td>0.004</td>\n",
       "      <td>251</td>\n",
       "      <td>lstm4/while/Merge_3</td>\n",
       "      <td>lstm4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  [node type]  [start]  [first]  [avg ms]       [%]    [cdf%]  \\\n",
       "39                      Enter  171.058    0.006     0.004    0.002%    9.435%   \n",
       "380                     Const  -24.271    0.002     0.003    0.001%   75.558%   \n",
       "485                    Switch   -0.856    0.008     0.003    0.341%   88.465%   \n",
       "417                     Enter  -24.129    0.003     0.003    0.001%   75.611%   \n",
       "460                     Range  -23.890    0.045     0.013    0.005%   84.005%   \n",
       "63                      Const  190.609    0.003     0.002    0.229%   15.452%   \n",
       "151                     Enter  120.857    0.023     0.006    0.003%   34.794%   \n",
       "288                       Mul   46.095    0.006     0.002    0.221%   62.502%   \n",
       "362                     Const  -24.330    0.003     0.002    0.001%   75.544%   \n",
       "72                       Less  190.664    0.008     0.003    0.338%   17.811%   \n",
       "465                  Identity   -5.152    0.006     0.002    0.242%   85.160%   \n",
       "340                     Const   41.708    0.005     0.002    0.233%   74.002%   \n",
       "238             NextIteration   91.052    0.006     0.004    0.420%   52.040%   \n",
       "387                     Const  -24.249    0.002     0.002    0.001%   75.563%   \n",
       "150                     Merge  144.830    0.025     0.004    0.437%   34.792%   \n",
       "\n",
       "     [mem KB]  [times called]                                        [Name]  \\\n",
       "39      0.000               1         lstm5/while/TensorArrayReadV3/Enter_1   \n",
       "380     0.000               1                                  lstm2/kernel   \n",
       "485     0.000             251                          lstm1/while/Switch_2   \n",
       "417     0.000               1                           lstm3/while/Enter_1   \n",
       "460     1.000               1                lstm1/TensorArrayUnstack/range   \n",
       "63      0.000             250  ConstantFolding/lstm5/while/split_1-folded-1   \n",
       "151     0.000               1           lstm4/while/TensorArrayReadV3/Enter   \n",
       "288     0.000             250                               lstm2/while/Mul   \n",
       "362     0.000               1                                    lstm5/bias   \n",
       "72      0.001             251                            lstm5/while/Less_1   \n",
       "465     0.000             250                          lstm1/while/Identity   \n",
       "340     0.000             250                   lstm2/while/strided_slice_1   \n",
       "238     0.000             250                     lstm3/while/NextIteration   \n",
       "387     0.000               1                   lstm5/strided_slice_7/stack   \n",
       "150     0.004             251                           lstm4/while/Merge_3   \n",
       "\n",
       "                name  \n",
       "39             lstm5  \n",
       "380            lstm2  \n",
       "485            lstm1  \n",
       "417            lstm3  \n",
       "460            lstm1  \n",
       "63   ConstantFolding  \n",
       "151            lstm4  \n",
       "288            lstm2  \n",
       "362            lstm5  \n",
       "72             lstm5  \n",
       "465            lstm1  \n",
       "340            lstm2  \n",
       "238            lstm3  \n",
       "387            lstm5  \n",
       "150            lstm4  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lstm.summary()\n",
    "\n",
    "tf.keras.backend.clear_session() # IMPORTANT for layer names to match up.\n",
    "\n",
    "lstm = create_lstm()\n",
    "\n",
    "# tf.keras.backend.set_learning_phase(0)\n",
    "lstm_benchmark = benchmark_model(lstm)\n",
    "lstm_benchmark.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_features = get_layer_features(lstm)\n",
    "lstm_df = join_benchmark(lstm_features, lstm_benchmark)\n",
    "cleaned_lstm = clean(lstm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.003043146131333064\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.900587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>16000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.242828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.900079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  filters  kernel_size  activation_relu  activation_sigmoid  \\\n",
       "7   48.0     -1.0         -1.0                0                   0   \n",
       "1   -1.0     32.0          3.0                1                   0   \n",
       "5   64.0     -1.0         -1.0                0                   0   \n",
       "\n",
       "   activation_tanh  activation_nan  activation_selu  activation_elu  \\\n",
       "7                1               0             -1.0            -1.0   \n",
       "1                0               0             -1.0            -1.0   \n",
       "5                1               0             -1.0            -1.0   \n",
       "\n",
       "   activation_softmax  activation_softplus  activation_softsign  \\\n",
       "7                -1.0                 -1.0                 -1.0   \n",
       "1                -1.0                 -1.0                 -1.0   \n",
       "5                -1.0                 -1.0                 -1.0   \n",
       "\n",
       "   activation_hard_sigmoid  activation_exponential  activation_linear  \\\n",
       "7                     -1.0                    -1.0               -1.0   \n",
       "1                     -1.0                    -1.0               -1.0   \n",
       "5                     -1.0                    -1.0               -1.0   \n",
       "\n",
       "   input_size  output_size  stride_size  avg_ms_actual  avg_ms_pred  \n",
       "7       32000           48         -1.0          0.941     0.900587  \n",
       "1       16000        16000          1.0          0.166     0.242828  \n",
       "5        8000        16000         -1.0          0.940     0.900079  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf, X_test, y_test = rf_regression_model(cleaned_lstm, '[avg ms]')\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 363.04756968992393\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.184864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>16000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>31.712296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.448108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  filters  kernel_size  activation_relu  activation_sigmoid  \\\n",
       "7   48.0     -1.0         -1.0                0                   0   \n",
       "1   -1.0     32.0          3.0                1                   0   \n",
       "5   64.0     -1.0         -1.0                0                   0   \n",
       "\n",
       "   activation_tanh  activation_nan  activation_selu  activation_elu  \\\n",
       "7                1               0             -1.0            -1.0   \n",
       "1                0               0             -1.0            -1.0   \n",
       "5                1               0             -1.0            -1.0   \n",
       "\n",
       "   activation_softmax  activation_softplus  activation_softsign  \\\n",
       "7                -1.0                 -1.0                 -1.0   \n",
       "1                -1.0                 -1.0                 -1.0   \n",
       "5                -1.0                 -1.0                 -1.0   \n",
       "\n",
       "   activation_hard_sigmoid  activation_exponential  activation_linear  \\\n",
       "7                     -1.0                    -1.0               -1.0   \n",
       "1                     -1.0                    -1.0               -1.0   \n",
       "5                     -1.0                    -1.0               -1.0   \n",
       "\n",
       "   input_size  output_size  stride_size  mem_kb_actual  mem_kb_pred  \n",
       "7       32000           48         -1.0           32.0    25.184864  \n",
       "1       16000        16000          1.0           64.0    31.712296  \n",
       "5        8000        16000         -1.0           32.0    32.448108  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf, X_test, y_test = rf_regression_model(cleaned_lstm, '[mem KB]')\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.32148484991487053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>16000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.343861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.850315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2500</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.947790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  filters  kernel_size  activation_relu  activation_sigmoid  \\\n",
       "1   -1.0     32.0          3.0                1                   0   \n",
       "8    1.0     -1.0         -1.0                0                   1   \n",
       "4   32.0     -1.0         -1.0                0                   0   \n",
       "\n",
       "   activation_tanh  activation_nan  activation_selu  activation_elu  \\\n",
       "1                0               0             -1.0            -1.0   \n",
       "8                0               0             -1.0            -1.0   \n",
       "4                1               0             -1.0            -1.0   \n",
       "\n",
       "   activation_softmax  activation_softplus  activation_softsign  \\\n",
       "1                -1.0                 -1.0                 -1.0   \n",
       "8                -1.0                 -1.0                 -1.0   \n",
       "4                -1.0                 -1.0                 -1.0   \n",
       "\n",
       "   activation_hard_sigmoid  activation_exponential  activation_linear  \\\n",
       "1                     -1.0                    -1.0               -1.0   \n",
       "8                     -1.0                    -1.0               -1.0   \n",
       "4                     -1.0                    -1.0               -1.0   \n",
       "\n",
       "   input_size  output_size  stride_size  avg_ms_actual  avg_ms_pred  \n",
       "1       16000        16000          1.0          0.166    -0.343861  \n",
       "8          48            1         -1.0          0.011     0.850315  \n",
       "4        2500         8000         -1.0          0.941     0.947790  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lstm stacked avg ms\n",
    "rf, X_test, y_test = stacked_regression_model(cleaned_lstm, '[avg ms]')\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1780.7031965625222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>35.150173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>128.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>32000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>104.931036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>2500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>28.359949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  filters  kernel_size  activation_relu  activation_sigmoid  \\\n",
       "7   48.0     -1.0         -1.0                0                   0   \n",
       "6  128.0     -1.0         -1.0                0                   0   \n",
       "3   10.0     -1.0         -1.0                0                   0   \n",
       "\n",
       "   activation_tanh  activation_nan  activation_selu  activation_elu  \\\n",
       "7                1               0             -1.0            -1.0   \n",
       "6                1               0             -1.0            -1.0   \n",
       "3                1               0             -1.0            -1.0   \n",
       "\n",
       "   activation_softmax  activation_softplus  activation_softsign  \\\n",
       "7                -1.0                 -1.0                 -1.0   \n",
       "6                -1.0                 -1.0                 -1.0   \n",
       "3                -1.0                 -1.0                 -1.0   \n",
       "\n",
       "   activation_hard_sigmoid  activation_exponential  activation_linear  \\\n",
       "7                     -1.0                    -1.0               -1.0   \n",
       "6                     -1.0                    -1.0               -1.0   \n",
       "3                     -1.0                    -1.0               -1.0   \n",
       "\n",
       "   input_size  output_size  stride_size  mem_kb_actual  mem_kb_pred  \n",
       "7       32000           48         -1.0           32.0    35.150173  \n",
       "6       16000        32000         -1.0           32.0   104.931036  \n",
       "3        8000         2500         -1.0           32.0    28.359949  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lstm stacked mem\n",
    "rf, X_test, y_test = stacked_regression_model(cleaned_lstm, '[mem KB]')\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.6067859381992454e+16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-1.267589e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>2500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.952</td>\n",
       "      <td>-1.267531e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2500</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.941</td>\n",
       "      <td>-1.267652e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  filters  kernel_size  activation_relu  activation_sigmoid  \\\n",
       "5   64.0     -1.0         -1.0                0                   0   \n",
       "3   10.0     -1.0         -1.0                0                   0   \n",
       "4   32.0     -1.0         -1.0                0                   0   \n",
       "\n",
       "   activation_tanh  activation_nan  activation_selu  activation_elu  \\\n",
       "5                1               0             -1.0            -1.0   \n",
       "3                1               0             -1.0            -1.0   \n",
       "4                1               0             -1.0            -1.0   \n",
       "\n",
       "   activation_softmax  activation_softplus  activation_softsign  \\\n",
       "5                -1.0                 -1.0                 -1.0   \n",
       "3                -1.0                 -1.0                 -1.0   \n",
       "4                -1.0                 -1.0                 -1.0   \n",
       "\n",
       "   activation_hard_sigmoid  activation_exponential  activation_linear  \\\n",
       "5                     -1.0                    -1.0               -1.0   \n",
       "3                     -1.0                    -1.0               -1.0   \n",
       "4                     -1.0                    -1.0               -1.0   \n",
       "\n",
       "   input_size  output_size  stride_size  avg_ms_actual   avg_ms_pred  \n",
       "5        8000        16000         -1.0          0.940 -1.267589e+08  \n",
       "3        8000         2500         -1.0          0.952 -1.267531e+08  \n",
       "4        2500         8000         -1.0          0.941 -1.267652e+08  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lstm stacked avg ms\n",
    "rf, X_test, y_test = stacked_regression_model_2(cleaned_lstm, '[avg ms]')\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 23843351824817.754\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2500</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.903031e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>16000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.843128e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.902622e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  filters  kernel_size  activation_relu  activation_sigmoid  \\\n",
       "4   32.0     -1.0         -1.0                0                   0   \n",
       "1   -1.0     32.0          3.0                1                   0   \n",
       "5   64.0     -1.0         -1.0                0                   0   \n",
       "\n",
       "   activation_tanh  activation_nan  activation_selu  activation_elu  \\\n",
       "4                1               0             -1.0            -1.0   \n",
       "1                0               0             -1.0            -1.0   \n",
       "5                1               0             -1.0            -1.0   \n",
       "\n",
       "   activation_softmax  activation_softplus  activation_softsign  \\\n",
       "4                -1.0                 -1.0                 -1.0   \n",
       "1                -1.0                 -1.0                 -1.0   \n",
       "5                -1.0                 -1.0                 -1.0   \n",
       "\n",
       "   activation_hard_sigmoid  activation_exponential  activation_linear  \\\n",
       "4                     -1.0                    -1.0               -1.0   \n",
       "1                     -1.0                    -1.0               -1.0   \n",
       "5                     -1.0                    -1.0               -1.0   \n",
       "\n",
       "   input_size  output_size  stride_size  mem_kb_actual   mem_kb_pred  \n",
       "4        2500         8000         -1.0           32.0  4.903031e+06  \n",
       "1       16000        16000          1.0           64.0  4.843128e+06  \n",
       "5        8000        16000         -1.0           32.0  4.902622e+06  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lstm stacked mem\n",
    "rf, X_test, y_test = stacked_regression_model_2(cleaned_lstm, '[mem KB]')\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving saved model and benchmark results.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[node type]</th>\n",
       "      <th>[start]</th>\n",
       "      <th>[first]</th>\n",
       "      <th>[avg ms]</th>\n",
       "      <th>[%]</th>\n",
       "      <th>[cdf%]</th>\n",
       "      <th>[mem KB]</th>\n",
       "      <th>[times called]</th>\n",
       "      <th>[Name]</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Const</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.377%</td>\n",
       "      <td>2.587%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm1/strided_slice/stack</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Const</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.556%</td>\n",
       "      <td>35.641%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>ConstantFolding/lstm1/while/split-folded-0</td>\n",
       "      <td>ConstantFolding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Enter</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.757%</td>\n",
       "      <td>67.111%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm1/while/TensorArrayReadV3/Enter_1</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Enter</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.415%</td>\n",
       "      <td>6.624%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm1/while/Enter</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>MatMul</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2.926%</td>\n",
       "      <td>81.040%</td>\n",
       "      <td>1.024</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm1/while/MatMul_3</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 [node type]  [start]  [first]  [avg ms]       [%]    [cdf%]  \\\n",
       "5                      Const   -0.108    0.002     0.002    0.377%    2.587%   \n",
       "47                     Const    0.052    0.003     0.002    0.556%   35.641%   \n",
       "59                     Enter    0.007    0.009     0.003    0.757%   67.111%   \n",
       "14                     Enter   -0.083    0.002     0.002    0.415%    6.624%   \n",
       "66                    MatMul    0.041    0.016     0.012    2.926%   81.040%   \n",
       "\n",
       "    [mem KB]  [times called]                                      [Name]  \\\n",
       "5      0.000               1                   lstm1/strided_slice/stack   \n",
       "47     0.000               1  ConstantFolding/lstm1/while/split-folded-0   \n",
       "59     0.000               1       lstm1/while/TensorArrayReadV3/Enter_1   \n",
       "14     0.000               1                           lstm1/while/Enter   \n",
       "66     1.024               1                        lstm1/while/MatMul_3   \n",
       "\n",
       "               name  \n",
       "5             lstm1  \n",
       "47  ConstantFolding  \n",
       "59            lstm1  \n",
       "14            lstm1  \n",
       "66            lstm1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lstm.summary()\n",
    "\n",
    "tf.keras.backend.clear_session() # IMPORTANT for layer names to match up.\n",
    "\n",
    "enc, dec = create_seq2seq()\n",
    "\n",
    "# tf.keras.backend.set_learning_phase(0)\n",
    "enc_benchmark = benchmark_model(enc, '../tensorflow/bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=encoder.pbtxt --input_layer=\"input_1:0\" --input_layer_shape=\"1,1,71\" --output_layer=\"lstm1/while/Exit_2:0\"')\n",
    "enc_benchmark.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving saved model and benchmark results.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[node type]</th>\n",
       "      <th>[start]</th>\n",
       "      <th>[first]</th>\n",
       "      <th>[avg ms]</th>\n",
       "      <th>[%]</th>\n",
       "      <th>[cdf%]</th>\n",
       "      <th>[mem KB]</th>\n",
       "      <th>[times called]</th>\n",
       "      <th>[Name]</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Minimum</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.612%</td>\n",
       "      <td>82.161%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm2_1/while/clip_by_value_1/Minimum</td>\n",
       "      <td>lstm2_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Const</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.555%</td>\n",
       "      <td>0.943%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm2/kernel</td>\n",
       "      <td>lstm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Const</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.247%</td>\n",
       "      <td>2.685%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm2_1/TensorArrayUnstack/strided_slice/stack_1</td>\n",
       "      <td>lstm2_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Prod</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.358%</td>\n",
       "      <td>98.355%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>dense1_1/Tensordot/Prod_1</td>\n",
       "      <td>dense1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Const</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.411%</td>\n",
       "      <td>27.194%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm2_1/while/strided_slice_1</td>\n",
       "      <td>lstm2_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  [node type]  [start]  [first]  [avg ms]       [%]    [cdf%]  \\\n",
       "91                    Minimum    0.130    0.010     0.004    0.612%   82.161%   \n",
       "1                       Const   -0.121    0.009     0.004    0.555%    0.943%   \n",
       "8                       Const   -0.101    0.002     0.002    0.247%    2.685%   \n",
       "118                      Prod    0.271    0.002     0.002    0.358%   98.355%   \n",
       "49                      Const    0.044    0.003     0.003    0.411%   27.194%   \n",
       "\n",
       "     [mem KB]  [times called]  \\\n",
       "91        0.0               1   \n",
       "1         0.0               1   \n",
       "8         0.0               1   \n",
       "118       0.0               1   \n",
       "49        0.0               1   \n",
       "\n",
       "                                               [Name]      name  \n",
       "91              lstm2_1/while/clip_by_value_1/Minimum   lstm2_1  \n",
       "1                                        lstm2/kernel     lstm2  \n",
       "8    lstm2_1/TensorArrayUnstack/strided_slice/stack_1   lstm2_1  \n",
       "118                         dense1_1/Tensordot/Prod_1  dense1_1  \n",
       "49                      lstm2_1/while/strided_slice_1   lstm2_1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_benchmark = benchmark_model(dec, '../tensorflow/bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=decoder.pbtxt --input_layer=\"input_2:0,input_3:0,input_4:0\" --input_layer_shape=\"1,1,93:1,256:1,256\" --input_layer_type=float,float,float --output_layer=\"dense1_1/truediv:0\"')\n",
    "dec_benchmark.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq2seq_benchmark = pd.concat([enc_benchmark, dec_benchmark])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_features = get_layer_features(enc)\n",
    "dec_features = get_layer_features(dec)\n",
    "seq2seq_features = pd.concat([enc_features, dec_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq2seq_df = join_benchmark(seq2seq_features, seq2seq_benchmark)\n",
    "cleaned_seq2seq = clean(seq2seq_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[avg ms]</th>\n",
       "      <th>[mem KB]</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>...</th>\n",
       "      <th>filters</th>\n",
       "      <th>input_shape</th>\n",
       "      <th>input_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>name</th>\n",
       "      <th>output_shape</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>strides</th>\n",
       "      <th>units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166</td>\n",
       "      <td>64.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>(None, 500, 32)</td>\n",
       "      <td>16000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>conv1d</td>\n",
       "      <td>(None, 500, 32)</td>\n",
       "      <td>16000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 48)</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>dense</td>\n",
       "      <td>(None, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033</td>\n",
       "      <td>64.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 500)</td>\n",
       "      <td>500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>embedding</td>\n",
       "      <td>(None, 500, 32)</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.039</td>\n",
       "      <td>32.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 500, 32)</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>max_pooling1d</td>\n",
       "      <td>(None, 250, 32)</td>\n",
       "      <td>8000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.941</td>\n",
       "      <td>32.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 250, 10)</td>\n",
       "      <td>2500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>lstm2</td>\n",
       "      <td>(None, 250, 32)</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.965</td>\n",
       "      <td>32.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 250, 64)</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>lstm4</td>\n",
       "      <td>(None, 250, 128)</td>\n",
       "      <td>32000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.952</td>\n",
       "      <td>32.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 250, 32)</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>lstm1</td>\n",
       "      <td>(None, 250, 10)</td>\n",
       "      <td>2500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.940</td>\n",
       "      <td>32.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 250, 32)</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>lstm3</td>\n",
       "      <td>(None, 250, 64)</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.941</td>\n",
       "      <td>32.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 250, 128)</td>\n",
       "      <td>32000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>lstm5</td>\n",
       "      <td>(None, 48)</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.382</td>\n",
       "      <td>1.024</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, None, 71)</td>\n",
       "      <td>71</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>lstm1</td>\n",
       "      <td>[(None, 256), (None, 256), (None, 256)]</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   [avg ms]  [mem KB]  activation_elu  activation_exponential  \\\n",
       "1     0.166    64.000            -1.0                    -1.0   \n",
       "8     0.011     0.004            -1.0                    -1.0   \n",
       "0     0.033    64.000             0.0                     0.0   \n",
       "2     0.039    32.000            -1.0                    -1.0   \n",
       "4     0.941    32.000            -1.0                    -1.0   \n",
       "6     0.965    32.000            -1.0                    -1.0   \n",
       "3     0.952    32.000            -1.0                    -1.0   \n",
       "5     0.940    32.000            -1.0                    -1.0   \n",
       "7     0.941    32.000            -1.0                    -1.0   \n",
       "1     0.382     1.024            -1.0                    -1.0   \n",
       "\n",
       "   activation_hard_sigmoid  activation_linear  activation_nan  \\\n",
       "1                     -1.0               -1.0               0   \n",
       "8                     -1.0               -1.0               0   \n",
       "0                      0.0                0.0               1   \n",
       "2                     -1.0               -1.0               1   \n",
       "4                     -1.0               -1.0               0   \n",
       "6                     -1.0               -1.0               0   \n",
       "3                     -1.0               -1.0               0   \n",
       "5                     -1.0               -1.0               0   \n",
       "7                     -1.0               -1.0               0   \n",
       "1                     -1.0               -1.0               0   \n",
       "\n",
       "   activation_relu  activation_selu  activation_sigmoid  ...  filters  \\\n",
       "1              1.0             -1.0                 0.0  ...     32.0   \n",
       "8              0.0             -1.0                 1.0  ...     -1.0   \n",
       "0              0.0              0.0                 0.0  ...     -1.0   \n",
       "2              0.0             -1.0                 0.0  ...     -1.0   \n",
       "4              0.0             -1.0                 0.0  ...     -1.0   \n",
       "6              0.0             -1.0                 0.0  ...     -1.0   \n",
       "3              0.0             -1.0                 0.0  ...     -1.0   \n",
       "5              0.0             -1.0                 0.0  ...     -1.0   \n",
       "7              0.0             -1.0                 0.0  ...     -1.0   \n",
       "1             -1.0             -1.0                -1.0  ...     -1.0   \n",
       "\n",
       "        input_shape  input_size  kernel_size           name  \\\n",
       "1   (None, 500, 32)       16000          3.0         conv1d   \n",
       "8        (None, 48)          48         -1.0          dense   \n",
       "0       (None, 500)         500         -1.0      embedding   \n",
       "2   (None, 500, 32)       16000         -1.0  max_pooling1d   \n",
       "4   (None, 250, 10)        2500         -1.0          lstm2   \n",
       "6   (None, 250, 64)       16000         -1.0          lstm4   \n",
       "3   (None, 250, 32)        8000         -1.0          lstm1   \n",
       "5   (None, 250, 32)        8000         -1.0          lstm3   \n",
       "7  (None, 250, 128)       32000         -1.0          lstm5   \n",
       "1  (None, None, 71)          71         -1.0          lstm1   \n",
       "\n",
       "                              output_shape  output_size  stride_size strides  \\\n",
       "1                          (None, 500, 32)        16000          1.0    (1,)   \n",
       "8                                (None, 1)            1         -1.0      -1   \n",
       "0                          (None, 500, 32)        16000         -1.0      -1   \n",
       "2                          (None, 250, 32)         8000          2.0    (2,)   \n",
       "4                          (None, 250, 32)         8000         -1.0      -1   \n",
       "6                         (None, 250, 128)        32000         -1.0      -1   \n",
       "3                          (None, 250, 10)         2500         -1.0      -1   \n",
       "5                          (None, 250, 64)        16000         -1.0      -1   \n",
       "7                               (None, 48)           48         -1.0      -1   \n",
       "1  [(None, 256), (None, 256), (None, 256)]          768         -1.0      -1   \n",
       "\n",
       "   units  \n",
       "1   -1.0  \n",
       "8    1.0  \n",
       "0   -1.0  \n",
       "2   -1.0  \n",
       "4   32.0  \n",
       "6  128.0  \n",
       "3   10.0  \n",
       "5   64.0  \n",
       "7   48.0  \n",
       "1  256.0  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_rnn = pd.concat([cleaned_lstm, cleaned_seq2seq]).fillna(0)\n",
    "cleaned_rnn.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2941177649690035\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>filters</th>\n",
       "      <th>input_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>units</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.919003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.256547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>71</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.921677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>605</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.921677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation_elu  activation_exponential  activation_hard_sigmoid  \\\n",
       "5            -1.0                    -1.0                     -1.0   \n",
       "0             0.0                     0.0                      0.0   \n",
       "1            -1.0                    -1.0                     -1.0   \n",
       "3            -1.0                    -1.0                     -1.0   \n",
       "\n",
       "   activation_linear  activation_nan  activation_relu  activation_selu  \\\n",
       "5               -1.0               0              0.0             -1.0   \n",
       "0                0.0               1              0.0              0.0   \n",
       "1               -1.0               0             -1.0             -1.0   \n",
       "3               -1.0               0             -1.0             -1.0   \n",
       "\n",
       "   activation_sigmoid  activation_softmax  activation_softplus  \\\n",
       "5                 0.0                -1.0                 -1.0   \n",
       "0                 0.0                 0.0                  0.0   \n",
       "1                -1.0                -1.0                 -1.0   \n",
       "3                -1.0                -1.0                 -1.0   \n",
       "\n",
       "   activation_softsign  activation_tanh  filters  input_size  kernel_size  \\\n",
       "5                 -1.0                1     -1.0        8000         -1.0   \n",
       "0                  0.0                0     -1.0         500         -1.0   \n",
       "1                 -1.0                1     -1.0          71         -1.0   \n",
       "3                 -1.0                1     -1.0         605         -1.0   \n",
       "\n",
       "   output_size  stride_size  units  avg_ms_actual  avg_ms_pred  \n",
       "5        16000         -1.0   64.0          0.940     0.919003  \n",
       "0        16000         -1.0   -1.0          0.033     0.256547  \n",
       "1          768         -1.0  256.0          0.382     0.921677  \n",
       "3          768         -1.0  256.0          0.008     0.921677  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_ms_rf, X_test, y_test = rf_regression_model(cleaned_rnn, '[avg ms]')\n",
    "\n",
    "y_pred = rnn_ms_rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "rnn_ms_rf_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 585.0930507439566\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>filters</th>\n",
       "      <th>input_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>units</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.000</td>\n",
       "      <td>32.992120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.000</td>\n",
       "      <td>27.936956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>71</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1.024</td>\n",
       "      <td>23.297088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>605</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.297088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation_elu  activation_exponential  activation_hard_sigmoid  \\\n",
       "5            -1.0                    -1.0                     -1.0   \n",
       "0             0.0                     0.0                      0.0   \n",
       "1            -1.0                    -1.0                     -1.0   \n",
       "3            -1.0                    -1.0                     -1.0   \n",
       "\n",
       "   activation_linear  activation_nan  activation_relu  activation_selu  \\\n",
       "5               -1.0               0              0.0             -1.0   \n",
       "0                0.0               1              0.0              0.0   \n",
       "1               -1.0               0             -1.0             -1.0   \n",
       "3               -1.0               0             -1.0             -1.0   \n",
       "\n",
       "   activation_sigmoid  activation_softmax  activation_softplus  \\\n",
       "5                 0.0                -1.0                 -1.0   \n",
       "0                 0.0                 0.0                  0.0   \n",
       "1                -1.0                -1.0                 -1.0   \n",
       "3                -1.0                -1.0                 -1.0   \n",
       "\n",
       "   activation_softsign  activation_tanh  filters  input_size  kernel_size  \\\n",
       "5                 -1.0                1     -1.0        8000         -1.0   \n",
       "0                  0.0                0     -1.0         500         -1.0   \n",
       "1                 -1.0                1     -1.0          71         -1.0   \n",
       "3                 -1.0                1     -1.0         605         -1.0   \n",
       "\n",
       "   output_size  stride_size  units  mem_kb_actual  mem_kb_pred  \n",
       "5        16000         -1.0   64.0         32.000    32.992120  \n",
       "0        16000         -1.0   -1.0         64.000    27.936956  \n",
       "1          768         -1.0  256.0          1.024    23.297088  \n",
       "3          768         -1.0  256.0          0.000    23.297088  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_mem_rf, X_test, y_test = rf_regression_model(cleaned_rnn, '[mem KB]')\n",
    "\n",
    "y_pred = rnn_mem_rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "rnn_mem_rf_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.3017690844111648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>filters</th>\n",
       "      <th>input_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>units</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.237778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.394734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.324446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>71</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.412191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation_elu  activation_exponential  activation_hard_sigmoid  \\\n",
       "6            -1.0                    -1.0                     -1.0   \n",
       "7            -1.0                    -1.0                     -1.0   \n",
       "5            -1.0                    -1.0                     -1.0   \n",
       "1            -1.0                    -1.0                     -1.0   \n",
       "\n",
       "   activation_linear  activation_nan  activation_relu  activation_selu  \\\n",
       "6               -1.0               0              0.0             -1.0   \n",
       "7               -1.0               0              0.0             -1.0   \n",
       "5               -1.0               0              0.0             -1.0   \n",
       "1               -1.0               0             -1.0             -1.0   \n",
       "\n",
       "   activation_sigmoid  activation_softmax  activation_softplus  \\\n",
       "6                 0.0                -1.0                 -1.0   \n",
       "7                 0.0                -1.0                 -1.0   \n",
       "5                 0.0                -1.0                 -1.0   \n",
       "1                -1.0                -1.0                 -1.0   \n",
       "\n",
       "   activation_softsign  activation_tanh  filters  input_size  kernel_size  \\\n",
       "6                 -1.0                1     -1.0       16000         -1.0   \n",
       "7                 -1.0                1     -1.0       32000         -1.0   \n",
       "5                 -1.0                1     -1.0        8000         -1.0   \n",
       "1                 -1.0                1     -1.0          71         -1.0   \n",
       "\n",
       "   output_size  stride_size  units  avg_ms_actual  avg_ms_pred  \n",
       "6        32000         -1.0  128.0          0.965     0.237778  \n",
       "7           48         -1.0   48.0          0.941     0.394734  \n",
       "5        16000         -1.0   64.0          0.940     0.324446  \n",
       "1          768         -1.0  256.0          0.382     0.412191  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_ms_stacked_1, X_test, y_test = stacked_regression_model(cleaned_rnn, '[avg ms]')\n",
    "\n",
    "y_pred = rnn_ms_stacked_1.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "rnn_ms_stacked_1_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 610.3908272774386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>filters</th>\n",
       "      <th>input_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>units</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.000</td>\n",
       "      <td>36.045711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.000</td>\n",
       "      <td>27.091656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>71</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1.024</td>\n",
       "      <td>7.303139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.000</td>\n",
       "      <td>24.042503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation_elu  activation_exponential  activation_hard_sigmoid  \\\n",
       "1            -1.0                    -1.0                     -1.0   \n",
       "7            -1.0                    -1.0                     -1.0   \n",
       "1            -1.0                    -1.0                     -1.0   \n",
       "0             0.0                     0.0                      0.0   \n",
       "\n",
       "   activation_linear  activation_nan  activation_relu  activation_selu  \\\n",
       "1               -1.0               0              1.0             -1.0   \n",
       "7               -1.0               0              0.0             -1.0   \n",
       "1               -1.0               0             -1.0             -1.0   \n",
       "0                0.0               1              0.0              0.0   \n",
       "\n",
       "   activation_sigmoid  activation_softmax  activation_softplus  \\\n",
       "1                 0.0                -1.0                 -1.0   \n",
       "7                 0.0                -1.0                 -1.0   \n",
       "1                -1.0                -1.0                 -1.0   \n",
       "0                 0.0                 0.0                  0.0   \n",
       "\n",
       "   activation_softsign  activation_tanh  filters  input_size  kernel_size  \\\n",
       "1                 -1.0                0     32.0       16000          3.0   \n",
       "7                 -1.0                1     -1.0       32000         -1.0   \n",
       "1                 -1.0                1     -1.0          71         -1.0   \n",
       "0                  0.0                0     -1.0         500         -1.0   \n",
       "\n",
       "   output_size  stride_size  units  mem_kb_actual  mem_kb_pred  \n",
       "1        16000          1.0   -1.0         64.000    36.045711  \n",
       "7           48         -1.0   48.0         32.000    27.091656  \n",
       "1          768         -1.0  256.0          1.024     7.303139  \n",
       "0        16000         -1.0   -1.0         64.000    24.042503  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_mem_stacked_1, X_test, y_test = stacked_regression_model(cleaned_rnn, '[mem KB]')\n",
    "\n",
    "y_pred = rnn_mem_stacked_1.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "rnn_mem_stacked_1_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2947922495872289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>filters</th>\n",
       "      <th>input_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>units</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.598679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>605</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.631385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.610192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.600166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation_elu  activation_exponential  activation_hard_sigmoid  \\\n",
       "4            -1.0                    -1.0                     -1.0   \n",
       "3            -1.0                    -1.0                     -1.0   \n",
       "2            -1.0                    -1.0                     -1.0   \n",
       "8            -1.0                    -1.0                     -1.0   \n",
       "\n",
       "   activation_linear  activation_nan  activation_relu  activation_selu  \\\n",
       "4               -1.0               0              0.0             -1.0   \n",
       "3               -1.0               0             -1.0             -1.0   \n",
       "2               -1.0               1              0.0             -1.0   \n",
       "8               -1.0               0              0.0             -1.0   \n",
       "\n",
       "   activation_sigmoid  activation_softmax  activation_softplus  \\\n",
       "4                 0.0                -1.0                 -1.0   \n",
       "3                -1.0                -1.0                 -1.0   \n",
       "2                 0.0                -1.0                 -1.0   \n",
       "8                 1.0                -1.0                 -1.0   \n",
       "\n",
       "   activation_softsign  activation_tanh  filters  input_size  kernel_size  \\\n",
       "4                 -1.0                1     -1.0        2500         -1.0   \n",
       "3                 -1.0                1     -1.0         605         -1.0   \n",
       "2                 -1.0                0     -1.0       16000         -1.0   \n",
       "8                 -1.0                0     -1.0          48         -1.0   \n",
       "\n",
       "   output_size  stride_size  units  avg_ms_actual  avg_ms_pred  \n",
       "4         8000         -1.0   32.0          0.941     0.598679  \n",
       "3          768         -1.0  256.0          0.008     0.631385  \n",
       "2         8000          2.0   -1.0          0.039     0.610192  \n",
       "8            1         -1.0    1.0          0.011     0.600166  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_ms_stacked_2, X_test, y_test = stacked_regression_model_2(cleaned_rnn, '[avg ms]')\n",
    "\n",
    "y_pred = rnn_ms_stacked_2.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "rnn_ms_stacked_2_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 631.1786728780157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>filters</th>\n",
       "      <th>input_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>units</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.000</td>\n",
       "      <td>41.050608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.000</td>\n",
       "      <td>50.459367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>32.000</td>\n",
       "      <td>67.846760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004</td>\n",
       "      <td>19.299120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation_elu  activation_exponential  activation_hard_sigmoid  \\\n",
       "0             0.0                     0.0                      0.0   \n",
       "5            -1.0                    -1.0                     -1.0   \n",
       "6            -1.0                    -1.0                     -1.0   \n",
       "8            -1.0                    -1.0                     -1.0   \n",
       "\n",
       "   activation_linear  activation_nan  activation_relu  activation_selu  \\\n",
       "0                0.0               1              0.0              0.0   \n",
       "5               -1.0               0              0.0             -1.0   \n",
       "6               -1.0               0              0.0             -1.0   \n",
       "8               -1.0               0              0.0             -1.0   \n",
       "\n",
       "   activation_sigmoid  activation_softmax  activation_softplus  \\\n",
       "0                 0.0                 0.0                  0.0   \n",
       "5                 0.0                -1.0                 -1.0   \n",
       "6                 0.0                -1.0                 -1.0   \n",
       "8                 1.0                -1.0                 -1.0   \n",
       "\n",
       "   activation_softsign  activation_tanh  filters  input_size  kernel_size  \\\n",
       "0                  0.0                0     -1.0         500         -1.0   \n",
       "5                 -1.0                1     -1.0        8000         -1.0   \n",
       "6                 -1.0                1     -1.0       16000         -1.0   \n",
       "8                 -1.0                0     -1.0          48         -1.0   \n",
       "\n",
       "   output_size  stride_size  units  mem_kb_actual  mem_kb_pred  \n",
       "0        16000         -1.0   -1.0         64.000    41.050608  \n",
       "5        16000         -1.0   64.0         32.000    50.459367  \n",
       "6        32000         -1.0  128.0         32.000    67.846760  \n",
       "8            1         -1.0    1.0          0.004    19.299120  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_mem_stacked_2, X_test, y_test = stacked_regression_model(cleaned_rnn, '[mem KB]')\n",
    "\n",
    "y_pred = rnn_mem_stacked_2.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "rnn_mem_stacked_2_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAewAAAFlCAYAAAApldtwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_ms = [cnn_ms_rf_mse, cnn_ms_stacked_1_mse,cnn_ms_stacked_2_mse]\n",
    "plt.bar([i for i in range(len(cnn_ms))], cnn_ms, tick_label=[\n",
    "    'Random Forest',\n",
    "    'Stacked with RF',\n",
    "    'Stacked with Polynomial',\n",
    "])\n",
    "plt.title('MSE of Regression of CNN Computation Time')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "plt.xlabel('Regressor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFlCAYAAADyLnFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1AEAsAShDgCAJQh1AAAs8f9WZNNXYl+w4QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnn_ms = [rnn_ms_rf_mse, rnn_ms_stacked_1_mse]#, rnn_ms_stacked_2_mse]\n",
    "\n",
    "plt.bar([i for i in range(len(rnn_ms))], rnn_ms, tick_label=[\n",
    "    'Random Forest',\n",
    "    'Stacked with RF',\n",
    "#     'Stacked with Polynomial',\n",
    "])\n",
    "plt.title('MSE of Regression of RNN Computation Time')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "plt.xlabel('Regressor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAggAAAFlCAYAAACOfhB6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_mem = [cnn_mem_rf_mse, cnn_mem_stacked_1_mse]\n",
    "plt.bar(range(len(cnn_mem)), cnn_mem, tick_label=[\"Random Forest\", \"Stacked with RF\"])\n",
    "\n",
    "plt.title('MSE of Regression of CNN Memory Usage')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Regressor')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFlCAYAAAAQ8morAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnn_mem = [rnn_mem_rf_mse, rnn_mem_stacked_1_mse, rnn_mem_stacked_2_mse]\n",
    "\n",
    "plt.bar(range(len(rnn_mem)), rnn_mem, tick_label=[\n",
    "    \"Random Forest\",\n",
    "    \"Stacked with RF\",\n",
    "    \"Stacked with Polynomial\",\n",
    "])\n",
    "\n",
    "plt.title('MSE of Regression of RNN Memory Usage')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Regressor')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_cnn_ms(model):\n",
    "    \n",
    "    layer_features = get_layer_features(model)\n",
    "    cleaned_features = clean(layer_features, inference=True)\n",
    "    \n",
    "    X = cleaned_features.drop(['name', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "    \n",
    "    predicted = cnn_ms_rf.predict(X)\n",
    "    \n",
    "    return predicted\n",
    "\n",
    "def predict_cnn_mem(model):\n",
    "    \n",
    "    layer_features = get_layer_features(model)\n",
    "    cleaned_features = clean(layer_features, inference=True)\n",
    "    \n",
    "    X = cleaned_features.drop(['name', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "    \n",
    "    predicted = cnn_mem_rf.predict(X)\n",
    "    \n",
    "    return predicted\n",
    "\n",
    "def predict_rnn_ms(model):\n",
    "    \n",
    "    layer_features = get_layer_features(model)\n",
    "    cleaned_features = clean(layer_features, inference=True)\n",
    "    \n",
    "    X = cleaned_features.drop(['name', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "    \n",
    "    predicted = rnn_ms_rf.predict(X)\n",
    "    \n",
    "    return predicted\n",
    "\n",
    "def predict_rnn_mem(model):\n",
    "    \n",
    "    layer_features = get_layer_features(model)\n",
    "    cleaned_features = clean(layer_features, inference=True)\n",
    "    \n",
    "    X = cleaned_features.drop(['name', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "    \n",
    "    predicted = rnn_mem_rf.predict(X)\n",
    "    \n",
    "    return predicted\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.36353538e-01, 2.36353538e-01, 7.42407467e-01, 1.35628300e+00,\n",
       "       4.46846130e+00, 1.35197900e+00, 1.74586864e+00, 5.04323484e-01,\n",
       "       2.55629047e-01, 1.47515977e+00, 4.86585900e-01, 2.55629047e-01,\n",
       "       1.47515977e+00, 1.20133403e+00, 1.20133403e+00, 1.35628300e+00,\n",
       "       1.35628300e+00, 3.12488300e+00, 4.46846130e+00, 2.70438022e+00,\n",
       "       2.55629047e-01, 1.47515977e+00, 4.86585900e-01, 2.55629047e-01,\n",
       "       1.47515977e+00, 1.20133403e+00, 1.35628300e+00, 3.12488300e+00,\n",
       "       4.46846130e+00, 2.70438022e+00, 2.55629047e-01, 1.47515977e+00,\n",
       "       4.86585900e-01, 2.55629047e-01, 1.47515977e+00, 1.20133403e+00,\n",
       "       1.35628300e+00, 3.12488300e+00, 4.46846130e+00, 2.86571840e+00,\n",
       "       6.65594775e-02, 1.42459369e+00, 8.60290160e-02, 6.65594775e-02,\n",
       "       1.42459369e+00, 7.10492832e+00, 1.14343720e+01, 1.89940684e-01,\n",
       "       1.89940684e-01, 1.16728879e+00, 1.24703656e+00, 2.60023692e-01,\n",
       "       6.65594775e-02, 1.42459369e+00, 8.60290160e-02, 6.65594775e-02,\n",
       "       1.42459369e+00, 7.10492832e+00, 1.89940684e-01, 1.16728879e+00,\n",
       "       1.24703656e+00, 2.60023692e-01, 6.65594775e-02, 1.42459369e+00,\n",
       "       8.60290160e-02, 6.65594775e-02, 1.42459369e+00, 7.10492832e+00,\n",
       "       1.89940684e-01, 1.16728879e+00, 1.24703656e+00, 2.60023692e-01,\n",
       "       6.65594775e-02, 1.42459369e+00, 8.60290160e-02, 6.65594775e-02,\n",
       "       1.42459369e+00, 7.10492832e+00, 1.89940684e-01, 1.16728879e+00,\n",
       "       1.24703656e+00, 3.01941884e-01, 6.00910866e-03, 8.53728027e-01,\n",
       "       2.52508947e-03, 6.00910866e-03, 8.53728027e-01, 5.09112797e+00,\n",
       "       9.68870979e+00, 2.55629047e-01, 2.55629047e-01, 2.85702845e-01,\n",
       "       1.47515977e+00, 9.74374053e-03, 6.00910866e-03, 8.53728027e-01,\n",
       "       2.52508947e-03, 6.00910866e-03, 8.53728027e-01, 5.09112797e+00,\n",
       "       2.55629047e-01, 2.85702845e-01, 1.47515977e+00, 9.74374053e-03,\n",
       "       6.00910866e-03, 8.53728027e-01, 2.52508947e-03, 6.00910866e-03,\n",
       "       8.53728027e-01, 5.09112797e+00, 2.55629047e-01, 2.85702845e-01,\n",
       "       1.47515977e+00, 9.74374053e-03, 6.00910866e-03, 8.53728027e-01,\n",
       "       2.52508947e-03, 6.00910866e-03, 8.53728027e-01, 5.09112797e+00,\n",
       "       2.55629047e-01, 2.85702845e-01, 1.47515977e+00, 9.74374053e-03,\n",
       "       6.00910866e-03, 8.53728027e-01, 2.52508947e-03, 6.00910866e-03,\n",
       "       8.53728027e-01, 5.09112797e+00, 2.55629047e-01, 2.85702845e-01,\n",
       "       1.47515977e+00, 9.74374053e-03, 6.00910866e-03, 8.53728027e-01,\n",
       "       2.52508947e-03, 6.00910866e-03, 8.53728027e-01, 5.09112797e+00,\n",
       "       2.55629047e-01, 2.85702845e-01, 1.47515977e+00, 5.41553876e+00,\n",
       "       3.01489281e-02, 8.78406013e-01, 4.97629271e+00, 3.01489281e-02,\n",
       "       8.78406013e-01, 4.99669227e+00, 5.45222057e+00, 6.65594775e-02,\n",
       "       6.65594775e-02, 1.02280348e-01, 1.42459369e+00, 4.95787602e+00,\n",
       "       3.01489281e-02, 8.78406013e-01, 4.97629271e+00, 3.01489281e-02,\n",
       "       8.78406013e-01, 4.99669227e+00, 6.65594775e-02, 1.02280348e-01,\n",
       "       1.42459369e+00, 4.95787602e+00, 3.01489281e-02, 8.78406013e-01,\n",
       "       4.97629271e+00, 3.01489281e-02, 8.78406013e-01, 4.99669227e+00,\n",
       "       6.65594775e-02, 1.02280348e-01, 1.42459369e+00, 1.29132322e+00,\n",
       "       1.73822969e+00])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = tf.keras.applications.resnet50.ResNet50(\n",
    "    include_top=True,\n",
    "    weights='imagenet')\n",
    "\n",
    "predict_cnn_ms(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.58366976e+02, 4.58366976e+02, 5.07565568e+02, 4.30150784e+02,\n",
       "       2.86710982e+03, 4.28269184e+02, 1.13164730e+03, 8.55955904e+02,\n",
       "       8.89117568e+02, 1.67417798e+03, 8.56031936e+02, 8.89117568e+02,\n",
       "       1.67417798e+03, 3.68758656e+02, 3.68758656e+02, 4.30150784e+02,\n",
       "       4.30150784e+02, 8.79883008e+02, 2.86710982e+03, 1.03390592e+03,\n",
       "       8.89117568e+02, 1.67417798e+03, 8.56031936e+02, 8.89117568e+02,\n",
       "       1.67417798e+03, 3.68758656e+02, 4.30150784e+02, 8.79883008e+02,\n",
       "       2.86710982e+03, 1.03390592e+03, 8.89117568e+02, 1.67417798e+03,\n",
       "       8.56031936e+02, 8.89117568e+02, 1.67417798e+03, 3.68758656e+02,\n",
       "       4.30150784e+02, 8.79883008e+02, 2.86710982e+03, 5.25025984e+02,\n",
       "       8.81678080e+01, 4.38814720e+02, 7.32794880e+01, 8.81678080e+01,\n",
       "       4.38814720e+02, 5.93681408e+02, 9.47827456e+02, 5.74959680e+02,\n",
       "       5.74959680e+02, 6.31324800e+02, 1.76282426e+03, 1.91770176e+02,\n",
       "       8.81678080e+01, 4.38814720e+02, 7.32794880e+01, 8.81678080e+01,\n",
       "       4.38814720e+02, 5.93681408e+02, 5.74959680e+02, 6.31324800e+02,\n",
       "       1.76282426e+03, 1.91770176e+02, 8.81678080e+01, 4.38814720e+02,\n",
       "       7.32794880e+01, 8.81678080e+01, 4.38814720e+02, 5.93681408e+02,\n",
       "       5.74959680e+02, 6.31324800e+02, 1.76282426e+03, 1.91770176e+02,\n",
       "       8.81678080e+01, 4.38814720e+02, 7.32794880e+01, 8.81678080e+01,\n",
       "       4.38814720e+02, 5.93681408e+02, 5.74959680e+02, 6.31324800e+02,\n",
       "       1.76282426e+03, 2.00086592e+02, 0.00000000e+00, 1.84960000e+02,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.84960000e+02, 8.17317184e+02,\n",
       "       1.00480858e+03, 8.89117568e+02, 8.89117568e+02, 8.78668544e+02,\n",
       "       1.67417798e+03, 0.00000000e+00, 0.00000000e+00, 1.84960000e+02,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.84960000e+02, 8.17317184e+02,\n",
       "       8.89117568e+02, 8.78668544e+02, 1.67417798e+03, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.84960000e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.84960000e+02, 8.17317184e+02, 8.89117568e+02, 8.78668544e+02,\n",
       "       1.67417798e+03, 0.00000000e+00, 0.00000000e+00, 1.84960000e+02,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.84960000e+02, 8.17317184e+02,\n",
       "       8.89117568e+02, 8.78668544e+02, 1.67417798e+03, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.84960000e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.84960000e+02, 8.17317184e+02, 8.89117568e+02, 8.78668544e+02,\n",
       "       1.67417798e+03, 0.00000000e+00, 0.00000000e+00, 1.84960000e+02,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.84960000e+02, 8.17317184e+02,\n",
       "       8.89117568e+02, 8.78668544e+02, 1.67417798e+03, 7.51539200e+01,\n",
       "       1.00480000e-02, 9.77305600e+01, 5.62971200e+00, 1.00480000e-02,\n",
       "       9.77305600e+01, 8.87514240e+01, 1.89005568e+02, 8.81678080e+01,\n",
       "       8.81678080e+01, 1.13683968e+02, 4.38814720e+02, 5.61974400e+00,\n",
       "       1.00480000e-02, 9.77305600e+01, 5.62971200e+00, 1.00480000e-02,\n",
       "       9.77305600e+01, 8.87514240e+01, 8.81678080e+01, 1.13683968e+02,\n",
       "       4.38814720e+02, 5.61974400e+00, 1.00480000e-02, 9.77305600e+01,\n",
       "       5.62971200e+00, 1.00480000e-02, 9.77305600e+01, 8.87514240e+01,\n",
       "       8.81678080e+01, 1.13683968e+02, 4.38814720e+02, 5.89724800e+00,\n",
       "       4.58342400e+00])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_cnn_mem(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/brian/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/brian/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/brian/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.130042, 0.145918, 0.130042, 0.148489])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(layers.Embedding(max_features, 128, input_length=maxlen))\n",
    "rnn_model.add(layers.Bidirectional(layers.LSTM(64)))\n",
    "rnn_model.add(layers.Dropout(0.5))\n",
    "rnn_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "rnn_model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "predict_rnn_ms(rnn_model)\n",
    "\n",
    "# get_layer_features(rnn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_rnn_mem(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean(get_layer_features(test_model), inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(predict_cnn_ms, 'cnn_cpu.joblib') \n",
    "dump(predict_cnn_mem, 'cnn_mem.joblib') \n",
    "\n",
    "dump(predict_rnn_ms, 'rnn_cpu.joblib') \n",
    "dump(predict_rnn_mem, 'rnn_mem.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
