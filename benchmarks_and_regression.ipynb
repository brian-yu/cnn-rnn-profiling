{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "from tensorflow.compat.v1.keras import layers\n",
    "import tensorflow.compat.v1 as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tf_graph_util import convert_variables_to_constants\n",
    "from lstm import create_lstm\n",
    "\n",
    "from seq2seq import create_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def benchmark_model(model, cmd=None):\n",
    "    bench_path = f\"{model.name}_benchmark.txt\"\n",
    "    if not os.path.exists(f\"{model.name}.pbtxt\") and not os.path.exists(bench_path):\n",
    "        if not os.path.exists(f\"{model.name}.pbtxt\"):\n",
    "            print(\"Saving model...\")\n",
    "            tf.keras.backend.clear_session()\n",
    "            sess = tf.keras.backend.get_session()\n",
    "    #         output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            output_graph_def = convert_variables_to_constants(\n",
    "                sess,\n",
    "                sess.graph.as_graph_def(),\n",
    "                [node.op.name for node in model.outputs])\n",
    "            tf.io.write_graph(output_graph_def, './', f'{model.name}.pbtxt')\n",
    "        else:\n",
    "            print(\"Retrieving saved model.\")\n",
    "    \n",
    "    \n",
    "        if not os.path.exists(bench_path):\n",
    "            if not cmd:\n",
    "                input_shape = f\"1,{','.join(str(dim) for dim in model.input.shape[1:])}\"\n",
    "                cmd = f'../tensorflow/bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph={model.name}.pbtxt --input_layer=\"{model.input.name}\" --input_layer_shape=\"{input_shape}\" --output_layer=\"{model.output.name}\"'\n",
    "                print(cmd)\n",
    "            print(\"Running benchmark...\")\n",
    "            benchmark = subprocess.run([cmd], stderr=subprocess.PIPE, shell=True)\n",
    "            print(\"Done.\")\n",
    "\n",
    "            output = benchmark.stderr.decode('unicode_escape')\n",
    "            split_output = output[output.find('Run Order'):output.find('Top by Computation Time')].split('\\n')\n",
    "\n",
    "            with open(bench_path, 'w') as f:\n",
    "                f.write(\"\\n\".join(split_output[1:-2]))\n",
    "        else:\n",
    "            print(\"Retrieving saved benchmark results.\")\n",
    "    else:\n",
    "        print(\"Retrieving saved model and benchmark results.\")\n",
    "    \n",
    "    f = open(bench_path)\n",
    "    benchmark = pd.read_csv(f, sep=\"\\t\").rename(columns=lambda x: x.strip())\n",
    "    benchmark = benchmark.drop(benchmark.columns[0], axis=1)\n",
    "    benchmark['name'] = benchmark['[Name]'].apply(lambda x: x.split('/')[0])\n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_layer_features(model):\n",
    "    layers = pd.DataFrame()\n",
    "    layers['name'] = pd.Series([layer.name for layer in model.layers])\n",
    "    \n",
    "#     input_dims = {layer.name: [dim.value for dim in layer.input.shape.dims] for layer in model.layers}\n",
    "    \n",
    "#     layers['input_shape'] = pd.Series([[dim.value for dim in layer.input.shape.dims] for layer in model.layers])\n",
    "#     layers['output_shape'] = pd.Series([[dim.value for dim in layer.output.shape.dims] for layer in model.layers])\n",
    "    layers['input_shape'] = pd.Series([layer.input_shape for layer in model.layers])\n",
    "    layers['output_shape'] = pd.Series([layer.output_shape for layer in model.layers])\n",
    "\n",
    "    features = ['units','filters','activation','strides','kernel_size']\n",
    "    for feature in features:\n",
    "        layers[feature] = pd.Series(\n",
    "            [layer.get_config()[feature] if feature in layer.get_config() else None for layer in model.layers])\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_benchmark(features, benchmark):\n",
    "    speed = benchmark[['name', '[avg ms]']].groupby('name').sum()\n",
    "    mem = benchmark[['name', '[mem KB]']].groupby('name').max()\n",
    "    \n",
    "    return features.join(speed, on='name').join(mem, on='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def flatten_shape(shape):\n",
    "    if not shape:\n",
    "        return None\n",
    "    \n",
    "    def reduce(tup):\n",
    "        acc = 1\n",
    "        for val in tup:\n",
    "            if val:\n",
    "                acc *= val\n",
    "        return acc\n",
    "    \n",
    "    if isinstance(shape, list):\n",
    "        return sum(reduce(tup) for tup in shape)\n",
    "    \n",
    "    return reduce(shape)\n",
    "\n",
    "\n",
    "def clean(data, inference=False):\n",
    "    if inference:\n",
    "        cleaned = pd.get_dummies(data, columns=['activation'], dummy_na=True)\n",
    "    else:\n",
    "        cleaned = pd.get_dummies(\n",
    "            data.dropna(subset=['[avg ms]', '[mem KB]']), columns=['activation'], dummy_na=True)\n",
    "    \n",
    "    for activation in ['selu', 'elu', 'softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'exponential', 'linear']:\n",
    "        col = f\"activation_{activation}\"\n",
    "        if col not in cleaned.columns:\n",
    "            cleaned[col] = pd.Series(0)\n",
    "    \n",
    "\n",
    "    cleaned['input_size'] = cleaned['input_shape'].apply(flatten_shape)\n",
    "    cleaned['output_size'] = cleaned['output_shape'].apply(flatten_shape)\n",
    "    cleaned['stride_size'] = cleaned['strides'].apply(flatten_shape)\n",
    "    cleaned['kernel_size'] = cleaned['kernel_size'].apply(flatten_shape)\n",
    "\n",
    "    return cleaned.fillna(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rf_regression_model(data, column):\n",
    "    X = data.drop(['name', '[avg ms]', '[mem KB]', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "    y = data[column]  # Labels\n",
    "\n",
    "    # Split dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state = RANDOM_SEED) # 70% training and 30% test\n",
    "\n",
    "    # Instantiate model with 1000 decision trees\n",
    "    rf = RandomForestRegressor(n_estimators = 1000, random_state = RANDOM_SEED)\n",
    "    # Train the model on training data\n",
    "    rf.fit(X_train, y_train);\n",
    "\n",
    "    return rf, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "def stacked_regression_model(data, column):\n",
    "\n",
    "    X = data.drop(['name', '[avg ms]', '[mem KB]', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "    y = data[column]  # Labels\n",
    "\n",
    "    estimators = [\n",
    "        ('rf', RandomForestRegressor(n_estimators=1000, random_state=RANDOM_SEED)),\n",
    "        ('lr', RidgeCV()),\n",
    "        ('svr', LinearSVR(random_state=42)),\n",
    "        \n",
    "    ]\n",
    "    reg = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "#         final_estimator=RandomForestRegressor(n_estimators=1000, random_state=RANDOM_SEED)\n",
    "    )\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    \n",
    "    reg.fit(X_train, y_train).score(X_test, y_test)\n",
    "    \n",
    "    return reg, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def stacked_regression_model_2(data, column):\n",
    "\n",
    "    X = data.drop(['name', '[avg ms]', '[mem KB]', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "    y = data[column]  # Labels\n",
    "    \n",
    "    poly = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=5)),\n",
    "        ('linear', LinearRegression(fit_intercept=False))\n",
    "    ])\n",
    "\n",
    "    estimators = [\n",
    "        ('poly', poly),\n",
    "        ('lr', RidgeCV()),\n",
    "        ('svr', LinearSVR(random_state=42)),\n",
    "        \n",
    "    ]\n",
    "    reg = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "#         final_estimator=RandomForestRegressor(n_estimators=1000, random_state=RANDOM_SEED)\n",
    "    )\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    \n",
    "    reg.fit(X_train, y_train).score(X_test, y_test)\n",
    "    \n",
    "    return reg, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() # IMPORTANT for layer names to match up.\n",
    "model = tf.keras.applications.vgg16.VGG16(\n",
    "    include_top=True,\n",
    "    weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving saved model and benchmark results.\n"
     ]
    }
   ],
   "source": [
    "benchmark = benchmark_model(model)\n",
    "features = get_layer_features(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_df = join_benchmark(features, benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving saved model and benchmark results.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session() # IMPORTANT for layer names to match up.\n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "inception = tf. keras.applications.inception_v3.InceptionV3(\n",
    "    include_top=True, weights='imagenet')\n",
    "inception_benchmark = benchmark_model(inception)\n",
    "inception_features = get_layer_features(inception)\n",
    "inception_df = join_benchmark(inception_features, inception_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # lstm.summary()\n",
    "\n",
    "# tf.keras.backend.clear_session() # IMPORTANT for layer names to match up.\n",
    "\n",
    "# lstm = create_lstm()\n",
    "\n",
    "# # tf.keras.backend.set_learning_phase(0)\n",
    "# lstm_benchmark = benchmark_model(lstm)\n",
    "# lstm_features = get_layer_features(lstm)\n",
    "# lstm_df = join_benchmark(lstm_features, lstm_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>input_shape</th>\n",
       "      <th>output_shape</th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>activation</th>\n",
       "      <th>strides</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>[avg ms]</th>\n",
       "      <th>[mem KB]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>batch_normalization_71</td>\n",
       "      <td>(None, 8, 8, 320)</td>\n",
       "      <td>(None, 8, 8, 320)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>batch_normalization_59</td>\n",
       "      <td>(None, 17, 17, 192)</td>\n",
       "      <td>(None, 17, 17, 192)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>activation_2</td>\n",
       "      <td>(None, 147, 147, 64)</td>\n",
       "      <td>(None, 147, 147, 64)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4.014</td>\n",
       "      <td>5531.904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>activation_36</td>\n",
       "      <td>(None, 17, 17, 128)</td>\n",
       "      <td>(None, 17, 17, 128)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.671</td>\n",
       "      <td>147.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>batch_normalization_7</td>\n",
       "      <td>(None, 35, 35, 64)</td>\n",
       "      <td>(None, 35, 35, 64)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>activation_73</td>\n",
       "      <td>(None, 17, 17, 192)</td>\n",
       "      <td>(None, 17, 17, 192)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.851</td>\n",
       "      <td>221.952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>conv2d_6</td>\n",
       "      <td>(None, 35, 35, 192)</td>\n",
       "      <td>(None, 35, 35, 48)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.0</td>\n",
       "      <td>linear</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>conv2d_11</td>\n",
       "      <td>(None, 35, 35, 192)</td>\n",
       "      <td>(None, 35, 35, 32)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>linear</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>conv2d_86</td>\n",
       "      <td>(None, 8, 8, 2048)</td>\n",
       "      <td>(None, 8, 8, 384)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>384.0</td>\n",
       "      <td>linear</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>conv2d_41</td>\n",
       "      <td>(None, 17, 17, 768)</td>\n",
       "      <td>(None, 17, 17, 160)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.0</td>\n",
       "      <td>linear</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name           input_shape          output_shape  \\\n",
       "243  batch_normalization_71     (None, 8, 8, 320)     (None, 8, 8, 320)   \n",
       "191  batch_normalization_59   (None, 17, 17, 192)   (None, 17, 17, 192)   \n",
       "9              activation_2  (None, 147, 147, 64)  (None, 147, 147, 64)   \n",
       "112           activation_36   (None, 17, 17, 128)   (None, 17, 17, 128)   \n",
       "33    batch_normalization_7    (None, 35, 35, 64)    (None, 35, 35, 64)   \n",
       "234           activation_73   (None, 17, 17, 192)   (None, 17, 17, 192)   \n",
       "21                 conv2d_6   (None, 35, 35, 192)    (None, 35, 35, 48)   \n",
       "31                conv2d_11   (None, 35, 35, 192)    (None, 35, 35, 32)   \n",
       "283               conv2d_86    (None, 8, 8, 2048)     (None, 8, 8, 384)   \n",
       "139               conv2d_41   (None, 17, 17, 768)   (None, 17, 17, 160)   \n",
       "\n",
       "     units  filters activation strides kernel_size  [avg ms]  [mem KB]  \n",
       "243    NaN      NaN       None    None        None     0.006     0.000  \n",
       "191    NaN      NaN       None    None        None     0.005     0.000  \n",
       "9      NaN      NaN       relu    None        None     4.014  5531.904  \n",
       "112    NaN      NaN       relu    None        None     0.671   147.968  \n",
       "33     NaN      NaN       None    None        None     0.004     0.000  \n",
       "234    NaN      NaN       relu    None        None     0.851   221.952  \n",
       "21     NaN     48.0     linear  (1, 1)      (1, 1)     0.002     0.000  \n",
       "31     NaN     32.0     linear  (1, 1)      (1, 1)     0.002     0.000  \n",
       "283    NaN    384.0     linear  (1, 1)      (1, 1)     0.002     0.000  \n",
       "139    NaN    160.0     linear  (1, 1)      (1, 1)     0.002     0.000  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([vgg_df, inception_df])\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>input_shape</th>\n",
       "      <th>output_shape</th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>strides</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>[avg ms]</th>\n",
       "      <th>[mem KB]</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>...</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>block1_conv1</td>\n",
       "      <td>(None, 224, 224, 3)</td>\n",
       "      <td>(None, 224, 224, 64)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.657</td>\n",
       "      <td>12845.056</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>150528</td>\n",
       "      <td>3211264</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>block1_conv2</td>\n",
       "      <td>(None, 224, 224, 64)</td>\n",
       "      <td>(None, 224, 224, 64)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.123</td>\n",
       "      <td>12845.056</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3211264</td>\n",
       "      <td>3211264</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>block1_pool</td>\n",
       "      <td>(None, 224, 224, 64)</td>\n",
       "      <td>(None, 112, 112, 64)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.437</td>\n",
       "      <td>3211.264</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3211264</td>\n",
       "      <td>802816</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>block2_conv1</td>\n",
       "      <td>(None, 112, 112, 64)</td>\n",
       "      <td>(None, 112, 112, 128)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.181</td>\n",
       "      <td>6422.528</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>802816</td>\n",
       "      <td>1605632</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>block2_conv2</td>\n",
       "      <td>(None, 112, 112, 128)</td>\n",
       "      <td>(None, 112, 112, 128)</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.004</td>\n",
       "      <td>6422.528</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1605632</td>\n",
       "      <td>1605632</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           name            input_shape           output_shape  units  filters  \\\n",
       "1  block1_conv1    (None, 224, 224, 3)   (None, 224, 224, 64)   -1.0     64.0   \n",
       "2  block1_conv2   (None, 224, 224, 64)   (None, 224, 224, 64)   -1.0     64.0   \n",
       "3   block1_pool   (None, 224, 224, 64)   (None, 112, 112, 64)   -1.0     -1.0   \n",
       "4  block2_conv1   (None, 112, 112, 64)  (None, 112, 112, 128)   -1.0    128.0   \n",
       "5  block2_conv2  (None, 112, 112, 128)  (None, 112, 112, 128)   -1.0    128.0   \n",
       "\n",
       "  strides  kernel_size  [avg ms]   [mem KB]  activation_linear  ...  \\\n",
       "1  (1, 1)          9.0     2.657  12845.056                  0  ...   \n",
       "2  (1, 1)          9.0    18.123  12845.056                  0  ...   \n",
       "3  (2, 2)         -1.0     3.437   3211.264                  0  ...   \n",
       "4  (1, 1)          9.0     7.181   6422.528                  0  ...   \n",
       "5  (1, 1)          9.0    13.004   6422.528                  0  ...   \n",
       "\n",
       "   activation_elu  activation_softplus  activation_softsign  activation_tanh  \\\n",
       "1            -1.0                 -1.0                 -1.0             -1.0   \n",
       "2            -1.0                 -1.0                 -1.0             -1.0   \n",
       "3            -1.0                 -1.0                 -1.0             -1.0   \n",
       "4            -1.0                 -1.0                 -1.0             -1.0   \n",
       "5            -1.0                 -1.0                 -1.0             -1.0   \n",
       "\n",
       "   activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "1                -1.0                     -1.0                    -1.0   \n",
       "2                -1.0                     -1.0                    -1.0   \n",
       "3                -1.0                     -1.0                    -1.0   \n",
       "4                -1.0                     -1.0                    -1.0   \n",
       "5                -1.0                     -1.0                    -1.0   \n",
       "\n",
       "   input_size  output_size  stride_size  \n",
       "1      150528      3211264          1.0  \n",
       "2     3211264      3211264          1.0  \n",
       "3     3211264       802816          4.0  \n",
       "4      802816      1605632          1.0  \n",
       "5     1605632      1605632          1.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned = clean(data)\n",
    "\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.7300811553189278\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>28672</td>\n",
       "      <td>28672</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.007771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.154</td>\n",
       "      <td>0.853289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20480</td>\n",
       "      <td>20480</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.050</td>\n",
       "      <td>1.142880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>81920</td>\n",
       "      <td>12288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>58800</td>\n",
       "      <td>58800</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.598330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>235200</td>\n",
       "      <td>78400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>36992</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1605632</td>\n",
       "      <td>401408</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.788</td>\n",
       "      <td>5.010697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>313600</td>\n",
       "      <td>78400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.976136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.853289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>36992</td>\n",
       "      <td>36992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.150</td>\n",
       "      <td>0.853289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>46240</td>\n",
       "      <td>46240</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.031</td>\n",
       "      <td>0.791480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "281   -1.0     -1.0         -1.0                  0                0   \n",
       "202   -1.0     -1.0         -1.0                  0                1   \n",
       "275   -1.0     -1.0         -1.0                  0                1   \n",
       "268   -1.0    192.0          1.0                  1                0   \n",
       "25    -1.0     -1.0         -1.0                  0                1   \n",
       "18    -1.0     64.0          1.0                  1                0   \n",
       "122   -1.0    192.0          7.0                  1                0   \n",
       "6     -1.0     -1.0         -1.0                  0                0   \n",
       "123   -1.0    192.0          1.0                  1                0   \n",
       "51    -1.0     64.0          1.0                  1                0   \n",
       "89    -1.0     -1.0         -1.0                  0                1   \n",
       "193   -1.0     -1.0         -1.0                  0                1   \n",
       "223   -1.0     -1.0         -1.0                  0                0   \n",
       "104   -1.0    128.0          7.0                  1                0   \n",
       "224   -1.0     -1.0         -1.0                  0                1   \n",
       "238   -1.0     -1.0         -1.0                  0                0   \n",
       "143   -1.0     -1.0         -1.0                  0                1   \n",
       "296   -1.0     -1.0         -1.0                  0                0   \n",
       "204   -1.0    192.0          7.0                  1                0   \n",
       "260   -1.0    384.0          3.0                  1                0   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "281                   0               1             -1.0            -1.0   \n",
       "202                   0               0             -1.0            -1.0   \n",
       "275                   0               0             -1.0            -1.0   \n",
       "268                   0               0             -1.0            -1.0   \n",
       "25                    0               0             -1.0            -1.0   \n",
       "18                    0               0             -1.0            -1.0   \n",
       "122                   0               0             -1.0            -1.0   \n",
       "6                     0               1             -1.0            -1.0   \n",
       "123                   0               0             -1.0            -1.0   \n",
       "51                    0               0             -1.0            -1.0   \n",
       "89                    0               0             -1.0            -1.0   \n",
       "193                   0               0             -1.0            -1.0   \n",
       "223                   0               1             -1.0            -1.0   \n",
       "104                   0               0             -1.0            -1.0   \n",
       "224                   0               0             -1.0            -1.0   \n",
       "238                   0               1             -1.0            -1.0   \n",
       "143                   0               0             -1.0            -1.0   \n",
       "296                   0               1             -1.0            -1.0   \n",
       "204                   0               0             -1.0            -1.0   \n",
       "260                   0               0             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "281                 -1.0                 -1.0             -1.0   \n",
       "202                 -1.0                 -1.0             -1.0   \n",
       "275                 -1.0                 -1.0             -1.0   \n",
       "268                 -1.0                 -1.0             -1.0   \n",
       "25                  -1.0                 -1.0             -1.0   \n",
       "18                  -1.0                 -1.0             -1.0   \n",
       "122                 -1.0                 -1.0             -1.0   \n",
       "6                   -1.0                 -1.0             -1.0   \n",
       "123                 -1.0                 -1.0             -1.0   \n",
       "51                  -1.0                 -1.0             -1.0   \n",
       "89                  -1.0                 -1.0             -1.0   \n",
       "193                 -1.0                 -1.0             -1.0   \n",
       "223                 -1.0                 -1.0             -1.0   \n",
       "104                 -1.0                 -1.0             -1.0   \n",
       "224                 -1.0                 -1.0             -1.0   \n",
       "238                 -1.0                 -1.0             -1.0   \n",
       "143                 -1.0                 -1.0             -1.0   \n",
       "296                 -1.0                 -1.0             -1.0   \n",
       "204                 -1.0                 -1.0             -1.0   \n",
       "260                 -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "281                -1.0                     -1.0                    -1.0   \n",
       "202                -1.0                     -1.0                    -1.0   \n",
       "275                -1.0                     -1.0                    -1.0   \n",
       "268                -1.0                     -1.0                    -1.0   \n",
       "25                 -1.0                     -1.0                    -1.0   \n",
       "18                 -1.0                     -1.0                    -1.0   \n",
       "122                -1.0                     -1.0                    -1.0   \n",
       "6                  -1.0                     -1.0                    -1.0   \n",
       "123                -1.0                     -1.0                    -1.0   \n",
       "51                 -1.0                     -1.0                    -1.0   \n",
       "89                 -1.0                     -1.0                    -1.0   \n",
       "193                -1.0                     -1.0                    -1.0   \n",
       "223                -1.0                     -1.0                    -1.0   \n",
       "104                -1.0                     -1.0                    -1.0   \n",
       "224                -1.0                     -1.0                    -1.0   \n",
       "238                -1.0                     -1.0                    -1.0   \n",
       "143                -1.0                     -1.0                    -1.0   \n",
       "296                -1.0                     -1.0                    -1.0   \n",
       "204                -1.0                     -1.0                    -1.0   \n",
       "260                -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  avg_ms_actual  avg_ms_pred  \n",
       "281       28672        28672         -1.0          0.004     0.007771  \n",
       "202       55488        55488         -1.0          1.154     0.853289  \n",
       "275       20480        20480         -1.0          1.050     1.142880  \n",
       "268       81920        12288          1.0          0.002     0.002463  \n",
       "25        58800        58800         -1.0          0.450     0.598330  \n",
       "18       235200        78400          1.0          0.002     0.002029  \n",
       "122       36992        55488          1.0          0.002     0.002223  \n",
       "6       1605632       401408          4.0          1.788     5.010697  \n",
       "123      221952        55488          1.0          0.002     0.002268  \n",
       "51       313600        78400          1.0          0.002     0.003717  \n",
       "89        78400        78400         -1.0          0.683     0.976136  \n",
       "193       55488        55488         -1.0          0.859     0.853289  \n",
       "223       55488        55488         -1.0          0.005     0.005504  \n",
       "104       36992        36992          1.0          0.002     0.002029  \n",
       "224       55488        55488         -1.0          1.150     0.853289  \n",
       "238       55488        55488         -1.0          0.005     0.005504  \n",
       "143       46240        46240         -1.0          1.031     0.791480  \n",
       "296       24576        24576         -1.0          0.004     0.005727  \n",
       "204       55488        55488          1.0          0.003     0.002670  \n",
       "260       24576        24576          1.0          0.002     0.002019  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = cleaned.drop(['name', '[avg ms]', '[mem KB]', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "y = cleaned['[avg ms]']  # Labels\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train);\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1).sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1480980.8144832274\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>mem_KB_actual</th>\n",
       "      <th>mem_KB_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>46240</td>\n",
       "      <td>46240</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>184.960</td>\n",
       "      <td>184.931840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12288</td>\n",
       "      <td>12288</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>25088</td>\n",
       "      <td>25088</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221.952</td>\n",
       "      <td>221.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>46240</td>\n",
       "      <td>46240</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>46240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>313.600</td>\n",
       "      <td>313.521600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221.952</td>\n",
       "      <td>221.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>352800</td>\n",
       "      <td>58800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.181120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>46240</td>\n",
       "      <td>46240</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>184.960</td>\n",
       "      <td>184.931840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20480</td>\n",
       "      <td>20480</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>81.920</td>\n",
       "      <td>84.459520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>46240</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221.952</td>\n",
       "      <td>221.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>235200</td>\n",
       "      <td>39200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1382976</td>\n",
       "      <td>341056</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1364.224</td>\n",
       "      <td>1536.642624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "143   -1.0     -1.0         -1.0                  0                1   \n",
       "188   -1.0     -1.0         -1.0                  0                0   \n",
       "264   -1.0     -1.0         -1.0                  0                0   \n",
       "305   -1.0     -1.0         -1.0                  0                0   \n",
       "19    -1.0     -1.0         -1.0                  0                0   \n",
       "213   -1.0     -1.0         -1.0                  0                1   \n",
       "179   -1.0     -1.0         -1.0                  0                0   \n",
       "125   -1.0     -1.0         -1.0                  0                0   \n",
       "165   -1.0    160.0          1.0                  1                0   \n",
       "37    -1.0     -1.0         -1.0                  0                1   \n",
       "189   -1.0     -1.0         -1.0                  0                0   \n",
       "129   -1.0     -1.0         -1.0                  0                1   \n",
       "67    -1.0     48.0          1.0                  1                0   \n",
       "150   -1.0     -1.0         -1.0                  0                1   \n",
       "204   -1.0    192.0          7.0                  1                0   \n",
       "275   -1.0     -1.0         -1.0                  0                1   \n",
       "153   -1.0    192.0          7.0                  1                0   \n",
       "163   -1.0     -1.0         -1.0                  0                1   \n",
       "31    -1.0     32.0          1.0                  1                0   \n",
       "10    -1.0     -1.0         -1.0                  0                0   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "143                   0               0             -1.0            -1.0   \n",
       "188                   0               1             -1.0            -1.0   \n",
       "264                   0               1             -1.0            -1.0   \n",
       "305                   0               1             -1.0            -1.0   \n",
       "19                    0               1             -1.0            -1.0   \n",
       "213                   0               0             -1.0            -1.0   \n",
       "179                   0               1             -1.0            -1.0   \n",
       "125                   0               1             -1.0            -1.0   \n",
       "165                   0               0             -1.0            -1.0   \n",
       "37                    0               0             -1.0            -1.0   \n",
       "189                   0               1             -1.0            -1.0   \n",
       "129                   0               0             -1.0            -1.0   \n",
       "67                    0               0             -1.0            -1.0   \n",
       "150                   0               0             -1.0            -1.0   \n",
       "204                   0               0             -1.0            -1.0   \n",
       "275                   0               0             -1.0            -1.0   \n",
       "153                   0               0             -1.0            -1.0   \n",
       "163                   0               0             -1.0            -1.0   \n",
       "31                    0               0             -1.0            -1.0   \n",
       "10                    0               1             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "143                 -1.0                 -1.0             -1.0   \n",
       "188                 -1.0                 -1.0             -1.0   \n",
       "264                 -1.0                 -1.0             -1.0   \n",
       "305                 -1.0                 -1.0             -1.0   \n",
       "19                  -1.0                 -1.0             -1.0   \n",
       "213                 -1.0                 -1.0             -1.0   \n",
       "179                 -1.0                 -1.0             -1.0   \n",
       "125                 -1.0                 -1.0             -1.0   \n",
       "165                 -1.0                 -1.0             -1.0   \n",
       "37                  -1.0                 -1.0             -1.0   \n",
       "189                 -1.0                 -1.0             -1.0   \n",
       "129                 -1.0                 -1.0             -1.0   \n",
       "67                  -1.0                 -1.0             -1.0   \n",
       "150                 -1.0                 -1.0             -1.0   \n",
       "204                 -1.0                 -1.0             -1.0   \n",
       "275                 -1.0                 -1.0             -1.0   \n",
       "153                 -1.0                 -1.0             -1.0   \n",
       "163                 -1.0                 -1.0             -1.0   \n",
       "31                  -1.0                 -1.0             -1.0   \n",
       "10                  -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "143                -1.0                     -1.0                    -1.0   \n",
       "188                -1.0                     -1.0                    -1.0   \n",
       "264                -1.0                     -1.0                    -1.0   \n",
       "305                -1.0                     -1.0                    -1.0   \n",
       "19                 -1.0                     -1.0                    -1.0   \n",
       "213                -1.0                     -1.0                    -1.0   \n",
       "179                -1.0                     -1.0                    -1.0   \n",
       "125                -1.0                     -1.0                    -1.0   \n",
       "165                -1.0                     -1.0                    -1.0   \n",
       "37                 -1.0                     -1.0                    -1.0   \n",
       "189                -1.0                     -1.0                    -1.0   \n",
       "129                -1.0                     -1.0                    -1.0   \n",
       "67                 -1.0                     -1.0                    -1.0   \n",
       "150                -1.0                     -1.0                    -1.0   \n",
       "204                -1.0                     -1.0                    -1.0   \n",
       "275                -1.0                     -1.0                    -1.0   \n",
       "153                -1.0                     -1.0                    -1.0   \n",
       "163                -1.0                     -1.0                    -1.0   \n",
       "31                 -1.0                     -1.0                    -1.0   \n",
       "10                 -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  mem_KB_actual  mem_KB_pred  \n",
       "143       46240        46240         -1.0        184.960   184.931840  \n",
       "188       55488        55488         -1.0          0.000     0.000000  \n",
       "264       24576        24576         -1.0          0.000     0.000000  \n",
       "305       12288        12288         -1.0          0.000     0.032768  \n",
       "19        25088        25088         -1.0          0.016     0.000000  \n",
       "213       55488        55488         -1.0        221.952   221.952000  \n",
       "179       46240        46240         -1.0          0.000     0.000000  \n",
       "125       55488        55488         -1.0          0.000     0.000000  \n",
       "165      221952        46240          1.0          0.000     0.000000  \n",
       "37        78400        78400         -1.0        313.600   313.521600  \n",
       "189       55488        55488         -1.0          0.000     0.000000  \n",
       "129       55488        55488         -1.0        221.952   221.952000  \n",
       "67       352800        58800          1.0          0.000    20.181120  \n",
       "150       46240        46240         -1.0        184.960   184.931840  \n",
       "204       55488        55488          1.0          0.000     0.000000  \n",
       "275       20480        20480         -1.0         81.920    84.459520  \n",
       "153       46240        55488          1.0          0.000     0.000000  \n",
       "163       55488        55488         -1.0        221.952   221.952000  \n",
       "31       235200        39200          1.0          0.000     0.000000  \n",
       "10      1382976       341056          4.0       1364.224  1536.642624  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = cleaned.drop(['name', '[avg ms]', '[mem KB]', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "y = cleaned['[mem KB]']  # Labels\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train);\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_KB_actual': y_test, 'mem_KB_pred': y_pred})], axis=1).sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.3499471311167615\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>36992</td>\n",
       "      <td>36992</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>268203</td>\n",
       "      <td>710432</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.512949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>313600</td>\n",
       "      <td>78400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>117600</td>\n",
       "      <td>117600</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.014585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.154</td>\n",
       "      <td>0.902445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "156   -1.0     -1.0         -1.0                  0                0   \n",
       "198   -1.0     -1.0         -1.0                  0                0   \n",
       "189   -1.0     -1.0         -1.0                  0                0   \n",
       "105   -1.0     -1.0         -1.0                  0                0   \n",
       "1     -1.0     32.0          9.0                  1                0   \n",
       "54    -1.0     64.0          1.0                  1                0   \n",
       "80    -1.0     -1.0         -1.0                  0                0   \n",
       "202   -1.0     -1.0         -1.0                  0                1   \n",
       "295   -1.0     -1.0         -1.0                  0                0   \n",
       "55    -1.0     -1.0         -1.0                  0                0   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "156                   0               1             -1.0            -1.0   \n",
       "198                   0               1             -1.0            -1.0   \n",
       "189                   0               1             -1.0            -1.0   \n",
       "105                   0               1             -1.0            -1.0   \n",
       "1                     0               0             -1.0            -1.0   \n",
       "54                    0               0             -1.0            -1.0   \n",
       "80                    0               1             -1.0            -1.0   \n",
       "202                   0               0             -1.0            -1.0   \n",
       "295                   0               1             -1.0            -1.0   \n",
       "55                    0               1             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "156                 -1.0                 -1.0             -1.0   \n",
       "198                 -1.0                 -1.0             -1.0   \n",
       "189                 -1.0                 -1.0             -1.0   \n",
       "105                 -1.0                 -1.0             -1.0   \n",
       "1                   -1.0                 -1.0             -1.0   \n",
       "54                  -1.0                 -1.0             -1.0   \n",
       "80                  -1.0                 -1.0             -1.0   \n",
       "202                 -1.0                 -1.0             -1.0   \n",
       "295                 -1.0                 -1.0             -1.0   \n",
       "55                  -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "156                -1.0                     -1.0                    -1.0   \n",
       "198                -1.0                     -1.0                    -1.0   \n",
       "189                -1.0                     -1.0                    -1.0   \n",
       "105                -1.0                     -1.0                    -1.0   \n",
       "1                  -1.0                     -1.0                    -1.0   \n",
       "54                 -1.0                     -1.0                    -1.0   \n",
       "80                 -1.0                     -1.0                    -1.0   \n",
       "202                -1.0                     -1.0                    -1.0   \n",
       "295                -1.0                     -1.0                    -1.0   \n",
       "55                 -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  avg_ms_actual  avg_ms_pred  \n",
       "156       55488        55488         -1.0          0.006     0.005430  \n",
       "198       55488        55488         -1.0          0.007     0.005430  \n",
       "189       55488        55488         -1.0          0.007     0.005430  \n",
       "105       36992        36992         -1.0          0.003     0.004729  \n",
       "1        268203       710432          4.0          0.002     0.512949  \n",
       "54       313600        78400          1.0          0.002     0.002025  \n",
       "80       117600       117600         -1.0          0.004     0.014585  \n",
       "202       55488        55488         -1.0          1.154     0.902445  \n",
       "295       24576        24576         -1.0          0.007     0.005137  \n",
       "55        78400        78400         -1.0          0.005     0.004020  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AVG MS RF\n",
    "cnn_ms_rf, X_test, y_test = rf_regression_model(cleaned, '[avg ms]')\n",
    "\n",
    "y_pred = cnn_ms_rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "cnn_ms_rf_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 70984.64512575942\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>36992</td>\n",
       "      <td>36992</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>147.968</td>\n",
       "      <td>147.535616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12288</td>\n",
       "      <td>12288</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>49.152</td>\n",
       "      <td>50.151424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>46240</td>\n",
       "      <td>46240</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>98.304</td>\n",
       "      <td>98.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>27744</td>\n",
       "      <td>27744</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>110.976</td>\n",
       "      <td>108.396544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>58800</td>\n",
       "      <td>58800</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>313.600</td>\n",
       "      <td>313.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "295   -1.0     -1.0         -1.0                  0                0   \n",
       "103   -1.0     -1.0         -1.0                  0                1   \n",
       "278   -1.0     -1.0         -1.0                  0                1   \n",
       "147   -1.0     -1.0         -1.0                  0                0   \n",
       "288   -1.0     -1.0         -1.0                  0                1   \n",
       "296   -1.0     -1.0         -1.0                  0                0   \n",
       "229   -1.0    192.0          1.0                  1                0   \n",
       "98    -1.0     -1.0         -1.0                  0                1   \n",
       "69    -1.0     -1.0         -1.0                  0                0   \n",
       "89    -1.0     -1.0         -1.0                  0                1   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "295                   0               1             -1.0            -1.0   \n",
       "103                   0               0             -1.0            -1.0   \n",
       "278                   0               0             -1.0            -1.0   \n",
       "147                   0               1             -1.0            -1.0   \n",
       "288                   0               0             -1.0            -1.0   \n",
       "296                   0               1             -1.0            -1.0   \n",
       "229                   0               0             -1.0            -1.0   \n",
       "98                    0               0             -1.0            -1.0   \n",
       "69                    0               1             -1.0            -1.0   \n",
       "89                    0               0             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "295                 -1.0                 -1.0             -1.0   \n",
       "103                 -1.0                 -1.0             -1.0   \n",
       "278                 -1.0                 -1.0             -1.0   \n",
       "147                 -1.0                 -1.0             -1.0   \n",
       "288                 -1.0                 -1.0             -1.0   \n",
       "296                 -1.0                 -1.0             -1.0   \n",
       "229                 -1.0                 -1.0             -1.0   \n",
       "98                  -1.0                 -1.0             -1.0   \n",
       "69                  -1.0                 -1.0             -1.0   \n",
       "89                  -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "295                -1.0                     -1.0                    -1.0   \n",
       "103                -1.0                     -1.0                    -1.0   \n",
       "278                -1.0                     -1.0                    -1.0   \n",
       "147                -1.0                     -1.0                    -1.0   \n",
       "288                -1.0                     -1.0                    -1.0   \n",
       "296                -1.0                     -1.0                    -1.0   \n",
       "229                -1.0                     -1.0                    -1.0   \n",
       "98                 -1.0                     -1.0                    -1.0   \n",
       "69                 -1.0                     -1.0                    -1.0   \n",
       "89                 -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  mem_kb_actual  mem_kb_pred  \n",
       "295       24576        24576         -1.0          0.000     0.000000  \n",
       "103       36992        36992         -1.0        147.968   147.535616  \n",
       "278       12288        12288         -1.0         49.152    50.151424  \n",
       "147       46240        46240         -1.0          0.000     0.000000  \n",
       "288       24576        24576         -1.0         98.304    98.304000  \n",
       "296       24576        24576         -1.0          0.000     0.000000  \n",
       "229      221952        55488          1.0          0.000     0.000000  \n",
       "98        27744        27744         -1.0        110.976   108.396544  \n",
       "69        58800        58800         -1.0          0.000     0.000000  \n",
       "89        78400        78400         -1.0        313.600   313.600000  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MEM KB RF\n",
    "cnn_mem_rf, X_test, y_test = rf_regression_model(cleaned, '[mem KB]')\n",
    "\n",
    "y_pred = cnn_mem_rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "cnn_mem_rf_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.4571264901270733\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20480</td>\n",
       "      <td>20480</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.050</td>\n",
       "      <td>1.091647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>46240</td>\n",
       "      <td>46240</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.222139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221952</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.367585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.019767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.164007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.164007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>117600</td>\n",
       "      <td>117600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.173694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>313600</td>\n",
       "      <td>313600</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.384</td>\n",
       "      <td>1.310546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.668197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2.238347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "275   -1.0     -1.0         -1.0                  0                1   \n",
       "180   -1.0     -1.0         -1.0                  0                0   \n",
       "229   -1.0    192.0          1.0                  1                0   \n",
       "81    -1.0     -1.0         -1.0                  0                0   \n",
       "158   -1.0     -1.0         -1.0                  0                0   \n",
       "191   -1.0     -1.0         -1.0                  0                0   \n",
       "53    -1.0     96.0          9.0                  1                0   \n",
       "40    -1.0     -1.0         -1.0                  0                0   \n",
       "236   -1.0    192.0          7.0                  1                0   \n",
       "261   -1.0    384.0          3.0                  1                0   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "275                   0               0             -1.0            -1.0   \n",
       "180                   0               1             -1.0            -1.0   \n",
       "229                   0               0             -1.0            -1.0   \n",
       "81                    0               1             -1.0            -1.0   \n",
       "158                   0               1             -1.0            -1.0   \n",
       "191                   0               1             -1.0            -1.0   \n",
       "53                    0               0             -1.0            -1.0   \n",
       "40                    0               1             -1.0            -1.0   \n",
       "236                   0               0             -1.0            -1.0   \n",
       "261                   0               0             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "275                 -1.0                 -1.0             -1.0   \n",
       "180                 -1.0                 -1.0             -1.0   \n",
       "229                 -1.0                 -1.0             -1.0   \n",
       "81                  -1.0                 -1.0             -1.0   \n",
       "158                 -1.0                 -1.0             -1.0   \n",
       "191                 -1.0                 -1.0             -1.0   \n",
       "53                  -1.0                 -1.0             -1.0   \n",
       "40                  -1.0                 -1.0             -1.0   \n",
       "236                 -1.0                 -1.0             -1.0   \n",
       "261                 -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "275                -1.0                     -1.0                    -1.0   \n",
       "180                -1.0                     -1.0                    -1.0   \n",
       "229                -1.0                     -1.0                    -1.0   \n",
       "81                 -1.0                     -1.0                    -1.0   \n",
       "158                -1.0                     -1.0                    -1.0   \n",
       "191                -1.0                     -1.0                    -1.0   \n",
       "53                 -1.0                     -1.0                    -1.0   \n",
       "40                 -1.0                     -1.0                    -1.0   \n",
       "236                -1.0                     -1.0                    -1.0   \n",
       "261                -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  avg_ms_actual  avg_ms_pred  \n",
       "275       20480        20480         -1.0          1.050     1.091647  \n",
       "180       46240        46240         -1.0          0.006    -0.222139  \n",
       "229      221952        55488          1.0          0.002     0.367585  \n",
       "81        78400        78400         -1.0          0.003    -0.019767  \n",
       "158       55488        55488         -1.0          0.006    -0.164007  \n",
       "191       55488        55488         -1.0          0.005    -0.164007  \n",
       "53       117600       117600          1.0          0.002     0.173694  \n",
       "40       313600       313600         -1.0          0.384     1.310546  \n",
       "236       55488        55488          1.0          0.003     0.668197  \n",
       "261       24576        24576          1.0          0.002     2.238347  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AVG MS stacked\n",
    "cnn_ms_stacked_1, X_test, y_test = stacked_regression_model(cleaned, '[avg ms]')\n",
    "\n",
    "y_pred = cnn_ms_stacked_1.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "cnn_ms_stacked_1_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 555240.4477081143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221.952</td>\n",
       "      <td>249.100040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>46240</td>\n",
       "      <td>46240</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-87.383302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221.952</td>\n",
       "      <td>249.100040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-48.972566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>46240</td>\n",
       "      <td>55488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-154.045969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-48.972566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>98.304</td>\n",
       "      <td>128.361360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>20480</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-30.102100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>352800</td>\n",
       "      <td>352800</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1411.200</td>\n",
       "      <td>1105.380382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>81920</td>\n",
       "      <td>28672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>118.071912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "129   -1.0     -1.0         -1.0                  0                1   \n",
       "137   -1.0     -1.0         -1.0                  0                0   \n",
       "208   -1.0     -1.0         -1.0                  0                1   \n",
       "205   -1.0     -1.0         -1.0                  0                0   \n",
       "186   -1.0    192.0          7.0                  1                0   \n",
       "159   -1.0     -1.0         -1.0                  0                0   \n",
       "302   -1.0     -1.0         -1.0                  0                1   \n",
       "241   -1.0    320.0          9.0                  1                0   \n",
       "63    -1.0     -1.0         -1.0                  0                0   \n",
       "249   -1.0    448.0          1.0                  1                0   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "129                   0               0             -1.0            -1.0   \n",
       "137                   0               1             -1.0            -1.0   \n",
       "208                   0               0             -1.0            -1.0   \n",
       "205                   0               1             -1.0            -1.0   \n",
       "186                   0               0             -1.0            -1.0   \n",
       "159                   0               1             -1.0            -1.0   \n",
       "302                   0               0             -1.0            -1.0   \n",
       "241                   0               0             -1.0            -1.0   \n",
       "63                    0               1             -1.0            -1.0   \n",
       "249                   0               0             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "129                 -1.0                 -1.0             -1.0   \n",
       "137                 -1.0                 -1.0             -1.0   \n",
       "208                 -1.0                 -1.0             -1.0   \n",
       "205                 -1.0                 -1.0             -1.0   \n",
       "186                 -1.0                 -1.0             -1.0   \n",
       "159                 -1.0                 -1.0             -1.0   \n",
       "302                 -1.0                 -1.0             -1.0   \n",
       "241                 -1.0                 -1.0             -1.0   \n",
       "63                  -1.0                 -1.0             -1.0   \n",
       "249                 -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "129                -1.0                     -1.0                    -1.0   \n",
       "137                -1.0                     -1.0                    -1.0   \n",
       "208                -1.0                     -1.0                    -1.0   \n",
       "205                -1.0                     -1.0                    -1.0   \n",
       "186                -1.0                     -1.0                    -1.0   \n",
       "159                -1.0                     -1.0                    -1.0   \n",
       "302                -1.0                     -1.0                    -1.0   \n",
       "241                -1.0                     -1.0                    -1.0   \n",
       "63                 -1.0                     -1.0                    -1.0   \n",
       "249                -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  mem_kb_actual  mem_kb_pred  \n",
       "129       55488        55488         -1.0        221.952   249.100040  \n",
       "137       46240        46240         -1.0          0.000   -87.383302  \n",
       "208       55488        55488         -1.0        221.952   249.100040  \n",
       "205       55488        55488         -1.0          0.000   -48.972566  \n",
       "186       46240        55488          1.0          0.000  -154.045969  \n",
       "159       55488        55488         -1.0          0.000   -48.972566  \n",
       "302       24576        24576         -1.0         98.304   128.361360  \n",
       "241       55488        20480          4.0          0.000   -30.102100  \n",
       "63       352800       352800         -1.0       1411.200  1105.380382  \n",
       "249       81920        28672          1.0          0.000   118.071912  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AVG MEM stacked\n",
    "cnn_mem_stacked_1, X_test, y_test = stacked_regression_model(cleaned, '[mem KB]')\n",
    "\n",
    "y_pred = cnn_mem_stacked_1.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "cnn_mem_stacked_1_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3.4732015849117315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.357337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.847737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>46240</td>\n",
       "      <td>46240</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.345422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.847737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.386857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "222   -1.0     -1.0         -1.0                  0                0   \n",
       "85    -1.0     -1.0         -1.0                  0                1   \n",
       "166   -1.0     -1.0         -1.0                  0                0   \n",
       "36    -1.0     -1.0         -1.0                  0                1   \n",
       "19    -1.0     -1.0         -1.0                  0                0   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "222                   0               1             -1.0            -1.0   \n",
       "85                    0               0             -1.0            -1.0   \n",
       "166                   0               1             -1.0            -1.0   \n",
       "36                    0               0             -1.0            -1.0   \n",
       "19                    0               1             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "222                 -1.0                 -1.0             -1.0   \n",
       "85                  -1.0                 -1.0             -1.0   \n",
       "166                 -1.0                 -1.0             -1.0   \n",
       "36                  -1.0                 -1.0             -1.0   \n",
       "19                  -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "222                -1.0                     -1.0                    -1.0   \n",
       "85                 -1.0                     -1.0                    -1.0   \n",
       "166                -1.0                     -1.0                    -1.0   \n",
       "36                 -1.0                     -1.0                    -1.0   \n",
       "19                 -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  avg_ms_actual  avg_ms_pred  \n",
       "222       55488        55488         -1.0          0.006     0.357337  \n",
       "85        78400        78400         -1.0          0.778     0.847737  \n",
       "166       46240        46240         -1.0          0.006     0.345422  \n",
       "36        78400        78400         -1.0          0.464     0.847737  \n",
       "19        78400        78400         -1.0          0.003     0.386857  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lstm stacked avg ms\n",
    "cnn_ms_stacked_2, X_test, y_test = stacked_regression_model_2(cleaned, '[avg ms]')\n",
    "\n",
    "y_pred = cnn_ms_stacked_2.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "cnn_ms_stacked_2_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 4468742733189014.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221.952</td>\n",
       "      <td>-6.692743e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>78400</td>\n",
       "      <td>78400</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-6.692776e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>802816</td>\n",
       "      <td>802816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3211.264</td>\n",
       "      <td>-6.692269e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55488</td>\n",
       "      <td>55488</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>221.952</td>\n",
       "      <td>-6.692743e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>46240</td>\n",
       "      <td>46240</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>184.960</td>\n",
       "      <td>-6.692747e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     units  filters  kernel_size  activation_linear  activation_relu  \\\n",
       "225   -1.0     -1.0         -1.0                  0                1   \n",
       "32    -1.0     -1.0         -1.0                  0                0   \n",
       "9     -1.0    256.0          9.0                  0                1   \n",
       "234   -1.0     -1.0         -1.0                  0                1   \n",
       "182   -1.0     -1.0         -1.0                  0                1   \n",
       "\n",
       "     activation_softmax  activation_nan  activation_selu  activation_elu  \\\n",
       "225                   0               0             -1.0            -1.0   \n",
       "32                    0               1             -1.0            -1.0   \n",
       "9                     0               0             -1.0            -1.0   \n",
       "234                   0               0             -1.0            -1.0   \n",
       "182                   0               0             -1.0            -1.0   \n",
       "\n",
       "     activation_softplus  activation_softsign  activation_tanh  \\\n",
       "225                 -1.0                 -1.0             -1.0   \n",
       "32                  -1.0                 -1.0             -1.0   \n",
       "9                   -1.0                 -1.0             -1.0   \n",
       "234                 -1.0                 -1.0             -1.0   \n",
       "182                 -1.0                 -1.0             -1.0   \n",
       "\n",
       "     activation_sigmoid  activation_hard_sigmoid  activation_exponential  \\\n",
       "225                -1.0                     -1.0                    -1.0   \n",
       "32                 -1.0                     -1.0                    -1.0   \n",
       "9                  -1.0                     -1.0                    -1.0   \n",
       "234                -1.0                     -1.0                    -1.0   \n",
       "182                -1.0                     -1.0                    -1.0   \n",
       "\n",
       "     input_size  output_size  stride_size  mem_kb_actual   mem_kb_pred  \n",
       "225       55488        55488         -1.0        221.952 -6.692743e+07  \n",
       "32        78400        78400         -1.0          0.000 -6.692776e+07  \n",
       "9        802816       802816          1.0       3211.264 -6.692269e+07  \n",
       "234       55488        55488         -1.0        221.952 -6.692743e+07  \n",
       "182       46240        46240         -1.0        184.960 -6.692747e+07  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lstm stacked mem\n",
    "cnn_mem_stacked_2, X_test, y_test = stacked_regression_model_2(cleaned, '[mem KB]')\n",
    "\n",
    "y_pred = cnn_mem_stacked_2.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "cnn_mem_stacked_2_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving saved model and benchmark results.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[node type]</th>\n",
       "      <th>[start]</th>\n",
       "      <th>[first]</th>\n",
       "      <th>[avg ms]</th>\n",
       "      <th>[%]</th>\n",
       "      <th>[cdf%]</th>\n",
       "      <th>[mem KB]</th>\n",
       "      <th>[times called]</th>\n",
       "      <th>[Name]</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Add</td>\n",
       "      <td>140.384</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.333%</td>\n",
       "      <td>30.046%</td>\n",
       "      <td>0.004</td>\n",
       "      <td>250</td>\n",
       "      <td>lstm4/while/add_9</td>\n",
       "      <td>lstm4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Switch</td>\n",
       "      <td>190.693</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.210%</td>\n",
       "      <td>16.796%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>251</td>\n",
       "      <td>lstm5/while/Switch</td>\n",
       "      <td>lstm5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Add</td>\n",
       "      <td>95.325</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.296%</td>\n",
       "      <td>45.060%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>250</td>\n",
       "      <td>lstm3/while/add_5</td>\n",
       "      <td>lstm3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>Minimum</td>\n",
       "      <td>-0.891</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.199%</td>\n",
       "      <td>96.554%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>250</td>\n",
       "      <td>lstm1/while/clip_by_value_2/Minimum</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>Pack</td>\n",
       "      <td>23.152</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004%</td>\n",
       "      <td>73.083%</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm2/zeros_1/packed</td>\n",
       "      <td>lstm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Maximum</td>\n",
       "      <td>46.129</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.234%</td>\n",
       "      <td>60.706%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>250</td>\n",
       "      <td>lstm2/while/clip_by_value_1</td>\n",
       "      <td>lstm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NextIteration</td>\n",
       "      <td>190.693</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.401%</td>\n",
       "      <td>10.244%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>250</td>\n",
       "      <td>lstm5/while/NextIteration</td>\n",
       "      <td>lstm5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Range</td>\n",
       "      <td>22.816</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002%</td>\n",
       "      <td>75.531%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm1/TensorArrayStack/range</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Shape</td>\n",
       "      <td>120.809</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.004%</td>\n",
       "      <td>38.173%</td>\n",
       "      <td>0.012</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm4/Shape</td>\n",
       "      <td>lstm4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>Enter</td>\n",
       "      <td>-24.183</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001%</td>\n",
       "      <td>75.582%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm1/while/split/ReadVariableOp/Enter</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>Conv2D</td>\n",
       "      <td>-24.180</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.056%</td>\n",
       "      <td>80.372%</td>\n",
       "      <td>64.000</td>\n",
       "      <td>1</td>\n",
       "      <td>conv1d/conv1d</td>\n",
       "      <td>conv1d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tanh</td>\n",
       "      <td>195.197</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.368%</td>\n",
       "      <td>1.856%</td>\n",
       "      <td>0.128</td>\n",
       "      <td>250</td>\n",
       "      <td>lstm5/while/Tanh_1</td>\n",
       "      <td>lstm5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>Less</td>\n",
       "      <td>41.763</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.309%</td>\n",
       "      <td>75.430%</td>\n",
       "      <td>0.001</td>\n",
       "      <td>251</td>\n",
       "      <td>lstm2/while/Less_1</td>\n",
       "      <td>lstm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>Mul</td>\n",
       "      <td>-0.824</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.325%</td>\n",
       "      <td>99.041%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>250</td>\n",
       "      <td>lstm1/while/mul_5</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>Const</td>\n",
       "      <td>-24.246</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001%</td>\n",
       "      <td>75.564%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>dense/kernel</td>\n",
       "      <td>dense</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  [node type]  [start]  [first]  [avg ms]       [%]    [cdf%]  \\\n",
       "131                       Add  140.384    0.004     0.003    0.333%   30.046%   \n",
       "68                     Switch  190.693    0.003     0.002    0.210%   16.796%   \n",
       "206                       Add   95.325    0.008     0.003    0.296%   45.060%   \n",
       "516                   Minimum   -0.891    0.002     0.002    0.199%   96.554%   \n",
       "334                      Pack   23.152    0.012     0.010    0.004%   73.083%   \n",
       "280                   Maximum   46.129    0.004     0.002    0.234%   60.706%   \n",
       "42              NextIteration  190.693    0.008     0.004    0.401%   10.244%   \n",
       "353                     Range   22.816    0.007     0.004    0.002%   75.531%   \n",
       "174                     Shape  120.809    0.007     0.009    0.004%   38.173%   \n",
       "401                     Enter  -24.183    0.003     0.003    0.001%   75.582%   \n",
       "434                    Conv2D  -24.180    0.157     0.143    0.056%   80.372%   \n",
       "12                       Tanh  195.197    0.010     0.004    0.368%    1.856%   \n",
       "348                      Less   41.763    0.011     0.003    0.309%   75.430%   \n",
       "525                       Mul   -0.824    0.003     0.003    0.325%   99.041%   \n",
       "388                     Const  -24.246    0.002     0.002    0.001%   75.564%   \n",
       "\n",
       "     [mem KB]  [times called]                                  [Name]    name  \n",
       "131     0.004             250                       lstm4/while/add_9   lstm4  \n",
       "68      0.000             251                      lstm5/while/Switch   lstm5  \n",
       "206     0.000             250                       lstm3/while/add_5   lstm3  \n",
       "516     0.000             250     lstm1/while/clip_by_value_2/Minimum   lstm1  \n",
       "334     0.008               1                    lstm2/zeros_1/packed   lstm2  \n",
       "280     0.000             250             lstm2/while/clip_by_value_1   lstm2  \n",
       "42      0.000             250               lstm5/while/NextIteration   lstm5  \n",
       "353     1.000               1            lstm1/TensorArrayStack/range   lstm1  \n",
       "174     0.012               1                             lstm4/Shape   lstm4  \n",
       "401     0.000               1  lstm1/while/split/ReadVariableOp/Enter   lstm1  \n",
       "434    64.000               1                           conv1d/conv1d  conv1d  \n",
       "12      0.128             250                      lstm5/while/Tanh_1   lstm5  \n",
       "348     0.001             251                      lstm2/while/Less_1   lstm2  \n",
       "525     0.000             250                       lstm1/while/mul_5   lstm1  \n",
       "388     0.000               1                            dense/kernel   dense  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lstm.summary()\n",
    "\n",
    "tf.keras.backend.clear_session() # IMPORTANT for layer names to match up.\n",
    "\n",
    "lstm = create_lstm()\n",
    "\n",
    "# tf.keras.backend.set_learning_phase(0)\n",
    "lstm_benchmark = benchmark_model(lstm)\n",
    "lstm_benchmark.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_features = get_layer_features(lstm)\n",
    "lstm_df = join_benchmark(lstm_features, lstm_benchmark)\n",
    "cleaned_lstm = clean(lstm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.003043146131333064\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.900587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>16000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.242828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.900079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  filters  kernel_size  activation_relu  activation_sigmoid  \\\n",
       "7   48.0     -1.0         -1.0                0                   0   \n",
       "1   -1.0     32.0          3.0                1                   0   \n",
       "5   64.0     -1.0         -1.0                0                   0   \n",
       "\n",
       "   activation_tanh  activation_nan  activation_selu  activation_elu  \\\n",
       "7                1               0             -1.0            -1.0   \n",
       "1                0               0             -1.0            -1.0   \n",
       "5                1               0             -1.0            -1.0   \n",
       "\n",
       "   activation_softmax  activation_softplus  activation_softsign  \\\n",
       "7                -1.0                 -1.0                 -1.0   \n",
       "1                -1.0                 -1.0                 -1.0   \n",
       "5                -1.0                 -1.0                 -1.0   \n",
       "\n",
       "   activation_hard_sigmoid  activation_exponential  activation_linear  \\\n",
       "7                     -1.0                    -1.0               -1.0   \n",
       "1                     -1.0                    -1.0               -1.0   \n",
       "5                     -1.0                    -1.0               -1.0   \n",
       "\n",
       "   input_size  output_size  stride_size  avg_ms_actual  avg_ms_pred  \n",
       "7       32000           48         -1.0          0.941     0.900587  \n",
       "1       16000        16000          1.0          0.166     0.242828  \n",
       "5        8000        16000         -1.0          0.940     0.900079  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf, X_test, y_test = rf_regression_model(cleaned_lstm, '[avg ms]')\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 363.04756968992393\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.184864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>16000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>31.712296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.448108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  filters  kernel_size  activation_relu  activation_sigmoid  \\\n",
       "7   48.0     -1.0         -1.0                0                   0   \n",
       "1   -1.0     32.0          3.0                1                   0   \n",
       "5   64.0     -1.0         -1.0                0                   0   \n",
       "\n",
       "   activation_tanh  activation_nan  activation_selu  activation_elu  \\\n",
       "7                1               0             -1.0            -1.0   \n",
       "1                0               0             -1.0            -1.0   \n",
       "5                1               0             -1.0            -1.0   \n",
       "\n",
       "   activation_softmax  activation_softplus  activation_softsign  \\\n",
       "7                -1.0                 -1.0                 -1.0   \n",
       "1                -1.0                 -1.0                 -1.0   \n",
       "5                -1.0                 -1.0                 -1.0   \n",
       "\n",
       "   activation_hard_sigmoid  activation_exponential  activation_linear  \\\n",
       "7                     -1.0                    -1.0               -1.0   \n",
       "1                     -1.0                    -1.0               -1.0   \n",
       "5                     -1.0                    -1.0               -1.0   \n",
       "\n",
       "   input_size  output_size  stride_size  mem_kb_actual  mem_kb_pred  \n",
       "7       32000           48         -1.0           32.0    25.184864  \n",
       "1       16000        16000          1.0           64.0    31.712296  \n",
       "5        8000        16000         -1.0           32.0    32.448108  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf, X_test, y_test = rf_regression_model(cleaned_lstm, '[mem KB]')\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.4664580543064454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>128.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>32000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.574058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.571586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.988990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  filters  kernel_size  activation_relu  activation_sigmoid  \\\n",
       "6  128.0     -1.0         -1.0                0                   0   \n",
       "0   -1.0     -1.0         -1.0                0                   0   \n",
       "8    1.0     -1.0         -1.0                0                   1   \n",
       "\n",
       "   activation_tanh  activation_nan  activation_selu  activation_elu  \\\n",
       "6                1               0             -1.0            -1.0   \n",
       "0                0               1              0.0             0.0   \n",
       "8                0               0             -1.0            -1.0   \n",
       "\n",
       "   activation_softmax  activation_softplus  activation_softsign  \\\n",
       "6                -1.0                 -1.0                 -1.0   \n",
       "0                 0.0                  0.0                  0.0   \n",
       "8                -1.0                 -1.0                 -1.0   \n",
       "\n",
       "   activation_hard_sigmoid  activation_exponential  activation_linear  \\\n",
       "6                     -1.0                    -1.0               -1.0   \n",
       "0                      0.0                     0.0                0.0   \n",
       "8                     -1.0                    -1.0               -1.0   \n",
       "\n",
       "   input_size  output_size  stride_size  avg_ms_actual  avg_ms_pred  \n",
       "6       16000        32000         -1.0          0.965     0.574058  \n",
       "0         500        16000         -1.0          0.033     0.571586  \n",
       "8          48            1         -1.0          0.011     0.988990  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lstm stacked avg ms\n",
    "rf, X_test, y_test = stacked_regression_model(cleaned_lstm, '[avg ms]')\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2678.164860583543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>2500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.000</td>\n",
       "      <td>55.145124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>8000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.000</td>\n",
       "      <td>85.770847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.004</td>\n",
       "      <td>67.882523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  filters  kernel_size  activation_relu  activation_sigmoid  \\\n",
       "3   10.0     -1.0         -1.0                0                   0   \n",
       "2   -1.0     -1.0         -1.0                0                   0   \n",
       "8    1.0     -1.0         -1.0                0                   1   \n",
       "\n",
       "   activation_tanh  activation_nan  activation_selu  activation_elu  \\\n",
       "3                1               0             -1.0            -1.0   \n",
       "2                0               1             -1.0            -1.0   \n",
       "8                0               0             -1.0            -1.0   \n",
       "\n",
       "   activation_softmax  activation_softplus  activation_softsign  \\\n",
       "3                -1.0                 -1.0                 -1.0   \n",
       "2                -1.0                 -1.0                 -1.0   \n",
       "8                -1.0                 -1.0                 -1.0   \n",
       "\n",
       "   activation_hard_sigmoid  activation_exponential  activation_linear  \\\n",
       "3                     -1.0                    -1.0               -1.0   \n",
       "2                     -1.0                    -1.0               -1.0   \n",
       "8                     -1.0                    -1.0               -1.0   \n",
       "\n",
       "   input_size  output_size  stride_size  mem_kb_actual  mem_kb_pred  \n",
       "3        8000         2500         -1.0         32.000    55.145124  \n",
       "2       16000         8000          2.0         32.000    85.770847  \n",
       "8          48            1         -1.0          0.004    67.882523  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lstm stacked mem\n",
    "rf, X_test, y_test = stacked_regression_model(cleaned_lstm, '[mem KB]')\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3.4902692556121484e+16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.941</td>\n",
       "      <td>-3.234538e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.940</td>\n",
       "      <td>6.578387e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>2500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.952</td>\n",
       "      <td>6.515199e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  filters  kernel_size  activation_relu  activation_sigmoid  \\\n",
       "7   48.0     -1.0         -1.0                0                   0   \n",
       "5   64.0     -1.0         -1.0                0                   0   \n",
       "3   10.0     -1.0         -1.0                0                   0   \n",
       "\n",
       "   activation_tanh  activation_nan  activation_selu  activation_elu  \\\n",
       "7                1               0             -1.0            -1.0   \n",
       "5                1               0             -1.0            -1.0   \n",
       "3                1               0             -1.0            -1.0   \n",
       "\n",
       "   activation_softmax  activation_softplus  activation_softsign  \\\n",
       "7                -1.0                 -1.0                 -1.0   \n",
       "5                -1.0                 -1.0                 -1.0   \n",
       "3                -1.0                 -1.0                 -1.0   \n",
       "\n",
       "   activation_hard_sigmoid  activation_exponential  activation_linear  \\\n",
       "7                     -1.0                    -1.0               -1.0   \n",
       "5                     -1.0                    -1.0               -1.0   \n",
       "3                     -1.0                    -1.0               -1.0   \n",
       "\n",
       "   input_size  output_size  stride_size  avg_ms_actual   avg_ms_pred  \n",
       "7       32000           48         -1.0          0.941 -3.234538e+08  \n",
       "5        8000        16000         -1.0          0.940  6.578387e+06  \n",
       "3        8000         2500         -1.0          0.952  6.515199e+06  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lstm stacked avg ms\n",
    "rf, X_test, y_test = stacked_regression_model_2(cleaned_lstm, '[avg ms]')\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 8890994646410.537\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>input_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2500</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-2.982063e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>2500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-2.981444e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-2.981728e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  filters  kernel_size  activation_relu  activation_sigmoid  \\\n",
       "4   32.0     -1.0         -1.0                0                   0   \n",
       "3   10.0     -1.0         -1.0                0                   0   \n",
       "5   64.0     -1.0         -1.0                0                   0   \n",
       "\n",
       "   activation_tanh  activation_nan  activation_selu  activation_elu  \\\n",
       "4                1               0             -1.0            -1.0   \n",
       "3                1               0             -1.0            -1.0   \n",
       "5                1               0             -1.0            -1.0   \n",
       "\n",
       "   activation_softmax  activation_softplus  activation_softsign  \\\n",
       "4                -1.0                 -1.0                 -1.0   \n",
       "3                -1.0                 -1.0                 -1.0   \n",
       "5                -1.0                 -1.0                 -1.0   \n",
       "\n",
       "   activation_hard_sigmoid  activation_exponential  activation_linear  \\\n",
       "4                     -1.0                    -1.0               -1.0   \n",
       "3                     -1.0                    -1.0               -1.0   \n",
       "5                     -1.0                    -1.0               -1.0   \n",
       "\n",
       "   input_size  output_size  stride_size  mem_kb_actual   mem_kb_pred  \n",
       "4        2500         8000         -1.0           32.0 -2.982063e+06  \n",
       "3        8000         2500         -1.0           32.0 -2.981444e+06  \n",
       "5        8000        16000         -1.0           32.0 -2.981728e+06  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lstm stacked mem\n",
    "rf, X_test, y_test = stacked_regression_model_2(cleaned_lstm, '[mem KB]')\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving saved model and benchmark results.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[node type]</th>\n",
       "      <th>[start]</th>\n",
       "      <th>[first]</th>\n",
       "      <th>[avg ms]</th>\n",
       "      <th>[%]</th>\n",
       "      <th>[cdf%]</th>\n",
       "      <th>[mem KB]</th>\n",
       "      <th>[times called]</th>\n",
       "      <th>[Name]</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>_Arg</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.425%</td>\n",
       "      <td>4.019%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>_arg_input_1_0_0</td>\n",
       "      <td>_arg_input_1_0_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>NextIteration</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.246%</td>\n",
       "      <td>57.495%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm1/while/NextIteration</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Mul</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.866%</td>\n",
       "      <td>75.220%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm1/while/Mul_1</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>MatMul</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2.894%</td>\n",
       "      <td>78.114%</td>\n",
       "      <td>1.024</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm1/while/MatMul</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LoopCond</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.913%</td>\n",
       "      <td>22.557%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>lstm1/while/LoopCond</td>\n",
       "      <td>lstm1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 [node type]  [start]  [first]  [avg ms]       [%]    [cdf%]  \\\n",
       "9                       _Arg   -0.098    0.002     0.002    0.425%    4.019%   \n",
       "56             NextIteration    0.087    0.004     0.005    1.246%   57.495%   \n",
       "64                       Mul    0.079    0.003     0.004    0.866%   75.220%   \n",
       "65                    MatMul    0.061    0.014     0.012    2.894%   78.114%   \n",
       "30                  LoopCond    0.050    0.003     0.002    0.913%   22.557%   \n",
       "\n",
       "    [mem KB]  [times called]                     [Name]              name  \n",
       "9      0.000               1           _arg_input_1_0_0  _arg_input_1_0_0  \n",
       "56     0.000               1  lstm1/while/NextIteration             lstm1  \n",
       "64     0.000               1          lstm1/while/Mul_1             lstm1  \n",
       "65     1.024               1         lstm1/while/MatMul             lstm1  \n",
       "30     0.000               2       lstm1/while/LoopCond             lstm1  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lstm.summary()\n",
    "\n",
    "tf.keras.backend.clear_session() # IMPORTANT for layer names to match up.\n",
    "\n",
    "enc, dec = create_seq2seq()\n",
    "\n",
    "# tf.keras.backend.set_learning_phase(0)\n",
    "enc_benchmark = benchmark_model(enc, '../tensorflow/bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=encoder.pbtxt --input_layer=\"input_1:0\" --input_layer_shape=\"1,1,71\" --output_layer=\"lstm1/while/Exit_2:0\"')\n",
    "enc_benchmark.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving saved model and benchmark results.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[node type]</th>\n",
       "      <th>[start]</th>\n",
       "      <th>[first]</th>\n",
       "      <th>[avg ms]</th>\n",
       "      <th>[%]</th>\n",
       "      <th>[cdf%]</th>\n",
       "      <th>[mem KB]</th>\n",
       "      <th>[times called]</th>\n",
       "      <th>[Name]</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>MatMul</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.015</td>\n",
       "      <td>2.428%</td>\n",
       "      <td>49.295%</td>\n",
       "      <td>1.024</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm2_1/while/MatMul_2</td>\n",
       "      <td>lstm2_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Const</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.236%</td>\n",
       "      <td>3.130%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>dense1_1/Tensordot/Const</td>\n",
       "      <td>dense1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LoopCond</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.725%</td>\n",
       "      <td>19.325%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>lstm2_1/while/LoopCond</td>\n",
       "      <td>lstm2_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Merge</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.602%</td>\n",
       "      <td>9.688%</td>\n",
       "      <td>0.004</td>\n",
       "      <td>2</td>\n",
       "      <td>lstm2_1/while/Merge</td>\n",
       "      <td>lstm2_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Add</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.794%</td>\n",
       "      <td>39.738%</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1</td>\n",
       "      <td>lstm2_1/while/add_8</td>\n",
       "      <td>lstm2_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 [node type]  [start]  [first]  [avg ms]       [%]    [cdf%]  \\\n",
       "69                    MatMul    0.096    0.014     0.015    2.428%   49.295%   \n",
       "10                     Const   -0.096    0.002     0.001    0.236%    3.130%   \n",
       "36                  LoopCond    0.066    0.002     0.002    0.725%   19.325%   \n",
       "27                     Merge    0.033    0.024     0.005    1.602%    9.688%   \n",
       "63                       Add    0.101    0.006     0.005    0.794%   39.738%   \n",
       "\n",
       "    [mem KB]  [times called]                    [Name]      name  \n",
       "69     1.024               1    lstm2_1/while/MatMul_2   lstm2_1  \n",
       "10     0.000               1  dense1_1/Tensordot/Const  dense1_1  \n",
       "36     0.000               2    lstm2_1/while/LoopCond   lstm2_1  \n",
       "27     0.004               2       lstm2_1/while/Merge   lstm2_1  \n",
       "63     0.004               1       lstm2_1/while/add_8   lstm2_1  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_benchmark = benchmark_model(dec, '../tensorflow/bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=decoder.pbtxt --input_layer=\"input_2:0,input_3:0,input_4:0\" --input_layer_shape=\"1,1,93:1,256:1,256\" --input_layer_type=float,float,float --output_layer=\"dense1_1/truediv:0\"')\n",
    "dec_benchmark.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq2seq_benchmark = pd.concat([enc_benchmark, dec_benchmark])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_features = get_layer_features(enc)\n",
    "dec_features = get_layer_features(dec)\n",
    "seq2seq_features = pd.concat([enc_features, dec_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq2seq_df = join_benchmark(seq2seq_features, seq2seq_benchmark)\n",
    "cleaned_seq2seq = clean(seq2seq_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[avg ms]</th>\n",
       "      <th>[mem KB]</th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>...</th>\n",
       "      <th>filters</th>\n",
       "      <th>input_shape</th>\n",
       "      <th>input_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>name</th>\n",
       "      <th>output_shape</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>strides</th>\n",
       "      <th>units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.965</td>\n",
       "      <td>32.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 250, 64)</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>lstm4</td>\n",
       "      <td>(None, 250, 128)</td>\n",
       "      <td>32000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.941</td>\n",
       "      <td>32.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 250, 10)</td>\n",
       "      <td>2500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>lstm2</td>\n",
       "      <td>(None, 250, 32)</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.039</td>\n",
       "      <td>32.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 500, 32)</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>max_pooling1d</td>\n",
       "      <td>(None, 250, 32)</td>\n",
       "      <td>8000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.952</td>\n",
       "      <td>32.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 250, 32)</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>lstm1</td>\n",
       "      <td>(None, 250, 10)</td>\n",
       "      <td>2500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.941</td>\n",
       "      <td>32.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 250, 128)</td>\n",
       "      <td>32000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>lstm5</td>\n",
       "      <td>(None, 48)</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.940</td>\n",
       "      <td>32.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 250, 32)</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>lstm3</td>\n",
       "      <td>(None, 250, 64)</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[(None, None, 93), (None, 256), (None, 256)]</td>\n",
       "      <td>605</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>lstm2</td>\n",
       "      <td>[(None, None, 256), (None, 256), (None, 256)]</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.382</td>\n",
       "      <td>1.024</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, None, 71)</td>\n",
       "      <td>71</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>lstm1</td>\n",
       "      <td>[(None, 256), (None, 256), (None, 256)]</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033</td>\n",
       "      <td>64.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 500)</td>\n",
       "      <td>500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>embedding</td>\n",
       "      <td>(None, 500, 32)</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(None, 48)</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>dense</td>\n",
       "      <td>(None, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   [avg ms]  [mem KB]  activation_elu  activation_exponential  \\\n",
       "6     0.965    32.000            -1.0                    -1.0   \n",
       "4     0.941    32.000            -1.0                    -1.0   \n",
       "2     0.039    32.000            -1.0                    -1.0   \n",
       "3     0.952    32.000            -1.0                    -1.0   \n",
       "7     0.941    32.000            -1.0                    -1.0   \n",
       "5     0.940    32.000            -1.0                    -1.0   \n",
       "3     0.008     0.000            -1.0                    -1.0   \n",
       "1     0.382     1.024            -1.0                    -1.0   \n",
       "0     0.033    64.000             0.0                     0.0   \n",
       "8     0.011     0.004            -1.0                    -1.0   \n",
       "\n",
       "   activation_hard_sigmoid  activation_linear  activation_nan  \\\n",
       "6                     -1.0               -1.0               0   \n",
       "4                     -1.0               -1.0               0   \n",
       "2                     -1.0               -1.0               1   \n",
       "3                     -1.0               -1.0               0   \n",
       "7                     -1.0               -1.0               0   \n",
       "5                     -1.0               -1.0               0   \n",
       "3                     -1.0               -1.0               0   \n",
       "1                     -1.0               -1.0               0   \n",
       "0                      0.0                0.0               1   \n",
       "8                     -1.0               -1.0               0   \n",
       "\n",
       "   activation_relu  activation_selu  activation_sigmoid  ...  filters  \\\n",
       "6              0.0             -1.0                 0.0  ...     -1.0   \n",
       "4              0.0             -1.0                 0.0  ...     -1.0   \n",
       "2              0.0             -1.0                 0.0  ...     -1.0   \n",
       "3              0.0             -1.0                 0.0  ...     -1.0   \n",
       "7              0.0             -1.0                 0.0  ...     -1.0   \n",
       "5              0.0             -1.0                 0.0  ...     -1.0   \n",
       "3             -1.0             -1.0                -1.0  ...     -1.0   \n",
       "1             -1.0             -1.0                -1.0  ...     -1.0   \n",
       "0              0.0              0.0                 0.0  ...     -1.0   \n",
       "8              0.0             -1.0                 1.0  ...     -1.0   \n",
       "\n",
       "                                    input_shape  input_size  kernel_size  \\\n",
       "6                               (None, 250, 64)       16000         -1.0   \n",
       "4                               (None, 250, 10)        2500         -1.0   \n",
       "2                               (None, 500, 32)       16000         -1.0   \n",
       "3                               (None, 250, 32)        8000         -1.0   \n",
       "7                              (None, 250, 128)       32000         -1.0   \n",
       "5                               (None, 250, 32)        8000         -1.0   \n",
       "3  [(None, None, 93), (None, 256), (None, 256)]         605         -1.0   \n",
       "1                              (None, None, 71)          71         -1.0   \n",
       "0                                   (None, 500)         500         -1.0   \n",
       "8                                    (None, 48)          48         -1.0   \n",
       "\n",
       "            name                                   output_shape  output_size  \\\n",
       "6          lstm4                               (None, 250, 128)        32000   \n",
       "4          lstm2                                (None, 250, 32)         8000   \n",
       "2  max_pooling1d                                (None, 250, 32)         8000   \n",
       "3          lstm1                                (None, 250, 10)         2500   \n",
       "7          lstm5                                     (None, 48)           48   \n",
       "5          lstm3                                (None, 250, 64)        16000   \n",
       "3          lstm2  [(None, None, 256), (None, 256), (None, 256)]          768   \n",
       "1          lstm1        [(None, 256), (None, 256), (None, 256)]          768   \n",
       "0      embedding                                (None, 500, 32)        16000   \n",
       "8          dense                                      (None, 1)            1   \n",
       "\n",
       "   stride_size strides  units  \n",
       "6         -1.0      -1  128.0  \n",
       "4         -1.0      -1   32.0  \n",
       "2          2.0    (2,)   -1.0  \n",
       "3         -1.0      -1   10.0  \n",
       "7         -1.0      -1   48.0  \n",
       "5         -1.0      -1   64.0  \n",
       "3         -1.0      -1  256.0  \n",
       "1         -1.0      -1  256.0  \n",
       "0         -1.0      -1   -1.0  \n",
       "8         -1.0      -1    1.0  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_rnn = pd.concat([cleaned_lstm, cleaned_seq2seq]).fillna(0)\n",
    "cleaned_rnn.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2941177649690035\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>filters</th>\n",
       "      <th>input_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>units</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.919003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.256547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>71</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.921677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>605</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.921677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation_elu  activation_exponential  activation_hard_sigmoid  \\\n",
       "5            -1.0                    -1.0                     -1.0   \n",
       "0             0.0                     0.0                      0.0   \n",
       "1            -1.0                    -1.0                     -1.0   \n",
       "3            -1.0                    -1.0                     -1.0   \n",
       "\n",
       "   activation_linear  activation_nan  activation_relu  activation_selu  \\\n",
       "5               -1.0               0              0.0             -1.0   \n",
       "0                0.0               1              0.0              0.0   \n",
       "1               -1.0               0             -1.0             -1.0   \n",
       "3               -1.0               0             -1.0             -1.0   \n",
       "\n",
       "   activation_sigmoid  activation_softmax  activation_softplus  \\\n",
       "5                 0.0                -1.0                 -1.0   \n",
       "0                 0.0                 0.0                  0.0   \n",
       "1                -1.0                -1.0                 -1.0   \n",
       "3                -1.0                -1.0                 -1.0   \n",
       "\n",
       "   activation_softsign  activation_tanh  filters  input_size  kernel_size  \\\n",
       "5                 -1.0                1     -1.0        8000         -1.0   \n",
       "0                  0.0                0     -1.0         500         -1.0   \n",
       "1                 -1.0                1     -1.0          71         -1.0   \n",
       "3                 -1.0                1     -1.0         605         -1.0   \n",
       "\n",
       "   output_size  stride_size  units  avg_ms_actual  avg_ms_pred  \n",
       "5        16000         -1.0   64.0          0.940     0.919003  \n",
       "0        16000         -1.0   -1.0          0.033     0.256547  \n",
       "1          768         -1.0  256.0          0.382     0.921677  \n",
       "3          768         -1.0  256.0          0.008     0.921677  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_ms_rf, X_test, y_test = rf_regression_model(cleaned_rnn, '[avg ms]')\n",
    "\n",
    "y_pred = rnn_ms_rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "rnn_ms_rf_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 585.0930507439566\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>filters</th>\n",
       "      <th>input_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>units</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.000</td>\n",
       "      <td>32.992120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.000</td>\n",
       "      <td>27.936956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>71</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1.024</td>\n",
       "      <td>23.297088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>605</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.297088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation_elu  activation_exponential  activation_hard_sigmoid  \\\n",
       "5            -1.0                    -1.0                     -1.0   \n",
       "0             0.0                     0.0                      0.0   \n",
       "1            -1.0                    -1.0                     -1.0   \n",
       "3            -1.0                    -1.0                     -1.0   \n",
       "\n",
       "   activation_linear  activation_nan  activation_relu  activation_selu  \\\n",
       "5               -1.0               0              0.0             -1.0   \n",
       "0                0.0               1              0.0              0.0   \n",
       "1               -1.0               0             -1.0             -1.0   \n",
       "3               -1.0               0             -1.0             -1.0   \n",
       "\n",
       "   activation_sigmoid  activation_softmax  activation_softplus  \\\n",
       "5                 0.0                -1.0                 -1.0   \n",
       "0                 0.0                 0.0                  0.0   \n",
       "1                -1.0                -1.0                 -1.0   \n",
       "3                -1.0                -1.0                 -1.0   \n",
       "\n",
       "   activation_softsign  activation_tanh  filters  input_size  kernel_size  \\\n",
       "5                 -1.0                1     -1.0        8000         -1.0   \n",
       "0                  0.0                0     -1.0         500         -1.0   \n",
       "1                 -1.0                1     -1.0          71         -1.0   \n",
       "3                 -1.0                1     -1.0         605         -1.0   \n",
       "\n",
       "   output_size  stride_size  units  mem_kb_actual  mem_kb_pred  \n",
       "5        16000         -1.0   64.0         32.000    32.992120  \n",
       "0        16000         -1.0   -1.0         64.000    27.936956  \n",
       "1          768         -1.0  256.0          1.024    23.297088  \n",
       "3          768         -1.0  256.0          0.000    23.297088  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_mem_rf, X_test, y_test = rf_regression_model(cleaned_rnn, '[mem KB]')\n",
    "\n",
    "y_pred = rnn_mem_rf.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "rnn_mem_rf_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.4080142242073288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>filters</th>\n",
       "      <th>input_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>units</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>71</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.770152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.786142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.552898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.090653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation_elu  activation_exponential  activation_hard_sigmoid  \\\n",
       "1            -1.0                    -1.0                     -1.0   \n",
       "0             0.0                     0.0                      0.0   \n",
       "1            -1.0                    -1.0                     -1.0   \n",
       "6            -1.0                    -1.0                     -1.0   \n",
       "\n",
       "   activation_linear  activation_nan  activation_relu  activation_selu  \\\n",
       "1               -1.0               0             -1.0             -1.0   \n",
       "0                0.0               1              0.0              0.0   \n",
       "1               -1.0               0              1.0             -1.0   \n",
       "6               -1.0               0              0.0             -1.0   \n",
       "\n",
       "   activation_sigmoid  activation_softmax  activation_softplus  \\\n",
       "1                -1.0                -1.0                 -1.0   \n",
       "0                 0.0                 0.0                  0.0   \n",
       "1                 0.0                -1.0                 -1.0   \n",
       "6                 0.0                -1.0                 -1.0   \n",
       "\n",
       "   activation_softsign  activation_tanh  filters  input_size  kernel_size  \\\n",
       "1                 -1.0                1     -1.0          71         -1.0   \n",
       "0                  0.0                0     -1.0         500         -1.0   \n",
       "1                 -1.0                0     32.0       16000          3.0   \n",
       "6                 -1.0                1     -1.0       16000         -1.0   \n",
       "\n",
       "   output_size  stride_size  units  avg_ms_actual  avg_ms_pred  \n",
       "1          768         -1.0  256.0          0.382     0.770152  \n",
       "0        16000         -1.0   -1.0          0.033     0.786142  \n",
       "1        16000          1.0   -1.0          0.166     0.552898  \n",
       "6        32000         -1.0  128.0          0.965     0.090653  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_ms_stacked_1, X_test, y_test = stacked_regression_model(cleaned_rnn, '[avg ms]')\n",
    "\n",
    "y_pred = rnn_ms_stacked_1.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "rnn_ms_stacked_1_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1180.2715904600846\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>filters</th>\n",
       "      <th>input_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>units</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.626302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.066135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.104823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>13.525459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation_elu  activation_exponential  activation_hard_sigmoid  \\\n",
       "5            -1.0                    -1.0                     -1.0   \n",
       "4            -1.0                    -1.0                     -1.0   \n",
       "3            -1.0                    -1.0                     -1.0   \n",
       "0             0.0                     0.0                      0.0   \n",
       "\n",
       "   activation_linear  activation_nan  activation_relu  activation_selu  \\\n",
       "5               -1.0               0              0.0             -1.0   \n",
       "4               -1.0               0              0.0             -1.0   \n",
       "3               -1.0               0              0.0             -1.0   \n",
       "0                0.0               1              0.0              0.0   \n",
       "\n",
       "   activation_sigmoid  activation_softmax  activation_softplus  \\\n",
       "5                 0.0                -1.0                 -1.0   \n",
       "4                 0.0                -1.0                 -1.0   \n",
       "3                 0.0                -1.0                 -1.0   \n",
       "0                 0.0                 0.0                  0.0   \n",
       "\n",
       "   activation_softsign  activation_tanh  filters  input_size  kernel_size  \\\n",
       "5                 -1.0                1     -1.0        8000         -1.0   \n",
       "4                 -1.0                1     -1.0        2500         -1.0   \n",
       "3                 -1.0                1     -1.0        8000         -1.0   \n",
       "0                  0.0                0     -1.0         500         -1.0   \n",
       "\n",
       "   output_size  stride_size  units  mem_kb_actual  mem_kb_pred  \n",
       "5        16000         -1.0   64.0           32.0     8.626302  \n",
       "4         8000         -1.0   32.0           32.0     6.066135  \n",
       "3         2500         -1.0   10.0           32.0     1.104823  \n",
       "0        16000         -1.0   -1.0           64.0    13.525459  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_mem_stacked_1, X_test, y_test = stacked_regression_model(cleaned_rnn, '[mem KB]')\n",
    "\n",
    "y_pred = rnn_mem_stacked_1.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "rnn_mem_stacked_1_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 49404540930644.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>filters</th>\n",
       "      <th>input_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>units</th>\n",
       "      <th>avg_ms_actual</th>\n",
       "      <th>avg_ms_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.039</td>\n",
       "      <td>1.967120e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>71</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.382</td>\n",
       "      <td>-8.319017e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.941</td>\n",
       "      <td>-1.405718e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>605</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-8.319017e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation_elu  activation_exponential  activation_hard_sigmoid  \\\n",
       "2            -1.0                    -1.0                     -1.0   \n",
       "1            -1.0                    -1.0                     -1.0   \n",
       "7            -1.0                    -1.0                     -1.0   \n",
       "3            -1.0                    -1.0                     -1.0   \n",
       "\n",
       "   activation_linear  activation_nan  activation_relu  activation_selu  \\\n",
       "2               -1.0               1              0.0             -1.0   \n",
       "1               -1.0               0             -1.0             -1.0   \n",
       "7               -1.0               0              0.0             -1.0   \n",
       "3               -1.0               0             -1.0             -1.0   \n",
       "\n",
       "   activation_sigmoid  activation_softmax  activation_softplus  \\\n",
       "2                 0.0                -1.0                 -1.0   \n",
       "1                -1.0                -1.0                 -1.0   \n",
       "7                 0.0                -1.0                 -1.0   \n",
       "3                -1.0                -1.0                 -1.0   \n",
       "\n",
       "   activation_softsign  activation_tanh  filters  input_size  kernel_size  \\\n",
       "2                 -1.0                0     -1.0       16000         -1.0   \n",
       "1                 -1.0                1     -1.0          71         -1.0   \n",
       "7                 -1.0                1     -1.0       32000         -1.0   \n",
       "3                 -1.0                1     -1.0         605         -1.0   \n",
       "\n",
       "   output_size  stride_size  units  avg_ms_actual   avg_ms_pred  \n",
       "2         8000          2.0   -1.0          0.039  1.967120e+02  \n",
       "1          768         -1.0  256.0          0.382 -8.319017e+04  \n",
       "7           48         -1.0   48.0          0.941 -1.405718e+07  \n",
       "3          768         -1.0  256.0          0.008 -8.319017e+04  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_ms_stacked_2, X_test, y_test = stacked_regression_model_2(cleaned_rnn, '[avg ms]')\n",
    "\n",
    "y_pred = rnn_ms_stacked_2.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "rnn_ms_stacked_2_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'avg_ms_actual': y_test, 'avg_ms_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 874.9401704718323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/brian/anaconda/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_elu</th>\n",
       "      <th>activation_exponential</th>\n",
       "      <th>activation_hard_sigmoid</th>\n",
       "      <th>activation_linear</th>\n",
       "      <th>activation_nan</th>\n",
       "      <th>activation_relu</th>\n",
       "      <th>activation_selu</th>\n",
       "      <th>activation_sigmoid</th>\n",
       "      <th>activation_softmax</th>\n",
       "      <th>activation_softplus</th>\n",
       "      <th>activation_softsign</th>\n",
       "      <th>activation_tanh</th>\n",
       "      <th>filters</th>\n",
       "      <th>input_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>stride_size</th>\n",
       "      <th>units</th>\n",
       "      <th>mem_kb_actual</th>\n",
       "      <th>mem_kb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>72.137536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.909328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>16000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>12.980111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>605</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>768</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.599047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation_elu  activation_exponential  activation_hard_sigmoid  \\\n",
       "6            -1.0                    -1.0                     -1.0   \n",
       "7            -1.0                    -1.0                     -1.0   \n",
       "5            -1.0                    -1.0                     -1.0   \n",
       "3            -1.0                    -1.0                     -1.0   \n",
       "\n",
       "   activation_linear  activation_nan  activation_relu  activation_selu  \\\n",
       "6               -1.0               0              0.0             -1.0   \n",
       "7               -1.0               0              0.0             -1.0   \n",
       "5               -1.0               0              0.0             -1.0   \n",
       "3               -1.0               0             -1.0             -1.0   \n",
       "\n",
       "   activation_sigmoid  activation_softmax  activation_softplus  \\\n",
       "6                 0.0                -1.0                 -1.0   \n",
       "7                 0.0                -1.0                 -1.0   \n",
       "5                 0.0                -1.0                 -1.0   \n",
       "3                -1.0                -1.0                 -1.0   \n",
       "\n",
       "   activation_softsign  activation_tanh  filters  input_size  kernel_size  \\\n",
       "6                 -1.0                1     -1.0       16000         -1.0   \n",
       "7                 -1.0                1     -1.0       32000         -1.0   \n",
       "5                 -1.0                1     -1.0        8000         -1.0   \n",
       "3                 -1.0                1     -1.0         605         -1.0   \n",
       "\n",
       "   output_size  stride_size  units  mem_kb_actual  mem_kb_pred  \n",
       "6        32000         -1.0  128.0           32.0    72.137536  \n",
       "7           48         -1.0   48.0           32.0    25.909328  \n",
       "5        16000         -1.0   64.0           32.0    12.980111  \n",
       "3          768         -1.0  256.0            0.0    38.599047  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_mem_stacked_2, X_test, y_test = stacked_regression_model(cleaned_rnn, '[mem KB]')\n",
    "\n",
    "y_pred = rnn_mem_stacked_2.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "rnn_mem_stacked_2_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "pd.concat([X_test, pd.DataFrame({'mem_kb_actual': y_test, 'mem_kb_pred': y_pred})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNwAAJkN4AwBgMoQ3AAAmQ3gDAGAy/w/OPHApv3vLyQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_ms = [cnn_ms_rf_mse, cnn_ms_stacked_1_mse,cnn_ms_stacked_2_mse]\n",
    "plt.bar([i for i in range(len(cnn_ms))], cnn_ms, tick_label=[\n",
    "    'Random Forest',\n",
    "    'Stacked with RF',\n",
    "    'Stacked with Polynomial',\n",
    "])\n",
    "plt.title('MSE of Regression of CNN Computation Time')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "plt.xlabel('Regressor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFlCAYAAADyLnFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X1YFXX+//Hn4Rww4EBKsXZjmFJo\n3oWY5R0kkpqGt6WiRaapWeqWN4g/yzSXkFK7UdOyNmo1lVIz27ZsrRaIvGWXFNebTdfUtUUUXAEV\nhPP5/eHl2fiqqMmRbXw9rsvrOjOf+cy8Z3DOaz7DcI7NGGMQERGRXz2vmi5AREREqodCXURExCIU\n6iIiIhahUBcREbEIhbqIiIhFKNRFREQsQqEu1ebAgQM0atSIRx555Ky2SZMm0ahRIwoKCgDIyckh\nPj6eHj16EBsby7Bhw/jHP/7hXr5Ro0b06NGDXr16Vfp34MCBS6pp6tSpdOrUiVdffbXS/A0bNtCi\nRYtK677vvvsYOXIkhYWFv2DvPe/1119n1apVHt/O9u3bue++++jbt+9Zxzs+Pp5OnTq5j1mPHj3o\n2rWru64z/wc++uijSv1+//vfM2nSJADmzp1L27Ztyc/Pr7RMbGwsGzZsOGdNeXl5TJo0iR49etCz\nZ0/69evH2rVrq2uXf7F58+ZdVB0/X646f46//e1v3T+Ln58z8fHx5OXlERcXVy3bkV8RI1JN9u/f\nb5o3b27atWtnDhw44J5fUlJiOnfubMLCwsyRI0dMaWmpufvuu01ubq57mVWrVpl7773XlJeXG2OM\ne9nL1ahRI/PTTz+dNX/9+vXmgQceqDSvvLzcPPnkk2bWrFmXvd1fs7lz55rJkyefs+2RRx4xn3/+\neaV5W7ZsMU2bNjVFRUVm//79pnHjxqZVq1Zm9+7d7mXeeecdk5iYaIwxZs6cOaZZs2ZmyJAhxuVy\nuZd54IEHzPr168/a5pEjR0zHjh3Nxx9/7F5++/btpk2bNubbb7+97P29HOc6Hpez3OWornNGft0c\nNX1RIdZit9vp1q0bn376KSNHjgTgyy+/JCYmhnfffReAEydOUFRUxPHjx939evbsidPppKKiArvd\nfknb/Mc//sH06dM5evQoNpuNoUOH0rt3bwYNGoQxhuHDhzN16lTuuuuuKtdTXFxMQUEBERERABQV\nFfHiiy+ya9cuTp06Rdu2bZk4cSIOh4P09HRmzZqFl5cXd9xxB9999x1Llixh48aNLF++nBMnTuB0\nOlm0aBEfffQRS5cuxeVyUbt2baZMmUJoaCibN28mJSUFl8sFwBNPPEHXrl3PO3/SpEncfvvtPP74\n42zevJmXX36ZEydO4O3tzTPPPENUVBQrV67kz3/+M15eXvz4449cc801vPTSS4SGhp61v2+88Qaf\nffYZdrudBg0aMGXKFNatW8fSpUupqKjg5MmTzJ49+4LHf//+/fj5+eHj4wPANddcw5AhQ5gwYQLL\nli1zz/+5nj178v333/Puu+/y+OOPV7n+JUuWEBERQe/evd3zGjduzJw5cwgMDASo8nh8+eWXuFwu\nDh48SN26denfvz+LFy9m7969DBkyhKFDh7Jy5Uq++OKLSsulpKRQt25d4uPjefjhh7n//vsB3NNH\njhwhNzeXl19+Gbvdzm233cb06dMpKSkhPz+fxo0b89prr7F8+fJKy3311VfV+nM8nwMHDtCjRw/+\n9re/MXfuXPbt20deXh75+fk0bdqUe+65h1WrVnHgwAESEhKIjY0FYMGCBe5jdvPNNzN16lTq1q17\n0duVGlbTVxViHfv37zfh4eFm69at5v7773fPHzx4sNm5c2elkcS7775rWrRoYTp16mQmTJhgPvro\nI3P8+HF3n7CwMBMbG2t69uzp/vfUU0+dtc1Tp06ZmJgYs2bNGmOMMf/+979NZGSk+etf/+pez7lG\nL+vXrzfNmzc3PXv2NN27dzdt2rQxvXv3Nm+99ZYpKyszxhgzadIk84c//MEYc3oUP2HCBLNw4UJT\nUFBg7r77brN9+3ZjjDErV640YWFhZv/+/WbFihWmdevWpqioyBhjzIYNG8ygQYPc+5aZmek+No8+\n+qj54x//aIw5PfKcNm1alfMTExPNO++8YwoKCkzbtm1NTk6OMcaYXbt2mbvvvtvs27fPrFixwrRq\n1cp9d2L69Olm4sSJZ+3/8uXLzYABA0xJSYkx5vToeejQoe7XL7zwwrl+xOaRRx4x0dHRpmfPnqZj\nx46mbdu2ZuzYsWbbtm3GmP/+H6ioqDAPP/ywSUlJMcacPVJ/4YUXzI4dO0xERIT7js35RupPPPGE\nWbx48TnrMcZc1PE4ePCgqaioMN27dzdjxowxFRUVZvv27aZ58+amoqLCrFixwoSHh5s9e/YYY4yZ\nOXOmGTNmjHuffz7K/vn0z1+npKSYVatWGWOMKSsrM7GxseaLL744a7nq/Dn+3P/9v37mZ3HmmEdH\nR5tjx46ZEydOmNatW5sZM2YYY4z585//bLp06WKMMebjjz82zzzzjDl16pQxxphly5aZYcOGVbld\n+d+ikbpUu2bNmmG328nNzeW6666jpKSEsLCwSssMGTKEfv36sWnTJjZt2sTbb7/N22+/zfLlywkI\nCADg/fffJygoqMpt7d27l9LSUrp06QJA3bp16dKlC5mZmbRs2bLKviEhIXzyyScArFixgldffZVu\n3brh7e0NwF/+8he2bt3K8uXLATh58iRwelQYGhpK48aNAejTpw9JSUnu9TZq1Ain0+lex48//ljp\nd5vHjh3j6NGjdOvWjenTp/P111/Trl07xo0bB3De+Wds2bKFkJAQ7rzzTgBuv/12IiIi2LhxIzab\njaZNm3LDDTcA0KRJE/785z+fte8ZGRn07dsXPz8/AB599FHefPNNysrKqjxmABMnTuT++++noKCA\n4cOHU7duXZo0aVJpGS8vL2bOnEnv3r3p0KHDOdfTqFEjnnnmGcaPH8/KlSvPuz2bzYap4tOsL3Q8\nmjdvzo033ghAvXr16NChA15eXtxyyy2UlpZy4sQJANq3b0+DBg0A6N+/P7169brgsfi5hIQEsrKy\nePvtt9m7dy+HDh2qdDfqUuu+mJ/jpWjXrp373PrNb35DZGQkcPo8OHr0KADffPMNW7du5cEHHwTA\n5XK5j4/8OijUxSN69uzJ6tWrCQoKOuvNMTs7m7/97W8MGzaM6OhooqOjGTduHLGxsWRlZblvc16M\niooKbDZbpXnGGMrLyy+p3gcffJDvv/+ep59+mg8//BCHw4HL5eL111933/I8duwYNpuNTZs2nRUy\nXl7/feb0TFDC6TfFXr16kZCQ4J4+dOgQ1157LXFxcURHR5OVlUVmZibz5s3jiy++OO/8i9lnb29v\nrrnmGvf88wWiy+WqtA6Xy3XJxywoKIjXXnuN2NhYWrZs6b6wOuPGG2/khRdeIDExsdKt85+Lj4/n\n22+/5cUXXzzvdsLDw8nJyTnrAcxly5Zx4sQJ6tevX+Xx+L+3/x2Oc7/t/fzXPi6Xq9L0z4/hqVOn\nztl/3LhxVFRU0K1bNzp27MhPP/1U5cVIdfwcL8XFHAeXy8WwYcMYNGgQAGVlZfznP/+5rO3KlaWn\n38UjevXqxRdffMGf/vQn9+/qzggKCmLBggVs3rzZPS8/P5/i4uKzRvQX0rBhQxwOB19++SVw+inp\nNWvW0K5du0uuecKECfz000988MEHAHTo0IH33nsPYwxlZWU8+eSTLF68mIiICPbu3cuOHTsAWLNm\njTvw/68OHTrw2WefcejQIQCWLl3K4MGDAYiLi2P79u307duX3/3udxw7doz8/Pzzzj8jPDycPXv2\nsGXLFuD0MwWbNm3i7rvvvuh9jYyMZMWKFe6R5KJFi2jduvU5f/9dlVtuuYWRI0fy4osvnnNUev/9\n9xMVFcX7779/3nXMmDGD9PR0fvzxx3O2DxgwgI0bN7J69Wp3sOXm5jJnzhzCwsKq5XgArF+/nry8\nPOD0BUN0dDRw+v9rbm4uAD/88AM7d+5097Hb7e6LoW+//ZZRo0bRvXt3AL7//nsqKirOWu6M6qq7\nOnXo0IHly5dTXFwMnH5Sf+LEiTVWj1w6jdTFI+rWrUtoaCgBAQHUrl27UluDBg144403ePXVV/n3\nv/9NrVq1CAgIIDk5mYYNG7qXGzx4cKURMJweDd17773uaW9vb+bPn09SUhJz586loqKCUaNG0aZN\nm0uuOTAwkAkTJjBjxgweeOABnn32WV588UV69OjBqVOnaNeuHcOGDcPb25tXXnmFxMREvLy8aNas\nGQ6HA19f37PW2aFDB4YPH87QoUOx2Ww4nU7mzZuHzWZjwoQJJCcn89prr2Gz2Rg9ejT16tU77/wz\ngoKCeP311/nd737HyZMnsdlszJgxgwYNGvC3v/3tovb1oYce4qeffqJfv364XC7q16/PrFmzLvmY\nATz++OOsWrWKBQsWMGDAgLPan3vuObKzs8/bPygoiJSUFIYNG3bO9tq1a7No0SJmzpzJW2+9hZeX\nF76+vrz44ou0b98e4LKPB5z+P5uQkEB+fr77oTeAJ598kkmTJpGenk7Dhg0rPXDZqVMnXnnlFU6d\nOsXYsWMZNWoUfn5+OJ1OWrduzb59+85a7uf7XR11V6d+/fqRl5dH//79sdls3HjjjaSkpNRILfLL\n2Mzl3tMRucoUFxczf/58xowZg6+vL9u2beOJJ54gMzPznKN1+d+3cuVK1qxZw1tvvVXTpYhcFo3U\nRS6R0+nE29ubhx56CIfDgcPhcI+qRURqkkbqIiIiFqEH5URERCxCoS4iImIRCnURERGL+NU/KJef\nX1TTJcgvUKeOH4WF5/+0LRHxPJ2Hv07BwQHnbdNIXWqEw3FpX9oiItVP56H1KNRFREQsQqEuIiJi\nEQp1ERERi1Coi4iIWIRCXURExCIU6iIiIhahUBcREbEIhbqIiIhFKNRFREQsQqEuIiJiEQp1ERER\ni1Coi4iIWMSv/lvaRMRahqZ8XdMliFSrdyd1umLb0khdRETEIhTqIiIiFuGxUHe5XDz//PMMGDCA\n+Ph4fvzxx3MuM2zYMJYuXQrAyZMnGTNmDIMGDWL48OEUFBR4qjwRERHL8Vior127lrKyMtLS0hg/\nfjwpKSlnLfPaa6/xn//8xz29dOlSwsLCWLJkCb1792b+/PmeKk9ERMRyPBbq2dnZREZGAhAeHk5u\nbm6l9i+++AKbzUZUVNQ5+0RFRbFu3TpPlSciImI5Hnv6vbi4GKfT6Z622+2Ul5fjcDjYtWsXf/zj\nH5kzZw5vvPFGpT4BAQEA+Pv7U1RUdMHt1Knjh8Nhr/4dEI8LDg6o6RJERDzuSr7XeSzUnU4nJSUl\n7mmXy4XDcXpzq1atIi8vj8GDB/Ovf/0Lb29vbr755kp9SkpKCAwMvOB2CguPe2YHxKOCgwPIz7/w\nRZuIyK9ddb/XVXWR4LFQj4iI4JtvvqF79+7k5OQQFhbmbps4caL79dy5c7n++uuJiorihx9+ID09\nnRYtWpCRkUGrVq08VZ6IiIjleCzUO3fuTFZWFnFxcRhjSE5OJjU1lZCQEGJiYs7ZZ+DAgSQmJjJw\n4EC8vb2ZPXu2p8oTERGxHJsxxtR0EZdDt3B/nXT7Xc5HnygnVlPdnyhX1e13ffiMiIiIRSjURURE\nLEKhLiIiYhEKdREREYtQqIuIiFiEQl1ERMQiFOoiIiIWoVAXERGxCIW6iIiIRSjURURELEKhLiIi\nYhEKdREREYtQqIuIiFiEQl1ERMQiFOoiIiIWoVAXERGxCIW6iIiIRSjURURELEKhLiIiYhEKdRER\nEYtQqIuIiFiEQl1ERMQiFOoiIiIWoVAXERGxCIW6iIiIRSjURURELMLhqRW7XC6mTZvGzp078fHx\nISkpifr167vbP/jgA1auXInNZmPUqFFER0djjCEqKopbb70VgPDwcMaPH++pEkVERCzFY6G+du1a\nysrKSEtLIycnh5SUFBYsWABAQUEBS5YsYdWqVZSWlvLAAw/QsWNH9u3bR9OmTXnzzTc9VZaIiIhl\neez2e3Z2NpGRkcDpEXdubq67LSgoiE8++QRvb28OHz5MYGAgNpuNbdu2kZeXR3x8PMOHD2fPnj2e\nKk9ERMRyPBbqxcXFOJ1O97Tdbqe8vNw97XA4WLx4MQMGDKBr164ABAcHM2LECBYtWsQTTzxBQkKC\np8oTERGxHI/dfnc6nZSUlLinXS4XDkflzT3yyCP079+f4cOHs379eu68807sdjsAd911F3l5eRhj\nsNls591OnTp+OBx2z+yEeFRwcEBNlyAi4nFX8r3OY6EeERHBN998Q/fu3cnJySEsLMzdtmfPHl55\n5RXmzp2Lt7c3Pj4+eHl5MW/ePGrXrs3w4cPZsWMHN910U5WBDlBYeNxTuyAeFBwcQH5+UU2XISLi\ncdX9XlfVRYLHQr1z585kZWURFxeHMYbk5GRSU1MJCQkhJiaGxo0bM2DAAGw2G5GRkdx99900atSI\nhIQE0tPTsdvtzJgxw1PliYiIWI7NGGNquojLodHer5NG6nI+Q1O+rukSRKrVu5M6Vev6qhqp68Nn\nRERELEKhLiIiYhEKdREREYtQqIuIiFiEQl1ERMQiFOoiIiIWoVAXERGxCIW6iIiIRSjURURELEKh\nLiIiYhEKdREREYtQqIuIiFiEQl1ERMQiFOoiIiIWoVAXERGxCIW6iIiIRSjURURELEKhLiIiYhEK\ndREREYtQqIuIiFiEQl1ERMQiFOoiIiIWoVAXERGxCIW6iIiIRSjURURELEKhLiIiYhEKdREREYtw\neGrFLpeLadOmsXPnTnx8fEhKSqJ+/fru9g8++ICVK1dis9kYNWoU0dHRnDx5koSEBI4cOYK/vz8v\nvfQSQUFBnipRRETEUjw2Ul+7di1lZWWkpaUxfvx4UlJS3G0FBQUsWbKEZcuW8d577zFt2jSMMSxd\nupSwsDCWLFlC7969mT9/vqfKExERsRyPhXp2djaRkZEAhIeHk5ub624LCgrik08+wdvbm8OHDxMY\nGIjNZqvUJyoqinXr1nmqPBEREcvx2O334uJinE6ne9put1NeXo7DcXqTDoeDxYsXM3fuXOLj4919\nAgICAPD396eoqOiC26lTxw+Hw+6BPRBPCw4OqOkSREQ87kq+13ks1J1OJyUlJe5pl8vlDvQzHnnk\nEfr378/w4cNZv359pT4lJSUEBgZecDuFhcert3C5IoKDA8jPv/BFm4jIr111v9dVdZHgsdvvERER\nZGRkAJCTk0NYWJi7bc+ePYwePRpjDN7e3vj4+ODl5UVERATp6ekAZGRk0KpVK0+VJyIiYjkeG6l3\n7tyZrKws4uLiMMaQnJxMamoqISEhxMTE0LhxYwYMGIDNZiMyMpK7776b5s2bk5iYyMCBA/H29mb2\n7NmeKk9ERMRybMYYU9NFXA7dwv110u13OZ+hKV/XdAki1erdSZ2qdX1V3X732Ej910pvKGIl1f1m\nIiL/2/SJciIiIhahUBcREbEIhbqIiIhFKNRFREQsQqEuIiJiEQp1ERERi1Coi4iIWIRCXURExCIU\n6iIiIhahUBcREbEIhbqIiIhFKNRFREQsQqEuIiJiEQp1ERERi1Coi4iIWIRCXURExCIU6iIiIhah\nUBcREbEIhbqIiIhFKNRFREQsQqEuIiJiEQp1ERERi1Coi4iIWIRCXURExCIU6iIiIhahUBcREbEI\nh6dW7HK5mDZtGjt37sTHx4ekpCTq16/vbn/vvff47LPPALj33nsZPXo0xhiioqK49dZbAQgPD2f8\n+PGeKlFERMRSPBbqa9eupaysjLS0NHJyckhJSWHBggUA7N+/n9WrV/PRRx9hs9kYNGgQ9913H76+\nvjRt2pQ333zTU2WJiIhYlsduv2dnZxMZGQmcHnHn5ua622644Qbeeecd7HY7Xl5elJeXU6tWLbZt\n20ZeXh7x8fEMHz6cPXv2eKo8ERERy/HYSL24uBin0+mettvtlJeX43A48Pb2JigoCGMML7/8Mk2a\nNKFBgwYcPnyYESNG0K1bNzZv3kxCQgIrVqyocjt16vjhcNg9tRsiv2rBwQE1XYLIVe9KnoceC3Wn\n00lJSYl72uVy4XD8d3OlpaVMnjwZf39/pk6dCkCzZs2w208H9F133UVeXh7GGGw223m3U1h43EN7\nIPLrl59fVNMliFz1qvs8rOoiwWO33yMiIsjIyAAgJyeHsLAwd5sxhqeeeopGjRoxffp0d5DPmzeP\n999/H4AdO3Zw0003VRnoIiIi8l8eG6l37tyZrKws4uLiMMaQnJxMamoqISEhuFwuNm7cSFlZGZmZ\nmQCMGzeOESNGkJCQQHp6Ona7nRkzZniqPBEREcvxWKh7eXkxffr0SvNCQ0Pdr7du3XrOfgsXLvRU\nSSIiIpamD58RERGxCIW6iIiIRSjURURELEKhLiIiYhEKdREREYtQqIuIiFiEQl1ERMQiFOoiIiIW\noVAXERGxCIW6iIiIRSjURURELKLKUM/Lyztv27p166q9GBEREfnlqgz1kSNHul+PGTOmUtvLL7/s\nmYpERETkF6ky1I0x7tf79+8/b5uIiIjUvCpD3WaznfP1uaZFRESkZulBOREREYtwVNWYn5/PvHnz\nznp9ZlpERET+d1Q5Uo+Lizvn63NNi4iISM2qcqQ+evToK1WHiIiIXKYqR+onT57kpZdeYsuWLQDM\nmDGDli1b8vDDD1f5N+wiIiJy5VUZ6i+++CInTpzg5ptvJj09nU8//ZSPP/6Yhx9+mOnTp1+pGkVE\nROQiVHn7PScnh08//RSAr776im7dunHrrbdy6623VnpoTkRERGpelSN1L6//Nm/YsIG2bdu6p0+d\nOuW5qkREROSSVTlSr127Nlu2bKGkpIRDhw7Rrl074HTA33DDDVekQBEREbk4VYb65MmTGTt2LEeO\nHGHq1Kn4+fkxf/58Fi1axFtvvXWlahQREZGLUGWob9++nREjRrg/533VqlUEBwczcuRI9uzZQ4sW\nLa5IkSIiInJhVYb6pEmTuO6662jbti3e3t5ntffu3dtjhYmIiMilqTLUP/74Y/70pz+RlZVF48aN\n6d69O+3atav0AN35uFwupk2bxs6dO/Hx8SEpKYn69eu729977z0+++wzAO69915Gjx7NyZMnSUhI\n4MiRI/j7+/PSSy8RFBR0mbsoIiJydagyne+44w7Gjx/PypUrGThwIFlZWTz00EM8//zzbNiwocoV\nr127lrKyMtLS0hg/fjwpKSnutv3797N69WqWLVtGWloa3377LTt27GDp0qWEhYWxZMkSevfuzfz5\n86tnL0VERK4CF/0tbc2bNycxMZHJkyeza9cuRo4cWeXy2dnZREZGAhAeHk5ubq677YYbbuCdd97B\nbrfj5eVFeXk5tWrVqtQnKiqKdevW/ZJ9EhERuSpVefsdwBjDpk2b+OKLL8jIyOCOO+4gPj6e6Ojo\nKvsVFxfjdDrd03a7nfLychwOB97e3gQFBWGM4eWXX6ZJkyY0aNCA4uJiAgICAPD396eoqOiCO1Cn\njh8Oh/2Cy4lcjYKDA2q6BJGr3pU8D6sM9alTp5KZmUmTJk3o1q0bCQkJ+Pr6XtSKnU4nJSUl7mmX\ny4XD8d/NlZaWMnnyZPz9/Zk6depZfUpKSggMDLzgdgoLj19UPSJXo/z8C18Yi4hnVfd5WNVFQpWh\nnpaWRu3atfn73//O3//+d1555ZVK7V999dV5+0ZERPDNN9/QvXt3cnJyCAsLc7cZY3jqqae45557\nGDFiRKU+6enptGjRgoyMDFq1anXBnRMREZHTqgz1qkL7Qjp37kxWVhZxcXEYY0hOTiY1NZWQkBBc\nLhcbN26krKyMzMxMAMaNG8fAgQNJTExk4MCBeHt7M3v27F+8fRERkatNlaF+8803/+IVe3l5nfVN\nbqGhoe7XW7duPWe/OXPm/OJtioiIXM0u+ul3ERER+d+mUBcREbEIhbqIiIhFKNRFREQsQqEuIiJi\nEQp1ERERi1Coi4iIWIRCXURExCIU6iIiIhahUBcREbEIhbqIiIhFKNRFREQsQqEuIiJiEQp1ERER\ni1Coi4iIWIRCXURExCIU6iIiIhahUBcREbEIhbqIiIhFKNRFREQsQqEuIiJiEQp1ERERi1Coi4iI\nWIRCXURExCIU6iIiIhahUBcREbEIhbqIiIhFODy1YpfLxbRp09i5cyc+Pj4kJSVRv379SssUFBQQ\nFxfHp59+Sq1atTDGEBUVxa233gpAeHg448eP91SJIiIiluKxUF+7di1lZWWkpaWRk5NDSkoKCxYs\ncLdnZmYye/ZsDh8+7J63b98+mjZtyptvvumpskRERCzLY7ffs7OziYyMBE6PuHNzcytv2MuL1NRU\nateu7Z63bds28vLyiI+PZ/jw4ezZs8dT5YmIiFiOx0bqxcXFOJ1O97Tdbqe8vByH4/Qm27dvf1af\n4OBgRowYQbdu3di8eTMJCQmsWLGiyu3UqeOHw2Gv3uJFLCI4OKCmSxC56l3J89Bjoe50OikpKXFP\nu1wud6CfT7NmzbDbTwf0XXfdRV5eHsYYbDbbefsUFh6vnoJFLCg/v6imSxC56lX3eVjVRYLHbr9H\nRESQkZEBQE5ODmFhYRfsM2/ePN5//30AduzYwU033VRloIuIiMh/eWyk3rlzZ7KysoiLi8MYQ3Jy\nMqmpqYSEhBATE3POPiNGjCAhIYH09HTsdjszZszwVHkiIiKW47FQ9/LyYvr06ZXmhYaGnrXc119/\n7X597bXXsnDhQk+VJCIiYmn68BkRERGLUKiLiIhYhEJdRETEIhTqIiIiFqFQFxERsQiFuoiIiEUo\n1EVERCxCoS4iImIRCnURERGLUKiLiIhYhEJdRETEIhTqIiIiFqFQFxERsQiFuoiIiEUo1EVERCxC\noS4iImIRCnURERGLUKiLiIhYhEJdRETEIhTqIiIiFqFQFxERsQiFuoiIiEUo1EVERCxCoS4iImIR\nCnURERGLUKiLiIhYhEJdRETEIjwW6i6Xi+eff54BAwYQHx/Pjz/+eNYyBQUFdOnShdLSUgBOnjzJ\nmDFjGDRoEMOHD6egoMBT5Yk/DRwaAAAS4klEQVSIiFiOx0J97dq1lJWVkZaWxvjx40lJSanUnpmZ\nydChQzl8+LB73tKlSwkLC2PJkiX07t2b+fPne6o8ERERy/FYqGdnZxMZGQlAeHg4ubm5lTfs5UVq\naiq1a9c+Z5+oqCjWrVvnqfJEREQsx+GpFRcXF+N0Ot3Tdrud8vJyHI7Tm2zfvv05+wQEBADg7+9P\nUVHRBbdTp44fDoe9mqoWsZbg4ICaLkHkqnclz0OPhbrT6aSkpMQ97XK53IF+MX1KSkoIDAy84HYK\nC49fXqEiFpaff+ELYxHxrOo+D6u6SPDY7feIiAgyMjIAyMnJISws7KL6pKenA5CRkUGrVq08VZ6I\niIjleGyk3rlzZ7KysoiLi8MYQ3JyMqmpqYSEhBATE3POPgMHDiQxMZGBAwfi7e3N7NmzPVWeiIiI\n5Xgs1L28vJg+fXqleaGhoWct9/XXX7tf+/r6MmfOHE+VJCIiYmn68BkRERGLUKiLiIhYhEJdRETE\nIhTqIiIiFqFQFxERsQiFuoiIiEUo1EVERCxCoS4iImIRCnURERGLUKiLiIhYhEJdRETEIhTqIiIi\nFqFQFxERsQiFuoiIiEUo1EVERCxCoS4iImIRCnURERGLUKiLiIhYhEJdRETEIhTqIiIiFqFQFxER\nsQiFuoiIiEUo1EVERCxCoS4iImIRCnURERGLUKiLiIhYhEJdRETEIhyeWrHL5WLatGns3LkTHx8f\nkpKSqF+/vrv9ww8/ZNmyZTgcDp588kmio6M5evQoXbt2JSwsDID77ruPwYMHe6pEERERS/FYqK9d\nu5aysjLS0tLIyckhJSWFBQsWAJCfn8+iRYtYsWIFpaWlDBo0iPbt2/P3v/+d2NhYpkyZ4qmyRERE\nLMtjt9+zs7OJjIwEIDw8nNzcXHfbli1baNmyJT4+PgQEBBASEsKOHTvIzc1l27ZtPPLII/z2t7/l\n0KFDnipPRETEcjw2Ui8uLsbpdLqn7XY75eXlOBwOiouLCQgIcLf5+/tTXFxMw4YNadasGe3atWP1\n6tUkJSUxZ86cKrdTp44fDofdU7sh8qsWHBxw4YVExKOu5HnosVB3Op2UlJS4p10uFw6H45xtJSUl\nBAQE0KJFC3x9fQHo3LnzBQMdoLDweDVXLmId+flFNV2CyFWvus/Dqi4SPHb7PSIigoyMDABycnLc\nD78BtGjRguzsbEpLSykqKmL37t2EhYXx3HPPsWbNGgDWrVtH06ZNPVWeiIiI5XhspN65c2eysrKI\ni4vDGENycjKpqamEhIQQExNDfHw8gwYNwhjD2LFjqVWrFuPHj2fy5MksXboUX19fkpKSPFWeiIiI\n5diMMaami7gc1X1bY2jK19W6PpGa9O6kTjVdwiXTOShWU93nYY3cfhcREZErS6EuIiJiEQp1ERER\ni1Coi4iIWIRCXURExCIU6iIiIhahUBcREbEIhbqIiIhFKNRFREQsQqEuIiJiEQp1ERERi1Coi4iI\nWIRCXURExCIU6iIiIhahUBcREbEIhbqIiIhFKNRFREQsQqEuIiJiEQp1ERERi1Coi4iIWIRCXURE\nxCIU6iIiIhahUBcREbEIhbqIiIhFKNRFREQsQqEuIiJiEQp1ERERi3B4asUul4tp06axc+dOfHx8\nSEpKon79+u72Dz/8kGXLluFwOHjyySeJjo6moKCACRMmcPLkSX7zm98wY8YMfH19PVWiiIiIpXhs\npL527VrKyspIS0tj/PjxpKSkuNvy8/NZtGgRy5Yt4/e//z2vvPIKZWVlzJ8/n9jYWJYsWUKTJk1I\nS0vzVHkiIiKW47FQz87OJjIyEoDw8HByc3PdbVu2bKFly5b4+PgQEBBASEgIO3bsqNQnKiqK7777\nzlPliYiIWI7Hbr8XFxfjdDrd03a7nfLychwOB8XFxQQEBLjb/P39KS4urjTf39+foqKiC24nODjg\ngstcik9n96rW9YnIpdE5KPLLeWyk7nQ6KSkpcU+7XC4cDsc520pKSggICKg0v6SkhMDAQE+VJyIi\nYjkeC/WIiAgyMjIAyMnJISwszN3WokULsrOzKS0tpaioiN27dxMWFkZERATp6ekAZGRk0KpVK0+V\nJyIiYjk2Y4zxxIrPPP2+a9cujDEkJyeTkZFBSEgIMTExfPjhh6SlpWGM4YknnqBr164cPnyYxMRE\nSkpKqFOnDrNnz8bPz88T5YmIiFiOx0JdRERErix9+IyIiIhFKNRFREQswmN/0ib/ezZs2MAzzzzD\nbbfdBpz+C4N69eoxa9YsfHx8fvF6x44dS1xcHPfcc89l17hy5UrmzJnDLbfc4p732GOPERMTc9nr\n/rlNmzYREBBA48aNq3W9IlVZuHAh3333HV5eXthsNsaOHUuzZs3YuXMnx44do3Xr1he9rg0bNrBs\n2TJeffXVS6ph6dKlHD58mDFjxlzU8vn5+bzxxhtMmzat0nnTvn17srKyzttv0qRJbNu2jdq1a2OM\n4ejRowwZMoQHH3zwip3nVyOF+lWmTZs2ld4Exo8fz9dff839999fg1VVFhsby4QJEzy6jRUrVtC9\ne3eFulwxP/zwA19//TVLly7FZrOxfft2EhMTWb16NV9++SXXX3/9JYX6lRIcHMy0adOASz9vEhIS\niIqKAuDo0aPExsbSt29f4Mqc51cjhfpVrKysjEOHDnHttddSUVHB888/z7///W8KCwuJiorimWee\nYdKkSfj4+PCvf/2LQ4cOkZKSQtOmTfnggw/46KOPCA4O5siRIwCcOnWKyZMns3//fioqKhgyZAjd\nu3cnPj6eRo0a8Y9//AM/Pz/uuusuvv32W44dO8a7777Ltddee8Fajx07RkJCAsXFxVRUVPD000/T\ntm1bYmNjufXWW/Hx8eGFF17g2WefpbCwEIDnnnuORo0aMWnSJPbt20dpaSmPP/44ISEhZGZmsm3b\nNm677TZuuukmjx5nEYCgoCAOHjzI8uXLiYqK4o477mD58uXk5eXx8ccf4+3tTdOmTTl48CAffPCB\nu9/rr79O7dq1SUpKYsuWLZw6dYoxY8a4P6jrxIkTjB49ml69etGzZ09mz57Npk2bMMbw2GOP0a1b\nNzZv3kxycjLXXnstXl5ehIeHV6qtT58+vPPOOwQGBnLPPfewePFimjRpQp8+fZg9ezaTJk3i+eef\nr3TelJWVMX78eA4ePEjt2rWZM2cO3t7e593/w4cP4+Pjg81m88wBFkChftVZv3498fHxHDlyBC8v\nL/r370/btm05cOAA4eHh9OvXj9LSUneoA9x0001Mnz7d/WeICQkJ/OEPf+DTTz/FZrO5r7zT0tKo\nU6cOM2fOpLi4mL59+9KmTRvg9GcTPPfcczz++ONcc801pKamkpiYyKZNm7jvvvsq1fjHP/6R77//\nHoA6deowZ84cFixYQLt27Rg8eDB5eXkMHDiQtWvXcvz4cZ566imaNGnCzJkzadOmDYMGDWLv3r38\nv//3/3j77bfZsGEDK1asACArK4tmzZoRGRlJ9+7dFehyxQQFBbFgwQIWL17MG2+8wTXXXMPYsWPp\n2rUrffr04frrr6dFixZ89913LFy4EF9fX55//nm+/fZbfH19KSwsZPny5eTn57N48WLatWvH8ePH\nGTlyJI8++igxMTGkp6dz4MABli1bRmlpKf3796d9+/bMmDGD2bNn06BBA6ZOnXpWbTExMWRmZnLD\nDTdQr149srKy8PHxcV8wA2edN8ePH2fs2LHUq1eP+Ph4tm/fTosWLSqtd+bMmbz55pscPHiQ0NBQ\nXn/9dXfbuc5zuXwK9avMmdvvhYWFDB06lHr16gFQu3Zttm7dyvr163E6nZSVlbn73HHHHQDccMMN\n/PWvf2XPnj3cdttt7pP9zIm8e/du2rVrB5z+1MDQ0FD2798PQNOmTQEIDAx0/04/MDCQ0tLSs2o8\n12253bt306NHDwDq1q2L0+mkoKAAgAYNGgCwa9cu1q9fz+effw6cHt07nU6mTJnClClTKC4upmfP\nnpd1/ER+qR9//BGn08mMGTMA2Lp1KyNGjDjrWZTrrruOxMRE/P392bNnD+Hh4fzzn/90j66Dg4MZ\nO3YsGzZsYOPGjTRq1Mh9vu7atYtt27YRHx8PQHl5OQcPHiQvL899nkRERLBv375K2+zSpQtvvvkm\nN954I2PHjmXRokUYY+jSpct59+faa691v39cf/31nDhx4qxlztx+T09PZ9asWYSEhLjbdPvdM/T0\n+1XqzIj6ueee49ChQ6xcuZKAgABmz57N0KFDOXnyJGc+wuD/3i675ZZb+OGHHzh58iQVFRVs374d\ngNDQUDZv3gyc/uz/Xbt2uU/6y/Xzdefl5XHs2DFq164NgJfX6f/GDRs25LHHHmPRokW89tpr9OjR\ng0OHDrFt2zbeeOMNFi5cyMyZMykvL8dms6GPaJAraefOnUybNs19IdugQQMCAgKw2+3YbDZcLhdF\nRUXMmTOHV199laSkJGrVqoUxhoYNG7J161YAioqKePzxxwHo2LEj8+bN47XXXiMvL4+GDRtyzz33\nsGjRIt5//326detGvXr1CA4OZvfu3QDu9fxcWFgYBw4cYMuWLdx7770cP36cr776yv378DN+ft5c\nym30e++9l5iYGKZMmXLpB04uiUbqV7HbbruN+Ph4kpKSGDNmDOPGjSM7OxtfX1/q16/PoUOHztkv\nKCiIp59+mri4OIKCgtzfed+/f3+mTJnCwIEDKS0tZfTo0Vx33XXVUusTTzzB5MmTWbNmDSdPnmT6\n9Onu7xI4Y+TIkTz77LN8+OGHFBcXM3r0aIKDg8nPz6d37974+fkxdOhQHA4Hd955J7NmzaJevXqE\nhoZWS40iVenSpQu7d++mX79++Pn5YYxh4sSJBAQE0KxZM15++WVCQ0OJiIigT58++Pn5ERgYyKFD\nh+jbty/r1q1j4MCBVFRUMGrUKPd6r7/+esaMGcPkyZN555132LhxI4MGDeL48ePcd999OJ1OZs6c\n6R79+/v7n/M5ltatW3PgwAG8vLxo3bo1P/zwA/7+/u5nVIBK582leuqpp+jbty9/+ctfftHxk4uj\nT5QTERGxCN1+FxERsQiFuoiIiEUo1EVERCxCoS4iImIRCnURERGL0J+0iVjQgQMHuP/++91/rudy\nuSgpKaF379789re/reHqRMRTFOoiFvWb3/yGTz75xD2dl5dH165deeCBB/S3+SIWpVAXuUrk5+dj\njMHf35+FCxfy+eefU1FRQYcOHUhISMBms/GHP/yBxYsXExAQQMOGDQkJCWHMmDG0adOGZs2akZ+f\nz/Lly0lNTT2rf0lJCePGjePw4cMAjBo1ipiYGFJTU/n444/x8vKiRYsWTJ8+HZfLRXJyMuvWrcNm\ns9GzZ09GjBjBhg0bmDlzJi6Xi9tvv52XXnqpho+ayK+LQl3Eog4dOkSvXr0oLS2lsLCQ5s2bM2/e\nPHbt2kVubi7Lly/HZrORkJDA6tWradSoER988AErV67E29ub+Ph492d1FxYWMnz4cO655x4yMjLO\n2d/lcnHzzTezcOFCtm/fzurVq+nYsSNvvfUWmZmZ2O12nn32WfLy8li7di0//fQTq1evpqysjPj4\neMLCwvD19WXv3r1888037m8hE5GLp1AXsagzt99dLhcpKSns3r2b9u3bM3PmTLZs2eL+dr2TJ09y\n0003UVBQQHR0NE6nE4AHHniAY8eOudd35513ArBu3bpz9n/wwQd55ZVXyMvLo2PHjowaNQq73U7L\nli156KGHiImJYciQIdStW5cNGzbQp08f7HY7vr6+9OjRg3Xr1tGpUyf3Z6KLyKVTqItYnJeXFxMn\nTqR37978/ve/p6KigsGDBzNkyBDg9LfZ2e12li9fjsvlOu96rrnmGoDz9vf39+fzzz8nMzOTb775\nhnfffZc//elPzJ8/n5ycHDIyMhg2bBizZs06azvGGCoqKiptR0Qunf6kTeQq4HA4mDhxIvPnz6dJ\nkyZ88sknlJSUUF5ezqhRo1izZg1t27YlPT2d4uJiysrK+PLLL8/5TVxt2rQ5Z//Fixczd+5cunXr\nxtSpUykoKODo0aN0796dsLAwnn76adq3b8/OnTtp06YNq1atoqKighMnTvDpp5+e9RWkInLpNFIX\nuUpERUXRsmVLNm/eTJcuXejfvz8VFRVERkbSp08fbDYbjz76KAMGDMDPz486depQq1ats9bTqVMn\nduzYcVb/Mw/K9ejRA7vdTkJCAkFBQQwYMICHHnoIX19fGjRowIMPPoi3tzd79+6lV69enDp1ih49\netC5c2c2bNhQA0dGxDr0LW0iAsA///lP0tPTeeyxxwB48skn6devH506darZwkTkommkLiIA3Hzz\nzWzdupXY2FhsNhsdOnQgOjq6pssSkUugkbqIiIhF6EE5ERERi1Coi4iIWIRCXURExCIU6iIiIhah\nUBcREbEIhbqIiIhF/H8LEhrNgyNjgwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnn_ms = [rnn_ms_rf_mse, rnn_ms_stacked_1_mse]#, rnn_ms_stacked_2_mse]\n",
    "\n",
    "plt.bar([i for i in range(len(rnn_ms))], rnn_ms, tick_label=[\n",
    "    'Random Forest',\n",
    "    'Stacked with RF',\n",
    "#     'Stacked with Polynomial',\n",
    "])\n",
    "plt.title('MSE of Regression of RNN Computation Time')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "plt.xlabel('Regressor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAFlCAYAAACZXICzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIvojlYaDisAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_mem = [cnn_mem_rf_mse, cnn_mem_stacked_1_mse]\n",
    "plt.bar(range(len(cnn_mem)), cnn_mem, tick_label=[\"Random Forest\", \"Stacked with RF\"])\n",
    "\n",
    "plt.title('MSE of Regression of CNN Memory Usage')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Regressor')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAFlCAYAAAAH0PriAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKRiHphNoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnn_mem = [rnn_mem_rf_mse, rnn_mem_stacked_1_mse, rnn_mem_stacked_2_mse]\n",
    "\n",
    "plt.bar(range(len(rnn_mem)), rnn_mem, tick_label=[\n",
    "    \"Random Forest\",\n",
    "    \"Stacked with RF\",\n",
    "    \"Stacked with Polynomial\",\n",
    "])\n",
    "\n",
    "plt.title('MSE of Regression of RNN Memory Usage')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Regressor')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_cnn_ms(model):\n",
    "    \n",
    "    layer_features = get_layer_features(model)\n",
    "    cleaned_features = clean(layer_features, inference=True)\n",
    "    \n",
    "    X = cleaned_features.drop(['name', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "    \n",
    "    predicted = cnn_ms_rf.predict(X)\n",
    "    \n",
    "    return predicted\n",
    "\n",
    "def predict_cnn_mem(model):\n",
    "    \n",
    "    layer_features = get_layer_features(model)\n",
    "    cleaned_features = clean(layer_features, inference=True)\n",
    "    \n",
    "    X = cleaned_features.drop(['name', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "    \n",
    "    predicted = cnn_mem_rf.predict(X)\n",
    "    \n",
    "    return predicted\n",
    "\n",
    "def predict_rnn_ms(model):\n",
    "    \n",
    "    layer_features = get_layer_features(model)\n",
    "    cleaned_features = clean(layer_features, inference=True)\n",
    "    \n",
    "    X = cleaned_features.drop(['name', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "    \n",
    "    predicted = rnn_ms_rf.predict(X)\n",
    "    \n",
    "    return predicted\n",
    "\n",
    "def predict_rnn_mem(model):\n",
    "    \n",
    "    layer_features = get_layer_features(model)\n",
    "    cleaned_features = clean(layer_features, inference=True)\n",
    "    \n",
    "    X = cleaned_features.drop(['name', 'input_shape', 'output_shape', 'strides'], axis=1)  # Features\n",
    "    \n",
    "    predicted = rnn_mem_rf.predict(X)\n",
    "    \n",
    "    return predicted\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.36353538e-01, 2.36353538e-01, 7.42407467e-01, 1.35628300e+00,\n",
       "       4.46846130e+00, 1.35197900e+00, 1.74586864e+00, 5.04323484e-01,\n",
       "       2.55629047e-01, 1.47515977e+00, 4.86585900e-01, 2.55629047e-01,\n",
       "       1.47515977e+00, 1.20133403e+00, 1.20133403e+00, 1.35628300e+00,\n",
       "       1.35628300e+00, 3.12488300e+00, 4.46846130e+00, 2.70438022e+00,\n",
       "       2.55629047e-01, 1.47515977e+00, 4.86585900e-01, 2.55629047e-01,\n",
       "       1.47515977e+00, 1.20133403e+00, 1.35628300e+00, 3.12488300e+00,\n",
       "       4.46846130e+00, 2.70438022e+00, 2.55629047e-01, 1.47515977e+00,\n",
       "       4.86585900e-01, 2.55629047e-01, 1.47515977e+00, 1.20133403e+00,\n",
       "       1.35628300e+00, 3.12488300e+00, 4.46846130e+00, 2.86571840e+00,\n",
       "       6.65594775e-02, 1.42459369e+00, 8.60290160e-02, 6.65594775e-02,\n",
       "       1.42459369e+00, 7.10492832e+00, 1.14343720e+01, 1.89940684e-01,\n",
       "       1.89940684e-01, 1.16728879e+00, 1.24703656e+00, 2.60023692e-01,\n",
       "       6.65594775e-02, 1.42459369e+00, 8.60290160e-02, 6.65594775e-02,\n",
       "       1.42459369e+00, 7.10492832e+00, 1.89940684e-01, 1.16728879e+00,\n",
       "       1.24703656e+00, 2.60023692e-01, 6.65594775e-02, 1.42459369e+00,\n",
       "       8.60290160e-02, 6.65594775e-02, 1.42459369e+00, 7.10492832e+00,\n",
       "       1.89940684e-01, 1.16728879e+00, 1.24703656e+00, 2.60023692e-01,\n",
       "       6.65594775e-02, 1.42459369e+00, 8.60290160e-02, 6.65594775e-02,\n",
       "       1.42459369e+00, 7.10492832e+00, 1.89940684e-01, 1.16728879e+00,\n",
       "       1.24703656e+00, 3.01941884e-01, 6.00910866e-03, 8.53728027e-01,\n",
       "       2.52508947e-03, 6.00910866e-03, 8.53728027e-01, 5.09112797e+00,\n",
       "       9.68870979e+00, 2.55629047e-01, 2.55629047e-01, 2.85702845e-01,\n",
       "       1.47515977e+00, 9.74374053e-03, 6.00910866e-03, 8.53728027e-01,\n",
       "       2.52508947e-03, 6.00910866e-03, 8.53728027e-01, 5.09112797e+00,\n",
       "       2.55629047e-01, 2.85702845e-01, 1.47515977e+00, 9.74374053e-03,\n",
       "       6.00910866e-03, 8.53728027e-01, 2.52508947e-03, 6.00910866e-03,\n",
       "       8.53728027e-01, 5.09112797e+00, 2.55629047e-01, 2.85702845e-01,\n",
       "       1.47515977e+00, 9.74374053e-03, 6.00910866e-03, 8.53728027e-01,\n",
       "       2.52508947e-03, 6.00910866e-03, 8.53728027e-01, 5.09112797e+00,\n",
       "       2.55629047e-01, 2.85702845e-01, 1.47515977e+00, 9.74374053e-03,\n",
       "       6.00910866e-03, 8.53728027e-01, 2.52508947e-03, 6.00910866e-03,\n",
       "       8.53728027e-01, 5.09112797e+00, 2.55629047e-01, 2.85702845e-01,\n",
       "       1.47515977e+00, 9.74374053e-03, 6.00910866e-03, 8.53728027e-01,\n",
       "       2.52508947e-03, 6.00910866e-03, 8.53728027e-01, 5.09112797e+00,\n",
       "       2.55629047e-01, 2.85702845e-01, 1.47515977e+00, 5.41553876e+00,\n",
       "       3.01489281e-02, 8.78406013e-01, 4.97629271e+00, 3.01489281e-02,\n",
       "       8.78406013e-01, 4.99669227e+00, 5.45222057e+00, 6.65594775e-02,\n",
       "       6.65594775e-02, 1.02280348e-01, 1.42459369e+00, 4.95787602e+00,\n",
       "       3.01489281e-02, 8.78406013e-01, 4.97629271e+00, 3.01489281e-02,\n",
       "       8.78406013e-01, 4.99669227e+00, 6.65594775e-02, 1.02280348e-01,\n",
       "       1.42459369e+00, 4.95787602e+00, 3.01489281e-02, 8.78406013e-01,\n",
       "       4.97629271e+00, 3.01489281e-02, 8.78406013e-01, 4.99669227e+00,\n",
       "       6.65594775e-02, 1.02280348e-01, 1.42459369e+00, 1.29132322e+00,\n",
       "       1.73822969e+00])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = tf.keras.applications.resnet50.ResNet50(\n",
    "    include_top=True,\n",
    "    weights='imagenet')\n",
    "\n",
    "predict_cnn_ms(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.58366976e+02, 4.58366976e+02, 5.07565568e+02, 4.30150784e+02,\n",
       "       2.86710982e+03, 4.28269184e+02, 1.13164730e+03, 8.55955904e+02,\n",
       "       8.89117568e+02, 1.67417798e+03, 8.56031936e+02, 8.89117568e+02,\n",
       "       1.67417798e+03, 3.68758656e+02, 3.68758656e+02, 4.30150784e+02,\n",
       "       4.30150784e+02, 8.79883008e+02, 2.86710982e+03, 1.03390592e+03,\n",
       "       8.89117568e+02, 1.67417798e+03, 8.56031936e+02, 8.89117568e+02,\n",
       "       1.67417798e+03, 3.68758656e+02, 4.30150784e+02, 8.79883008e+02,\n",
       "       2.86710982e+03, 1.03390592e+03, 8.89117568e+02, 1.67417798e+03,\n",
       "       8.56031936e+02, 8.89117568e+02, 1.67417798e+03, 3.68758656e+02,\n",
       "       4.30150784e+02, 8.79883008e+02, 2.86710982e+03, 5.25025984e+02,\n",
       "       8.81678080e+01, 4.38814720e+02, 7.32794880e+01, 8.81678080e+01,\n",
       "       4.38814720e+02, 5.93681408e+02, 9.47827456e+02, 5.74959680e+02,\n",
       "       5.74959680e+02, 6.31324800e+02, 1.76282426e+03, 1.91770176e+02,\n",
       "       8.81678080e+01, 4.38814720e+02, 7.32794880e+01, 8.81678080e+01,\n",
       "       4.38814720e+02, 5.93681408e+02, 5.74959680e+02, 6.31324800e+02,\n",
       "       1.76282426e+03, 1.91770176e+02, 8.81678080e+01, 4.38814720e+02,\n",
       "       7.32794880e+01, 8.81678080e+01, 4.38814720e+02, 5.93681408e+02,\n",
       "       5.74959680e+02, 6.31324800e+02, 1.76282426e+03, 1.91770176e+02,\n",
       "       8.81678080e+01, 4.38814720e+02, 7.32794880e+01, 8.81678080e+01,\n",
       "       4.38814720e+02, 5.93681408e+02, 5.74959680e+02, 6.31324800e+02,\n",
       "       1.76282426e+03, 2.00086592e+02, 0.00000000e+00, 1.84960000e+02,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.84960000e+02, 8.17317184e+02,\n",
       "       1.00480858e+03, 8.89117568e+02, 8.89117568e+02, 8.78668544e+02,\n",
       "       1.67417798e+03, 0.00000000e+00, 0.00000000e+00, 1.84960000e+02,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.84960000e+02, 8.17317184e+02,\n",
       "       8.89117568e+02, 8.78668544e+02, 1.67417798e+03, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.84960000e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.84960000e+02, 8.17317184e+02, 8.89117568e+02, 8.78668544e+02,\n",
       "       1.67417798e+03, 0.00000000e+00, 0.00000000e+00, 1.84960000e+02,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.84960000e+02, 8.17317184e+02,\n",
       "       8.89117568e+02, 8.78668544e+02, 1.67417798e+03, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.84960000e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.84960000e+02, 8.17317184e+02, 8.89117568e+02, 8.78668544e+02,\n",
       "       1.67417798e+03, 0.00000000e+00, 0.00000000e+00, 1.84960000e+02,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.84960000e+02, 8.17317184e+02,\n",
       "       8.89117568e+02, 8.78668544e+02, 1.67417798e+03, 7.51539200e+01,\n",
       "       1.00480000e-02, 9.77305600e+01, 5.62971200e+00, 1.00480000e-02,\n",
       "       9.77305600e+01, 8.87514240e+01, 1.89005568e+02, 8.81678080e+01,\n",
       "       8.81678080e+01, 1.13683968e+02, 4.38814720e+02, 5.61974400e+00,\n",
       "       1.00480000e-02, 9.77305600e+01, 5.62971200e+00, 1.00480000e-02,\n",
       "       9.77305600e+01, 8.87514240e+01, 8.81678080e+01, 1.13683968e+02,\n",
       "       4.38814720e+02, 5.61974400e+00, 1.00480000e-02, 9.77305600e+01,\n",
       "       5.62971200e+00, 1.00480000e-02, 9.77305600e+01, 8.87514240e+01,\n",
       "       8.81678080e+01, 1.13683968e+02, 4.38814720e+02, 5.89724800e+00,\n",
       "       4.58342400e+00])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_cnn_mem(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.130042, 0.145918, 0.130042, 0.148489])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(layers.Embedding(max_features, 128, input_length=maxlen))\n",
    "rnn_model.add(layers.Bidirectional(layers.LSTM(64)))\n",
    "rnn_model.add(layers.Dropout(0.5))\n",
    "rnn_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "rnn_model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "predict_rnn_ms(rnn_model)\n",
    "\n",
    "# get_layer_features(rnn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.705252, 29.568892, 24.705252, 24.705252])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn_mem(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
